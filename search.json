[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/bruin/HW0.html",
    "href": "posts/bruin/HW0.html",
    "title": "Contructing data visualization of the Palmer Penguin data set",
    "section": "",
    "text": "In this blog, I will be explaining how to construct a data visualization of the Palmer Penguins data set."
  },
  {
    "objectID": "posts/bruin/HW0.html#breakdown-of-the-code",
    "href": "posts/bruin/HW0.html#breakdown-of-the-code",
    "title": "Contructing data visualization of the Palmer Penguin data set",
    "section": "Breakdown of the Code",
    "text": "Breakdown of the Code\nTo generate the interactive plot, we will be using the following code to generate the plot. fig = px.scatter(data_frame = dataset, x = “column name 1”, y = “column name 2”, title = “title of plot”, color = “column name 3”, hover_data = [“other columns to display”], width = num, height = num, opacity = num )\nThe arguments used is:  - data_frame = dataset we are using for this plot - x = a specific column from the data set that will be used on the x-axis - y = a sepcific column from the data set tht will be used on the y-axis - title = main title of the plot - color = the color of the points (can be used to seperate different categories i.e. species of penguins) - hover_data = other column information that you would like to display when user hovers over a specific point (note: the columns used for x, y, and color will already be shown when the user hovers over a specific point) - width = the width of the plot - height = the height of the plot - opacity = allows the point on the plot to be sheer\nWe will now be replacing our data set with the respective column names into the code.\n\nfig = px.scatter(data_frame = penguins,\n                 x = \"Body Mass (g)\",\n                 y = \"Flipper Length (mm)\",\n                 title = \"Body Mass and Flipper Length in Penguin Species\",\n                 color = \"Species\",\n                 hover_data = [\"Individual ID\"],\n                 width = 500,\n                 height = 300,\n                 opacity = 0.5\n                )\n\nfig.update_layout(margin={\"r\":0, \"t\":30, \"l\":0, \"b\":0})\nfig.show()\n\n                                                \n\n\nIf we hover our cursor over the points in the plot, we can see information about the specific point. For this particular example, it is able to show you the type of species, the body mass/flipper length for this particular penguin, and the penguin’s ID number."
  },
  {
    "objectID": "posts/bruin/HW0.html#analysis-of-plot",
    "href": "posts/bruin/HW0.html#analysis-of-plot",
    "title": "Contructing data visualization of the Palmer Penguin data set",
    "section": "Analysis of Plot",
    "text": "Analysis of Plot\nFrom the outputted plot, we can see that Gentoo Penguins have a bigger body mass and longer flippers compared to Adelie and Chinstrap penguins. In addition, we can see the Adelie and Chinstrap penguins have similar body mass and Flipper length. Therefore, there does seem to be a correlation between flipper length and body mass of the penguins. For penguins with bigger body mass, they seem to have longer flippers, while smaller body mass penguins have shorter flippers. It is possible that the bigger penguins will need to have bigger flippers for balance and support!"
  },
  {
    "objectID": "posts/bruin/HW0.html#additional-style-choices",
    "href": "posts/bruin/HW0.html#additional-style-choices",
    "title": "Contructing data visualization of the Palmer Penguin data set",
    "section": "Additional Style Choices",
    "text": "Additional Style Choices\nIn addition, we added a following line of code: fig.update_layout(margin={“r”:0, “t”:0, “l”:0, “b”:0})\nThis allows the plot to look a lot neater when generated. Let me show you the difference of using it and not using it.\n\nfig = px.scatter(data_frame = penguins,\n                 x = \"Body Mass (g)\",\n                 y = \"Flipper Length (mm)\",\n                 title = \"Body Mass and Flipper Length in Penguin Species\",\n                 color = \"Species\",\n                 hover_data = [\"Individual ID\"],\n                 width = 500,\n                 height = 300,\n                 opacity = 0.5\n                )\n\n# fig.update_layout(margin={\"r\":0, \"t\":0, \"l\":0, \"b\":0})\nfig.show()\n\n                                                \n\n\nAbove is what it looks like when we commented out the line of code. The plot looks squashed and not as clear compared to when we adjusted the layout of the plot. There are still a lot of other arguments that you can add to the plot to customize it. More information can be found on this website: https://plotly.com"
  },
  {
    "objectID": "posts/bruin/HW4.html",
    "href": "posts/bruin/HW4.html",
    "title": "Heat Diffusion",
    "section": "",
    "text": "In this blog, I will be going over how to simulate two-dimensional heat diffusion in various ways.  We will use the equation below to simulate two-dimensional heat diffusion as a sequence of matrix-vector multiplications.\n  u  i , j   k + 1   ≈  u  i , j  k  + ϵ  (  u  i + 1 , j  k  +  u  i − 1 , j  k  +  u  i , j + 1  k  +  u  i , j − 1  k  − 4  u  i , j  k  )  , \nFor this specific example, we will be setting N = 101 and epilson = 0.2\nTo begin, let’s import all the necessary libraries.\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport timeit\nimport jax\nfrom jax.experimental import sparse\nfrom jax import jit\nimport jax.numpy as jnp\n\n\nMatrix Multiplication\nFirst, let’s try to create this simulation using matrix multiplication.\n\nfrom heat_equation import advance_time_matvecmul\nimport inspect\nprint(inspect.getsource(advance_time_matvecmul))\n\ndef advance_time_matvecmul(A, u, epsilon):\n    \"\"\"Advances the simulation by one timestep, via matrix-vector multiplication\n    Args:\n        A: The 2d finite difference matrix, N^2 x N^2. \n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    N = u.shape[0]\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N))\n    return u\n\n\n\nThe purpose of the function is to return the updated matrix after one timestep. We will later use this function to iterate through 2700 times to observe the heat diffusion.\nNext, let us create a function called get_A(N).\n\nfrom heat_equation import get_A\nimport inspect\nprint(inspect.getsource(get_A))\n\ndef get_A(N) :\n    \"\"\"\n    Takes in the dimensions, N\n    Creates the corresponding matrix\n    \"\"\"\n    n = N * N\n    diagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\n    diagonals[1][(N-1)::N] = 0\n    diagonals[2][(N-1)::N] = 0\n    A = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n    return A\n\n\n\nThis function will take in the dimensions we have preset, which is 101 and generate a matrix without all-zero rows or all-zero columns.\nNow, let us use the two functions we have defined above to create a simulation of the heat diffusion after 2700 iterations. Let’s also output the heat diffusion after ever 300 iterations. Therefore, we should create an empty list to store NxN grid state. Since we will be creating this simulation in various ways, we should also calculate the amount of time it takes for this simulation to run. In the end, we will compare and see which method takes the least amount of time!\n\nN = 101 # dimension\nepsilon = 0.2\nplots = [] # empty list\nmatrix = get_A(N) # getting inital array\n\nu = np.zeros((N, N)) # defining u to be passed through advance_time_matvecmul\nu[int(N/2), int(N/2)] = 1.0\n\nstart = timeit.default_timer() # getting the start time\n\nfor i in range(1, 2701) : # iterating through 2700 times\n    u = advance_time_matvecmul(matrix, u, epsilon) \n    if i % 300 == 0 : # after every 300th iteration, save the NxN grid state\n        plots.append(u)\n        \nstop = timeit.default_timer() # getting the end time\nexecution_time = stop - start # calculating the time it took to run\nprint(\"Time elapsed:\", execution_time)\n\nTime elapsed: 23.147153500001878\n\n\nTo observe our plots, we can run the for loop below to visualize the heat diffusion.\n\nfor i in range(len(plots)) :\n    plt.subplot(3, 3, i+1)\n    plt.imshow(plots[i])\n\n\n\n\n\n\n\n\nWe can see from our results that this methodology took a very long time to run. Therefore, it might not be the most efficient method. Let’s see if there is another way to perform the same simulation.\n\n\nSparse matrix in JAX\nLet’s use the data structure that exploits a lot of zeros in the matrix A, which utiliziees the sparse matrix data structures. In addition, the JAX package holds an experimental sparse matrix support through utilizing jnp.\nWe can use the same function advance_time_matvecmul to get the NxN grid state. However, let’s use a different method to create the matrix. We will define a function called get_sparse_A.\n\nfrom heat_equation import get_sparse_A\nprint(inspect.getsource(get_sparse_A))\n\ndef get_sparse_A(N) :\n    \"\"\"\n    Takes in the dimensions, N\n    Utilizies previously defined function and changing it to a jnp array\n    Puts the array into a sparse array\n    \"\"\"\n    A = get_A(N)\n    j_array = jnp.array(A)\n    A_sp_matrix = sparse.BCOO.fromdense(j_array)\n    return A_sp_matrix\n\n\n\nIn this function, we are still using the get_A to generate our inital matrix. However, the JAX library is only able to read in jnp array’s and not np array’s. Therefore, we will need to convert the array into jnp format. The sparse.BCOO.fromdense function is a batched coordinate sparse array, which offers a compressed storage format. This will allow our code to run a lot faster and be more efficient. In addition, we want to make sure we use the jited version of advance_time_matvecmul as this allows jax to perform an optimized calculation.\n\nN = 101 # dimension\nepsilon = 0.2 \nu = np.zeros((N, N)) # reseting u\nu[int(N/2), int(N/2)] = 1.0\n\nplots = [] # list to insert NxN grid state\n\njitted = jax.jit(advance_time_matvecmul) # \n\nmatrix = get_sparse_A(N) # getting the matrix using the new function we defined\nstart = timeit.default_timer() # getting start time\nfor i in range(1, 2701) : # looping through 2700 iterations\n    u = jitted(matrix, u, epsilon) # using jitted function\n    if i % 300 == 0 : # getting every 300th iteration\n        plots.append(u) # adding to the list\nstop = timeit.default_timer() # getting stop time\nexecution_time = stop - start # calculating the duration\nprint(\"Time elapsed:\", execution_time)\n\nTime elapsed: 0.9678866250033025\n\n\nWe can see that using the sparse matrix in JAX, the heat diffusion simulation was a lot faster than our previous method. It was about 10x faster. Let’s take a look at the plots and see if we got the same result.\n\nfor i in range(len(plots)) :\n    plt.subplot(3, 3, i+1)\n    plt.imshow(plots[i])\n\n\n\n\n\n\n\n\n\n\nDirect Operation with Numpy\nIn this example, we will be using numpy to advance the solution by one timestep. We will be using vectorized array operation, like np.roll().\n\nfrom heat_equation import advance_time_numpy\nprint(inspect.getsource(advance_time_numpy))\n\ndef advance_time_numpy(u, epsilon) :\n    \"\"\"\n    Takes in the NxN grid state and episilon\n    Using the heat diffusion equation\n    Using np.roll to generate the boundary condition\n    Returns the new grid state after advancing by one timestep\n    \"\"\"\n    padded = np.pad(u, 1, mode = 'constant') # modifies array by padding edges\n    np_u = padded + epsilon * (np.roll(padded, 1, axis = 1) + \n                               np.roll(padded, -1, axis = 1) + \n                               np.roll(padded, -1, axis = 0) + \n                               np.roll(padded, 1, axis = 0) - \n                               (4 * padded))\n    np_u_slc = np_u[1:-1, 1:-1]\n    return np_u_slc\n\n\n\nWe will now use our new function for the simulation.\n\nepsilon = 0.2 \nplots = []\n\nu = np.zeros((N, N))\nu[int(N/2), int(N/2)] = 1.0\n\nstart = timeit.default_timer() \nfor i in range(1, 2701) :\n    u = advance_time_numpy(u, epsilon) \n    if i % 300 == 0 :\n        plots.append(u)\nstop = timeit.default_timer()\nexecution_time = stop - start\nprint(\"Time elapsed:\", execution_time)\n\nTime elapsed: 0.2493002920091385\n\n\nWow! We can see that numpy runs abour 100x times faster than our first method! Let’s view our plots for the simulation.\n\nfor i in range(len(plots)) :\n    plt.subplot(3, 3, i+1)\n    plt.imshow(plots[i])\n\n\n\n\n\n\n\n\n\n\nWith JAX\nSimilarly to the numpy function above, we will instead be using jnp instead of np.\n\nfrom heat_equation import advance_time_jax\nprint(inspect.getsource(advance_time_jax))\n\ndef advance_time_jax(u, epsilon) : \n    \"\"\"\n    Takes in u, which is the NxN grid state, and episilon\n    Using the heat diffusion equation\n    Uses jnp.roll to generate the boundary condition\n    Returns the new grid state after advancing by one timestep\n    \"\"\"\n    padded = jnp.pad(u, 1, mode = 'constant') # modifies array by padding edges\n    jnp_u = padded + epsilon * (jnp.roll(padded, 1, axis = 1) + \n                                jnp.roll(padded, -1, axis = 1) + \n                                jnp.roll(padded, -1, axis = 0) + \n                                jnp.roll(padded, 1, axis = 0) - \n                                (4 * padded))\n    jnp_u_slc = jnp_u[1:-1, 1:-1]\n    return jnp_u_slc\n\n\n\nYou can observe from about that in every instance that we used np, we switched it into jnp as jax only runs with jnp arrays. Now, let’s fun our function!\n\nepsilon = 0.2\nplots = []\n\nu = np.zeros((N, N))\nu[int(N/2), int(N/2)] = 1.0\n\njitted_jax = jax.jit(advance_time_jax) # remember to jit!\nstart = timeit.default_timer()\nfor i in range(1, 2701) :\n    u = jitted_jax(u, epsilon) \n    if i % 300 == 0 :\n        plots.append(u)\nstop = timeit.default_timer()\nexecution_time = stop - start\nprint(\"Time elapsed:\", execution_time)\n\nTime elapsed: 0.11503070800972637\n\n\n\nfor i in range(len(plots)) :\n    plt.subplot(3, 3, i+1)\n    plt.imshow(plots[i])\n\n\n\n\n\n\n\n\nWe have to remember to jit our function as well similar to what we did in part 2. Looking at our execution time, we can see that the JAX method performed the fastest out of all the methods!\n\n\nComparison\nOut of all of the methods, the slowest was generating our own functions. This does not utilize a lot of efficient libraries that can help speed up the run time of our simulation. It took about 21.8 seconds to run through 2700 simulations. We can see that if we use the same approach while using JAX, it helps to speed the process up to around 0.89 seconds. Just simply using the JAX library with our own defined function was able to speed the simulation up by 20x. Next, we also recreated our inital function of advancing the simulation by one timestep through using the np.roll method. This significantly sped up the processes without the need to use JAX. In fact, it out performed the first and second method with a run time of 0.28 seconds. Impressive right?! Next, let’s see how our newly created function would work if we used the JAX library. We see that using the JAX library on our numpy function, our simulation ran in 0.092 seconds. Through comparing all of these simulations, we see that using the JAX library can significantly optimize and decrease our run time. Though it might be a little hard to understand what each function does in the JAX library, once you understand it, it makes coding the heat diffusion simulation a lot simplier. In addition, understanding the math behind the function will also help a lot. If you are proficient in these topics, creating the heat simulation with JAX is a lot simplier than using matrix multiplication and numpy.\n\n\nFinal Takeaways\nThrough creating the heat diffusion simulation in various ways, we see that using the JAX library is extremely helpful and runs really quickly. Therefore, it is more common to use JAX over numpy especially when working with very large amounts of data!"
  },
  {
    "objectID": "posts/bruin/HW1.html",
    "href": "posts/bruin/HW1.html",
    "title": "SQL Tutorial on Climate",
    "section": "",
    "text": "In this blog, I will be explaining how to construct a database using the Temp, stations, and country datasets."
  },
  {
    "objectID": "posts/bruin/HW1.html#interactive-box-plot",
    "href": "posts/bruin/HW1.html#interactive-box-plot",
    "title": "SQL Tutorial on Climate",
    "section": "Interactive Box Plot",
    "text": "Interactive Box Plot\nWe will now create an interactive box plot for the new pandas dataframe we have created. Again, to connect to the database and create the pandas dataframe, we use a similar method as the previous example. We want to create a function that will allow users to easily change parameters to look for different information in the databse and generate a plot. For this specific example, we want to find the temperature changes in UCLA and USC through the months.  To create the box plot, we will use the following code: fig = px.box(df, x = “x-values”, y = “y-values”, color = “specific column”, width = m, height = n, kwargs) - df: dataframe used to create the plot - x: column used for the x-axis - y: column used for the y-axis - color: color of the box plots - kwargs: additional parameters passed through by the user\n\ndef station_temp_plot(db_file, station_one, station_two, **kwargs) :\n    \"\"\"\n    Creating an interactive box plot that identifies the change in temperature between two stations through the months\n    Function will: \n    1. Use station_climate_comparison_database to create a dataframe with all the information needed\n    2. Creating an interactive box plot\n    \"\"\"\n    # inputting parameters passed through station_temp_plot into station_climate_comparison_database\n    df = station_climate_comparison_database(db_file, station_one, station_two) \n    \n    # creating a interactive box plot (more information regarding arguments above)\n    fig = px.box(df,\n             x = \"NAME\",\n             y = \"Temp\",\n             color = \"Month\",\n             width = 600,\n             height = 300,\n            **kwargs)\n\n    # reduce whitespace\n    fig.update_layout(margin={\"r\":0,\"t\":30,\"l\":0,\"b\":10})\n    return fig\n\nNow that we have finished creating our function, we can run our code!\n\nfig = station_temp_plot(\"temps.db\", station_one = \"U_C_L_A\", station_two = \"LOS_ANGELES_DWTN_USC_CAMPUS\",title=\"Temperature Changes Through the Years at UCLA and USC\")\nfig.show()\n\n\n\n\nThrough this plot, we can see the changes in temperatures throughout the year at UCLA and USC. It is clear that it is colder during the in October, November, December, and Janurary while it gets very hot in July for both USC and UCLA. We can see that the trends are typically what we experience in LA. In addition, when we hover over the box plots, we can see information regaring the maximum, median, minimum etc of a specific month."
  },
  {
    "objectID": "posts/bruin/HW1.html#interactive-scatter-plot",
    "href": "posts/bruin/HW1.html#interactive-scatter-plot",
    "title": "SQL Tutorial on Climate",
    "section": "Interactive Scatter Plot",
    "text": "Interactive Scatter Plot\nNow we will be creating an interactive scatter plot! We will again be using the the same code as above to generate our pandas dataframe. In this function (temp_comparison_plot), we want to look at the changes in temperature through the years at UCLA and USC by the seasons. To do so, we will first like to perform some data cleaning. As observed before, our variable Month consisted of 1 to 12, which represents January to December. Since we will like to perform analysis by season, we want to change the months to their respective season. To do so, we will look through the Month column using the .loc function and changing the numbers to the correct season. Now we can generate our plot!\nWe will be using the following code to generate the scatter plot: fig = px.scatter(data_frame = df, x = “Year”, y = “Temp”, color = “Temp”, hover_data = [“LATITUDE”, “LONGITUDE”], size_max = 8, width = 500, height = 300, opacity = 0.5, facet_col = “Month”, facet_row = “NAME”, **kwargs) - data_frame: dataframe used to create the plot - x: column for our x-axis - y: column for our y-axis - color: column corresponding to the color of the points - hover_data: adding more information for the user to see when they hover over a specific point on the plot - facet_col: splitting the plots columnwise using a specific column from the dataframe - facet_row: splitting the plots rowwise using a specific column from the dataframe\n\ndef temp_comparison_plot(db_file, station_one, station_two, **kwargs) :\n    \"\"\"\n    Creating an interactive scatter plot to look at the changes in temperature through the years at two stations by seasons\n    Function will:\n    1. Use station_climate_comparison_database to create a dataframe that contains all the information needed\n    2. Change the specific months into its corresponding season\n    3. Create the interactive scatter plot\n    \"\"\"\n    # using parameters from temp_comparison_plot into station_climate_comparison_database\n    df = station_climate_comparison_database(db_file, station_one, station_two)\n    \n    # replacing the month's to the season they belong in\n    df.loc[(df['Month']==1) | (df['Month']==2) | (df['Month']==12), 'Month'] = \"Winter\" \n    df.loc[(df['Month']==3) | (df['Month']==4) | (df['Month']==5), 'Month'] = \"Spring\"\n    df.loc[(la['Month']==6) | (df['Month']==7) | (df['Month']==8), 'Month'] = \"Summer\"\n    df.loc[(la['Month']==9) | (df['Month']==10) | (df['Month']==11), 'Month'] = \"Fall\" \n\n    # creating an interactive scatter plot (more information regarding arguments above)\n    fig = px.scatter(data_frame = df,\n                 x = \"Year\",\n                 y = \"Temp\",\n                 color = \"Temp\",\n                 hover_data = [\"LATITUDE\", \"LONGITUDE\"],\n                 size_max = 8,\n                 width = 500,\n                 height = 300,\n                 opacity = 0.5,\n                 facet_col = \"Month\",\n                 facet_row = \"NAME\",\n                 **kwargs)\n\n    #reduce whitespace\n    fig.update_layout(margin={\"r\":0, \"t\":80, \"l\":0, \"b\":0})\n    return fig\n\nNow that we have finished creating our function, lets run it!\n\nfig = temp_comparison_plot(\"temps.db\", station_one = \"U_C_L_A\", station_two = \"LOS_ANGELES_DWTN_USC_CAMPUS\",title=\"Temperature Trends Through the Years by Season &lt;br&gt;in UCLA and USC\")\nfig.show()\n\n\n\n\nFrom the output, we can see the changes in temperature throughout the years. We see that all seasons experienced an increase in temperature from the earliest 1901 to 2021. In addition, UCLA and USC seem to follow the same trends."
  },
  {
    "objectID": "posts/bruin/HW6-2.html",
    "href": "posts/bruin/HW6-2.html",
    "title": "Text Classification",
    "section": "",
    "text": "In this blog, I will be covering how to create a fake news classifier using the library Keras. We will be using Google Colab for this demonstration.\nFirst, let’s begin by importing all the necessary libraries.\nThe current version of Keras we have is 2.15. However, for this text classification model, we would like to use Keras 3. To do so, let’s update the library!\n\n!pip install keras --upgrade\n\nRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.1.1)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.25.2)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)\nRequirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.7)\nRequirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.9.0)\nRequirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.10.0)\nRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.2.0)\nRequirement already satisfied: typing-extensions&gt;=4.0.0 in /usr/local/lib/python3.10/dist-packages (from optree-&gt;keras) (4.10.0)\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (3.0.0)\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (2.16.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;keras) (0.1.2)\n\n\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport re\nimport string\nimport keras\nfrom keras import utils, models, layers\nfrom keras.layers import TextVectorization\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport plotly.io as pio\npio.templates.default = \"plotly_white\"\n\n\nprint(keras.__version__)\n\n3.1.1\n\n\nWe can see that we now have Keras 3 imported!\nAfter you are done, we can now import the dataset. We will be using data from the following github: “https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true”. Make sure to pass in the dataset using pd.read_csv.\n\ntrain_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true\"\ndata = pd.read_csv(train_url)\ndata.head()\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\ntitle\ntext\nfake\n\n\n\n\n0\n17366\nMerkel: Strong result for Austria's FPO 'big c...\nGerman Chancellor Angela Merkel said on Monday...\n0\n\n\n1\n5634\nTrump says Pence will lead voter fraud panel\nWEST PALM BEACH, Fla.President Donald Trump sa...\n0\n\n\n2\n17487\nJUST IN: SUSPECTED LEAKER and “Close Confidant...\nOn December 5, 2017, Circa s Sara Carter warne...\n1\n\n\n3\n12217\nThyssenkrupp has offered help to Argentina ove...\nGermany s Thyssenkrupp, has offered assistance...\n0\n\n\n4\n5535\nTrump say appeals court decision on travel ban...\nPresident Donald Trump on Thursday called the ...\n0\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nWhen we first read in the data, we see that there is an extra column in addition to title, text, and fake. We can simply remove that column because that is not important and will not give us any helpful information.\n\ndata  = data[[\"title\", \"text\", \"fake\"]]\ndata.head()\n\n\n  \n    \n\n\n\n\n\n\ntitle\ntext\nfake\n\n\n\n\n0\nMerkel: Strong result for Austria's FPO 'big c...\nGerman Chancellor Angela Merkel said on Monday...\n0\n\n\n1\nTrump says Pence will lead voter fraud panel\nWEST PALM BEACH, Fla.President Donald Trump sa...\n0\n\n\n2\nJUST IN: SUSPECTED LEAKER and “Close Confidant...\nOn December 5, 2017, Circa s Sara Carter warne...\n1\n\n\n3\nThyssenkrupp has offered help to Argentina ove...\nGermany s Thyssenkrupp, has offered assistance...\n0\n\n\n4\nTrump say appeals court decision on travel ban...\nPresident Donald Trump on Thursday called the ...\n0\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nWe can now only see title, text, and fake on our dataset!\nNext, let’s create a function to clean up our dataset. We want to get ride of all the stopword’s in our data as well as make all the letter’s lowercase. We want to also return a tf.data.Dataset, which consists of two inputs and one output. The two inputs should be title and text, while the output should be fake. In the function, you can see that we also included batch. Batch will help increase the speed of training especially since we are working with a large dataset.\n\nimport tensorflow as tf\nimport nltk\nfrom nltk.corpus import stopwords\n\nnltk.download('stopwords')\nstop = stopwords.words('english')\n\ndef make_dataset(input_data) :\n  \"\"\"\n  Function will make all text lowercase\n  Removes stopwords from title and text\n  Returns a tf.data.Dataset with two inputs and one output\n  \"\"\"\n  input_data['title_wo_stopwords'] = input_data['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop]))\n  input_data['text_wo_stopwords'] = input_data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop]))\n\n  data = tf.data.Dataset.from_tensor_slices(({\"title\": input_data['title_wo_stopwords'],\n                                              \"text\": input_data['text_wo_stopwords']},\n                                              {\"fake\": input_data['fake']}))\n\n  data = data.map(lambda x, y: ((tf.strings.lower(x[\"title\"]), tf.strings.lower(x[\"text\"])), y))  # Lowercasing the text\n\n  data = data.batch(100)\n\n  return data\n\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n\n\n\ndata = make_dataset(data)\n\nAfter we have passed our dataset through the function we have just created, we can split our data for training. We will leave 20% of the data to be used for validation and the remaining 80% for training.\n\ndata = data.shuffle(buffer_size = len(data), reshuffle_each_iteration=False)\ntrain_size = int(0.8*len(data))\nval_size   = int(0.2*len(data))\n\ntrain = data.take(train_size)\nval   = data.skip(train_size).take(val_size)\n\nlen(train), len(val)\n\n(180, 45)\n\n\nNext, let us look at base rate. We can calculate the base rate by looking at the number of fake and real articles in our dataset. To do so, let’s create a for loop.\n\n# Counting real and fake articles\nreal = 0\nfake = 0\n\nfor batch in train.take(-1):\n    # Getting the labels\n    labels = batch[1]\n\n    for label in labels[\"fake\"].numpy():\n        # If label is 0, real article\n        if label == 0:\n            real += 1\n        # If label is 1, fake news\n        elif label == 1:\n            fake += 1\n\nprint(\"Real title/text:\", real)\nprint(\"Fake title/text:\", fake)\n\nReal title/text: 8534\nFake title/text: 9415\n\n\nWe see that we have a total of 8588 real articles and 9361 fake articles. This means that the base rate is approximately 50%. Let’s see if we can create models to bring the accurate up from 50%!\n\nText Vectorization\nWe will be using the following code to prepare a text vectorization layer for the tensorflow models.\n\nimport tensorflow as tf\nimport re\nimport string\n\nsize_vocabulary = 2000\n\ndef standardization(input_data):\n  \"\"\"\n  cleaning the data making all text lower and removing special characters\n  \"\"\"\n  lowercase = tf.strings.lower(input_data)\n  no_punctuation = tf.strings.regex_replace(lowercase,\n                                  '[%s]' % re.escape(string.punctuation),'')\n  return no_punctuation\n\ntitle_vectorize_layer = tf.keras.layers.TextVectorization(\n  standardize=standardization,\n  max_tokens=size_vocabulary, # only consider this many words\n  output_mode='int',\n  output_sequence_length=500)\n\ntitle_vectorize_layer.adapt(train.map(lambda x, y: x[0]))\n\n\n\nModel 1\nLet’s start by creating a model using only the article title as an input. We will be utilizing the text vecorization created above.\n\nmodel_title_only = tf.keras.Sequential([\n    title_vectorize_layer,\n    layers.Embedding(input_dim=size_vocabulary, output_dim=64),\n    layers.GlobalAveragePooling1D(),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(1, activation='sigmoid', name = \"fake\")\n])\n\ntrain_combined = train.map(lambda x, y: (x[0], y))\nval_combined = val.map(lambda x, y: (x[0], y))\n\n\nmodel_title_only.compile(optimizer='adam',\n                          loss='binary_crossentropy',\n                          metrics=['accuracy'])\n\nhistory_title_only = model_title_only.fit(train_combined, epochs=20, validation_data=val_combined)\n\nEpoch 1/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 8s 31ms/step - accuracy: 0.5080 - loss: 0.6935 - val_accuracy: 0.5260 - val_loss: 0.6865\nEpoch 2/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 25ms/step - accuracy: 0.5580 - loss: 0.6814 - val_accuracy: 0.8642 - val_loss: 0.6026\nEpoch 3/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 33ms/step - accuracy: 0.7476 - loss: 0.5584 - val_accuracy: 0.8278 - val_loss: 0.3927\nEpoch 4/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 10s 34ms/step - accuracy: 0.8341 - loss: 0.3847 - val_accuracy: 0.9282 - val_loss: 0.2560\nEpoch 5/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 27ms/step - accuracy: 0.8979 - loss: 0.2754 - val_accuracy: 0.9416 - val_loss: 0.2019\nEpoch 6/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 10s 24ms/step - accuracy: 0.9268 - loss: 0.2091 - val_accuracy: 0.9462 - val_loss: 0.1740\nEpoch 7/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 8s 39ms/step - accuracy: 0.9403 - loss: 0.1723 - val_accuracy: 0.9580 - val_loss: 0.1488\nEpoch 8/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 8s 27ms/step - accuracy: 0.9480 - loss: 0.1465 - val_accuracy: 0.9698 - val_loss: 0.1173\nEpoch 9/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 24ms/step - accuracy: 0.9559 - loss: 0.1245 - val_accuracy: 0.9704 - val_loss: 0.0981\nEpoch 10/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 25ms/step - accuracy: 0.9605 - loss: 0.1112 - val_accuracy: 0.9698 - val_loss: 0.0914\nEpoch 11/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 23ms/step - accuracy: 0.9607 - loss: 0.1129 - val_accuracy: 0.9558 - val_loss: 0.1126\nEpoch 12/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 23ms/step - accuracy: 0.9591 - loss: 0.1119 - val_accuracy: 0.9540 - val_loss: 0.1194\nEpoch 13/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 24ms/step - accuracy: 0.9567 - loss: 0.1164 - val_accuracy: 0.9469 - val_loss: 0.1359\nEpoch 14/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 10s 25ms/step - accuracy: 0.9539 - loss: 0.1247 - val_accuracy: 0.9767 - val_loss: 0.0773\nEpoch 15/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 22ms/step - accuracy: 0.9615 - loss: 0.1021 - val_accuracy: 0.9687 - val_loss: 0.0977\nEpoch 16/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 23ms/step - accuracy: 0.9673 - loss: 0.0919 - val_accuracy: 0.9620 - val_loss: 0.1101\nEpoch 17/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 26ms/step - accuracy: 0.9671 - loss: 0.0928 - val_accuracy: 0.9113 - val_loss: 0.1963\nEpoch 18/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 23ms/step - accuracy: 0.9558 - loss: 0.1206 - val_accuracy: 0.9758 - val_loss: 0.0695\nEpoch 19/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 28ms/step - accuracy: 0.9723 - loss: 0.0807 - val_accuracy: 0.9791 - val_loss: 0.0658\nEpoch 20/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 24ms/step - accuracy: 0.9713 - loss: 0.0815 - val_accuracy: 0.9773 - val_loss: 0.0663\n\n\nLet’s generate a summary of our model.\n\nmodel_title_only.summary()\n\nModel: \"sequential\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ text_vectorization                   │ (None, 500)                 │               0 │\n│ (TextVectorization)                  │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ embedding (Embedding)                │ (None, 500, 64)             │         128,000 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling1d             │ (None, 64)                  │               0 │\n│ (GlobalAveragePooling1D)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (Dense)                        │ (None, 64)                  │           4,160 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ fake (Dense)                         │ (None, 1)                   │              65 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 396,677 (1.51 MB)\n\n\n\n Trainable params: 132,225 (516.50 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n Optimizer params: 264,452 (1.01 MB)\n\n\n\nLooking at the plot below, we can see that the accuracy of our model is really high! However, in the previous blog, remember that if the training accuracy is higher than the validation accuracy, there is a possible of overfitting in our model. Let’s attempt a different model to see if overfitting will not occur.\n\nplt.plot(history_title_only.history[\"accuracy\"], label = \"training\")\nplt.plot(history_title_only.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\nLet’s also create a visualization of our model so we can see exactly the layers and the shapes.\n\nutils.plot_model(model_title_only, \"output_filename.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\n\n\nModel 2\nNow, since we only used title for our previous model, let’s try to only use text for our second model. We will use the same text vectorization method as we did before.\n\ntext_vectorize_layer = tf.keras.layers.TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary, # only consider this many words\n    output_mode='int',\n    output_sequence_length=500)\n\ntext_vectorize_layer.adapt(train.map(lambda x, y: x[1]))\n\nNow we can start building our model using only text.\n\nmodel_text_only = tf.keras.Sequential([\n    text_vectorize_layer,\n    layers.Embedding(input_dim=size_vocabulary, output_dim=64),\n    layers.GlobalAveragePooling1D(),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(1, activation='sigmoid', name = \"fake\")\n])\n\ntrain_combined = train.map(lambda x, y: (x[1], y))\nval_combined = val.map(lambda x, y: (x[1], y))\n\n\nmodel_text_only.compile(optimizer='adam',\n                          loss='binary_crossentropy',\n                          metrics=['accuracy'])\n\nhistory_text_only = model_text_only.fit(train_combined, epochs=20, validation_data=val_combined)\n\nEpoch 1/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 39ms/step - accuracy: 0.6653 - loss: 0.6015 - val_accuracy: 0.9276 - val_loss: 0.2148\nEpoch 2/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 32ms/step - accuracy: 0.9300 - loss: 0.2042 - val_accuracy: 0.9598 - val_loss: 0.1350\nEpoch 3/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 10s 33ms/step - accuracy: 0.9446 - loss: 0.1528 - val_accuracy: 0.9660 - val_loss: 0.1112\nEpoch 4/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 10s 32ms/step - accuracy: 0.9499 - loss: 0.1334 - val_accuracy: 0.9709 - val_loss: 0.0977\nEpoch 5/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 11s 39ms/step - accuracy: 0.9536 - loss: 0.1204 - val_accuracy: 0.9744 - val_loss: 0.0878\nEpoch 6/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 10s 37ms/step - accuracy: 0.9572 - loss: 0.1104 - val_accuracy: 0.9773 - val_loss: 0.0805\nEpoch 7/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 10s 38ms/step - accuracy: 0.9607 - loss: 0.1018 - val_accuracy: 0.9787 - val_loss: 0.0761\nEpoch 8/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 35ms/step - accuracy: 0.9636 - loss: 0.0943 - val_accuracy: 0.9807 - val_loss: 0.0755\nEpoch 9/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 38ms/step - accuracy: 0.9662 - loss: 0.0882 - val_accuracy: 0.9787 - val_loss: 0.0767\nEpoch 10/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 32ms/step - accuracy: 0.9678 - loss: 0.0843 - val_accuracy: 0.9636 - val_loss: 0.0784\nEpoch 11/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 8s 41ms/step - accuracy: 0.9682 - loss: 0.0816 - val_accuracy: 0.9629 - val_loss: 0.0792\nEpoch 12/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 33ms/step - accuracy: 0.9696 - loss: 0.0798 - val_accuracy: 0.9660 - val_loss: 0.0744\nEpoch 13/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 35ms/step - accuracy: 0.9687 - loss: 0.0801 - val_accuracy: 0.9796 - val_loss: 0.0702\nEpoch 14/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 37ms/step - accuracy: 0.9675 - loss: 0.0832 - val_accuracy: 0.9809 - val_loss: 0.0666\nEpoch 15/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 30ms/step - accuracy: 0.9669 - loss: 0.0875 - val_accuracy: 0.9791 - val_loss: 0.0638\nEpoch 16/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 35ms/step - accuracy: 0.9685 - loss: 0.0811 - val_accuracy: 0.9636 - val_loss: 0.0902\nEpoch 17/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 30ms/step - accuracy: 0.9709 - loss: 0.0720 - val_accuracy: 0.9529 - val_loss: 0.1136\nEpoch 18/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 11s 32ms/step - accuracy: 0.9752 - loss: 0.0641 - val_accuracy: 0.9682 - val_loss: 0.0829\nEpoch 19/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 10s 31ms/step - accuracy: 0.9712 - loss: 0.0737 - val_accuracy: 0.9558 - val_loss: 0.0911\nEpoch 20/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 11s 32ms/step - accuracy: 0.9795 - loss: 0.0593 - val_accuracy: 0.9609 - val_loss: 0.0818\n\n\nNow that we have completed our model, let’s output the summary.\n\nmodel_text_only.summary()\n\nModel: \"sequential_1\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ text_vectorization_1                 │ (None, 500)                 │               0 │\n│ (TextVectorization)                  │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ embedding_1 (Embedding)              │ (None, 500, 64)             │         128,000 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling1d_1           │ (None, 64)                  │               0 │\n│ (GlobalAveragePooling1D)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (Dense)                      │ (None, 64)                  │           4,160 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ fake (Dense)                         │ (None, 1)                   │              65 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 396,677 (1.51 MB)\n\n\n\n Trainable params: 132,225 (516.50 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n Optimizer params: 264,452 (1.01 MB)\n\n\n\nThe accuracy of our model is still very high with accuracy close to 98%. However, in the plot, we can still observe overfitting. Let’s try a different model to see if we can prevent overfitting from occuring.\n\nplt.plot(history_text_only.history[\"accuracy\"], label = \"training\")\nplt.plot(history_text_only.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\nLet’s also create a visualization of our model so we can see all the different layers involved.\n\nutils.plot_model(model_text_only, \"output_filename.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\n\n\nModel 3\nLast but not least, let’s create a model that uses both title and text to predict fake news.\n\n# Define title and text inputs\ntitle_input = tf.keras.Input(shape=(1,), name=\"title\", dtype=\"string\")\ntext_input = tf.keras.Input(shape=(1,), name=\"text\", dtype=\"string\")\n\ntitle_features = title_vectorize_layer(title_input)\ntext_features = text_vectorize_layer(text_input)\n\ntitle_features = layers.Embedding(size_vocabulary, 3)(title_features)\ntext_features = layers.Embedding(size_vocabulary, 3)(text_features)\n\ntitle_features = layers.Dropout(0.2)(title_features)\ntext_features = layers.Dropout(0.2)(text_features)\n\ntitle_features = layers.GlobalAveragePooling1D()(title_features)\ntext_features = layers.GlobalAveragePooling1D()(text_features)\n\ntitle_features = layers.Dropout(0.2)(title_features)\ntext_features = layers.Dropout(0.2)(text_features)\n\ntitle_features = layers.Dense(32, activation=\"relu\")(title_features)\ntext_features = layers.Dense(32, activation=\"relu\")(text_features)\n\nx = layers.concatenate([title_features, text_features], axis=1)\n\nfake_pred = layers.Dense(1, activation=\"sigmoid\", name=\"fake\")(x)\n\ncombine_model = tf.keras.Model(\n    inputs=[title_input, text_input],\n    outputs=fake_pred\n)\n\n\n# Compile the model\ncombine_model.compile(optimizer='adam',\n                          loss='binary_crossentropy',\n                          metrics=['accuracy'])\n\n# Fit the model\nhistory_combine = combine_model.fit(train, epochs=20, validation_data=val)\n\nEpoch 1/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 10s 39ms/step - accuracy: 0.5234 - loss: 0.6898 - val_accuracy: 0.8536 - val_loss: 0.6300\nEpoch 2/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 31ms/step - accuracy: 0.8328 - loss: 0.5673 - val_accuracy: 0.9500 - val_loss: 0.3295\nEpoch 3/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 33ms/step - accuracy: 0.9243 - loss: 0.3150 - val_accuracy: 0.9462 - val_loss: 0.2080\nEpoch 4/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 11s 36ms/step - accuracy: 0.9367 - loss: 0.2247 - val_accuracy: 0.9562 - val_loss: 0.1614\nEpoch 5/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 10s 36ms/step - accuracy: 0.9462 - loss: 0.1847 - val_accuracy: 0.9564 - val_loss: 0.1407\nEpoch 6/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 32ms/step - accuracy: 0.9493 - loss: 0.1666 - val_accuracy: 0.9467 - val_loss: 0.1406\nEpoch 7/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 10s 30ms/step - accuracy: 0.9527 - loss: 0.1533 - val_accuracy: 0.9642 - val_loss: 0.1173\nEpoch 8/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 10s 32ms/step - accuracy: 0.9596 - loss: 0.1408 - val_accuracy: 0.9524 - val_loss: 0.1214\nEpoch 9/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 11s 35ms/step - accuracy: 0.9615 - loss: 0.1271 - val_accuracy: 0.9707 - val_loss: 0.0984\nEpoch 10/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 11s 38ms/step - accuracy: 0.9663 - loss: 0.1185 - val_accuracy: 0.9753 - val_loss: 0.0865\nEpoch 11/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 30ms/step - accuracy: 0.9683 - loss: 0.1085 - val_accuracy: 0.9778 - val_loss: 0.0780\nEpoch 12/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 10s 29ms/step - accuracy: 0.9728 - loss: 0.0976 - val_accuracy: 0.9804 - val_loss: 0.0693\nEpoch 13/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 11s 31ms/step - accuracy: 0.9738 - loss: 0.0880 - val_accuracy: 0.9827 - val_loss: 0.0634\nEpoch 14/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 11s 34ms/step - accuracy: 0.9758 - loss: 0.0804 - val_accuracy: 0.9840 - val_loss: 0.0575\nEpoch 15/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 11s 37ms/step - accuracy: 0.9767 - loss: 0.0731 - val_accuracy: 0.9851 - val_loss: 0.0544\nEpoch 16/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 11s 41ms/step - accuracy: 0.9784 - loss: 0.0673 - val_accuracy: 0.9869 - val_loss: 0.0472\nEpoch 17/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 30ms/step - accuracy: 0.9807 - loss: 0.0593 - val_accuracy: 0.9702 - val_loss: 0.0695\nEpoch 18/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 11s 31ms/step - accuracy: 0.9815 - loss: 0.0577 - val_accuracy: 0.9796 - val_loss: 0.0528\nEpoch 19/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 11s 36ms/step - accuracy: 0.9813 - loss: 0.0553 - val_accuracy: 0.9827 - val_loss: 0.0471\nEpoch 20/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 10s 37ms/step - accuracy: 0.9817 - loss: 0.0537 - val_accuracy: 0.9893 - val_loss: 0.0351\n\n\nWow! We can see that the accuracy of this model is close to 100%! We can see that the issue with overfitting has been greatly improve using this model.\n\nplt.plot(history_combine.history[\"accuracy\"], label = \"training\")\nplt.plot(history_combine.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\nLet’s create a visualization of our model to see all the layers involved. This one is more complex due to the use of two inputs.\n\nutils.plot_model(combine_model, \"output_filename.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\n\n\nModel Evaluation\nSince the best performing model is the one using both title and text, let’s try to run our testing dataset.\nWe will import the dataset from the following github: “https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true”. Make sure to clean the dataset to get rid of any unhelpful columns.\n\ntest_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true\"\ntest = pd.read_csv(test_url)\ntest = test[[\"title\", \"text\", \"fake\"]]\ntest.head()\n\n\n  \n    \n\n\n\n\n\n\ntitle\ntext\nfake\n\n\n\n\n0\nCNN And MSNBC Destroy Trump, Black Out His Fa...\nDonald Trump practically does something to cri...\n1\n\n\n1\nExclusive: Kremlin tells companies to deliver ...\nThe Kremlin wants good news. The Russian lead...\n0\n\n\n2\nGolden State Warriors Coach Just WRECKED Trum...\nOn Saturday, the man we re forced to call Pre...\n1\n\n\n3\nPutin opens monument to Stalin's victims, diss...\nPresident Vladimir Putin inaugurated a monumen...\n0\n\n\n4\nBREAKING: DNC HACKER FIRED For Bank Fraud…Blam...\nApparently breaking the law and scamming the g...\n1\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nRemember the function we created a while ago, make sure to use that function to clean our testing dataset.\n\ndata2 = make_dataset(test)\n\nNext, we will be using evaluate() to evaluate how our model does on the testing dataset. We can see that the accuracy is still very high with 98.72% accuracy!\n\ncombine_model.evaluate(data2)\n\n225/225 ━━━━━━━━━━━━━━━━━━━━ 3s 13ms/step - accuracy: 0.9872 - loss: 0.0430\n\n\n[0.04207614064216614, 0.9867254495620728]\n\n\n\n\nEmbedding Visualization\nThe point of embedding is to place words on certain vectors and place words with similar meanings in close proximity to each other. The following code generates a dataframe of the position of each word. We can use the following dataframe to plot the words and see its relationship with other words.\n\nfrom sklearn.decomposition import PCA\nweights = combine_model.get_layer('embedding_2').get_weights()[0] # get the weights from the embedding layer\nvocab = title_vectorize_layer.get_vocabulary()                # get the vocabulary from our data prep for later\n\npca = PCA(n_components=2)\nweights = pca.fit_transform(weights)\n\nembedding_df = pd.DataFrame({\n    'word' : vocab,\n    'x0'   : weights[:,0],\n    'x1'   : weights[:,1]\n})\nembedding_df\n\n\n  \n    \n\n\n\n\n\n\nword\nx0\nx1\n\n\n\n\n0\n\n-0.455995\n-0.036738\n\n\n1\n[UNK]\n6.431587\n0.031929\n\n\n2\ntrump\n-0.263259\n0.037156\n\n\n3\nvideo\n14.122568\n0.150382\n\n\n4\nto\n11.734525\n0.069527\n\n\n...\n...\n...\n...\n\n\n1995\nconsumer\n-0.887943\n-0.052701\n\n\n1996\nconsider\n-1.383336\n0.009815\n\n\n1997\ncompletely\n-0.091579\n-0.095368\n\n\n1998\nclosed\n-0.439425\n-0.005176\n\n\n1999\ncbs\n-0.270297\n-0.099974\n\n\n\n\n2000 rows × 3 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nBelow, let’s generate an interactive plot that will allow people to hover over a dot and look at the words to see the associations with each other. For example, if you hover over the point x0 = -1.689893 and x1 = 0.2999115, you will encounter the word illegal. If you move your cursor around illegal, you will find other words like shrieff, holds, undercover, fighting, criticizes, and record. These words are greatly related to each other which shows that our embedding is working properly. We often see these words in articles related to crimes or the government. Feel free to play around the plot to find other associations!\n\nimport plotly.express as px\nfig = px.scatter(embedding_df,\n                 x = \"x0\",\n                 y = \"x1\",\n                 size = list(np.ones(len(embedding_df))),\n                 size_max = 3,\n                 hover_name = \"word\")\n\nfig.show()\n\n\n\n\n                                \n                                            \n\n\n\n\n\n\n\nfig.png\n\n\n\n\nFinal Takeaway\nThat is all of creating a fake news classifier using Keras. Hope you found this tutorial interesting!"
  },
  {
    "objectID": "posts/bruin/HW5.html",
    "href": "posts/bruin/HW5.html",
    "title": "Image Classification",
    "section": "",
    "text": "In this blog post, you will be learning how to create different image classification modles in Keras to classify cats and dogs.\nTo do so, let us first import the important libraries to build our models!\n\nimport os\nimport keras\nfrom keras import utils, datasets, layers, models\nimport tensorflow_datasets as tfds\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport random\n\nAfter importing all libarires, we will be using the tensorflow_datasets to retrieve the dataset on cats vs dogs for this example. Many code in the blog was inspired by the following website. If you would like to learn more, please reference the following link: https://www.tensorflow.org/tutorials/images/transfer_learning.\nWe will be spliting the data by 40% for training, 10% for validation, and 10% for the test. The rest of the dataset will not be used.\n\ntrain_ds, validation_ds, test_ds = tfds.load(\n    \"cats_vs_dogs\",\n    # 40% for training, 10% for validation, and 10% for test (the rest unused)\n    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n    as_supervised=True,  # Include labels\n)\n\nprint(f\"Number of training samples: {train_ds.cardinality()}\")\nprint(f\"Number of validation samples: {validation_ds.cardinality()}\")\nprint(f\"Number of test samples: {test_ds.cardinality()}\")\n\nDownloading and preparing dataset 786.67 MiB (download: 786.67 MiB, generated: 1.04 GiB, total: 1.81 GiB) to /root/tensorflow_datasets/cats_vs_dogs/4.0.1...\nDataset cats_vs_dogs downloaded and prepared to /root/tensorflow_datasets/cats_vs_dogs/4.0.1. Subsequent calls will reuse this data.\nNumber of training samples: 9305\nNumber of validation samples: 2326\nNumber of test samples: 2326\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWARNING:absl:1738 images were corrupted and were skipped\n\n\n\n\n\nIn the following code chunk, we will be resizing the images. This is to allow for a constant size between all images, as some images have different sizes.\n\nresize_fn = keras.layers.Resizing(150, 150)\n\ntrain_ds = train_ds.map(lambda x, y: (resize_fn(x), y))\nvalidation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y))\ntest_ds = test_ds.map(lambda x, y: (resize_fn(x), y))\n\nSince we are working with very big datasets, we will be using the following code chunk to speed up the time for reading the data.\n\nfrom tensorflow import data as tf_data\nbatch_size = 64 # amount of data points pulled from directory\n\ntrain_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\nvalidation_ds = validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\ntest_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\n\nLet’s experiment with how we can work with the data. Specifically, how can we see what images we are using. Let us define a function that will create two-row visualization. The first row will consist of three random pictures of cats from our dataset. The second row will consist of three random random pictures of dogs!\n\ndef visualize(dataset, n) :\n  fig, axes = plt.subplots(2, 3, figsize=(10, 10)) # setting the plot grids\n\n  for i, (images, labels) in enumerate(dataset.take(n)):\n      ncats = 0 # current number of cats on plot\n      ndogs = 0 # current number of dogs on plot\n      for image, label in zip(images, labels):\n          if label == 0 and ncats &lt; 3:\n              axes[0, ncats].imshow(image.numpy().astype(\"uint8\")) # adding cat\n              axes[0, ncats].set_title('Cat')\n              axes[0, ncats].axis(\"off\")\n              ncats = ncats + 1\n          elif label == 1 and ndogs &lt; 3:\n              axes[1, ndogs].imshow(image.numpy().astype(\"uint8\")) # adding dog\n              axes[1, ndogs].set_title('Dog')\n              axes[1, ndogs].axis(\"off\")\n              ndogs = ndogs + 1\n\n  plt.show()\n\n\nvisualize(train_ds, random.randint(1,64))\n\n\n\n\n\n\n\n\n\nCheck Label Frequencies\n\nlabels_iterator= train_ds.unbatch().map(lambda image, label: label).as_numpy_iterator()\n\nLet’s look at the number of dogs and cats in our dataset. The label 0 corresponds to a cat, while label 1 corresponds to a dog. By figuring out the number of cats and dogs, we can have a general idea of our baseline machine learning model.\n\nn_cat = 0\nn_dog = 0\n\nfor label in labels_iterator:\n    if label == 0:  # Cat label\n        n_cat = n_cat + 1\n    elif label == 1:  # Dog label\n        n_dog = n_dog + 1\n\nprint(\"Num of cat:\", n_cat)\nprint(\"Number of dog:\", n_dog)\n\nNum of cat: 4637\nNumber of dog: 4668\n\n\nLooking at the results, we see that there seems to be a 50% accuracy Since the amount of dogs is 50.17% and the amount of cats is 49.83%. Therefore, we should aim for the following models we create to get an accuracy over 50%.\n\n\nFirst Model: Keras Squential\nLet’s create out first model using keras.Squential. We will be using 3 Conv2D layers, 2 MaxPooling2D layers, 1 Flatten layer, 2 Dense layers, and 1 Dropout layer.\n\nmodel1 = models.Sequential([\n    layers.Input((150, 150, 3)),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(10),\n    layers.Dropout(0.5)\n])\n\nWe have completed our first model! Let’s take a look at our summary.\n\nmodel1.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 148, 148, 32)      896       \n                                                                 \n max_pooling2d (MaxPooling2  (None, 74, 74, 32)        0         \n D)                                                              \n                                                                 \n conv2d_1 (Conv2D)           (None, 72, 72, 32)        9248      \n                                                                 \n max_pooling2d_1 (MaxPoolin  (None, 36, 36, 32)        0         \n g2D)                                                            \n                                                                 \n conv2d_2 (Conv2D)           (None, 34, 34, 64)        18496     \n                                                                 \n flatten (Flatten)           (None, 73984)             0         \n                                                                 \n dense (Dense)               (None, 64)                4735040   \n                                                                 \n dense_1 (Dense)             (None, 10)                650       \n                                                                 \n dropout (Dropout)           (None, 10)                0         \n                                                                 \n=================================================================\nTotal params: 4764330 (18.17 MB)\nTrainable params: 4764330 (18.17 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\nmodel1.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\n\nhistory = model1.fit(train_ds,\n                    epochs=20,\n                    validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 17s 69ms/step - loss: 9.6262 - accuracy: 0.5069 - val_loss: 0.6614 - val_accuracy: 0.6307\nEpoch 2/20\n146/146 [==============================] - 4s 31ms/step - loss: 1.3889 - accuracy: 0.5560 - val_loss: 0.6517 - val_accuracy: 0.6350\nEpoch 3/20\n146/146 [==============================] - 4s 30ms/step - loss: 1.3208 - accuracy: 0.5940 - val_loss: 0.6425 - val_accuracy: 0.6664\nEpoch 4/20\n146/146 [==============================] - 4s 30ms/step - loss: 1.2623 - accuracy: 0.6261 - val_loss: 0.6331 - val_accuracy: 0.6905\nEpoch 5/20\n146/146 [==============================] - 4s 31ms/step - loss: 1.1927 - accuracy: 0.6688 - val_loss: 0.6317 - val_accuracy: 0.7072\nEpoch 6/20\n146/146 [==============================] - 4s 30ms/step - loss: 1.1432 - accuracy: 0.7030 - val_loss: 0.6756 - val_accuracy: 0.6956\nEpoch 7/20\n146/146 [==============================] - 4s 31ms/step - loss: 1.1319 - accuracy: 0.7094 - val_loss: 0.7739 - val_accuracy: 0.7025\nEpoch 8/20\n146/146 [==============================] - 5s 31ms/step - loss: 1.1219 - accuracy: 0.7193 - val_loss: 0.8208 - val_accuracy: 0.6883\nEpoch 9/20\n146/146 [==============================] - 5s 32ms/step - loss: 1.0819 - accuracy: 0.7387 - val_loss: 0.7523 - val_accuracy: 0.7008\nEpoch 10/20\n146/146 [==============================] - 5s 35ms/step - loss: 1.0193 - accuracy: 0.7666 - val_loss: 0.8239 - val_accuracy: 0.7008\nEpoch 11/20\n146/146 [==============================] - 5s 34ms/step - loss: 1.0131 - accuracy: 0.7785 - val_loss: 0.8924 - val_accuracy: 0.6991\nEpoch 12/20\n146/146 [==============================] - 5s 31ms/step - loss: 0.9589 - accuracy: 0.8035 - val_loss: 1.0549 - val_accuracy: 0.7038\nEpoch 13/20\n146/146 [==============================] - 5s 31ms/step - loss: 0.9469 - accuracy: 0.8128 - val_loss: 1.0034 - val_accuracy: 0.6965\nEpoch 14/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.9401 - accuracy: 0.8145 - val_loss: 1.0947 - val_accuracy: 0.6896\nEpoch 15/20\n146/146 [==============================] - 5s 31ms/step - loss: 0.9230 - accuracy: 0.8272 - val_loss: 1.2134 - val_accuracy: 0.6810\nEpoch 16/20\n146/146 [==============================] - 5s 31ms/step - loss: 0.9295 - accuracy: 0.8182 - val_loss: 1.2677 - val_accuracy: 0.6870\nEpoch 17/20\n146/146 [==============================] - 5s 31ms/step - loss: 0.9555 - accuracy: 0.8219 - val_loss: 1.3564 - val_accuracy: 0.6788\nEpoch 18/20\n146/146 [==============================] - 4s 31ms/step - loss: 0.9434 - accuracy: 0.8216 - val_loss: 1.3587 - val_accuracy: 0.6745\nEpoch 19/20\n146/146 [==============================] - 5s 31ms/step - loss: 0.9548 - accuracy: 0.8208 - val_loss: 1.3813 - val_accuracy: 0.6690\nEpoch 20/20\n146/146 [==============================] - 4s 31ms/step - loss: 0.9582 - accuracy: 0.8212 - val_loss: 1.4138 - val_accuracy: 0.6831\n\n\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\nAccuracy of the model was between 62% to 68%, which is a lot better than the baseline of approximately 50%. Therefore, we performed about 12-18% better than our baseline which is a big improvement! However, look carefully at our plot. We can observe a little bit of overfitting since our training accuracy is much higher than the validation accuracy. Let’s try a different method and see if we can get a higher accuracy and prevent overfitting from occuring.\n\n\nModel with Data Augmentation\nIn this model, we will be generating modifications of the same image for the training dataset. For example, let’s say we have an image of a dog. Even if the dog is flipped upside down or rotated 30 degrees, the image still represents a dog regardless. Therefore, let’s train the model to be able to tell these difference apart.\nLet’s just take a random image in our dataset and make some changes to it!\n\nfor images, labels in train_ds.take(9):\n    image = images[0]\n    label = labels[0]\n\nNext, we will need to perform data augmentation. We want to flip our image horizontally and vertically and also give it a little rotation.\nRemember to also add the image to a batch, or else it will only show up as pixel when trying to plot the original and new copies of the image!\nLet’s also create a plot to see what the image originally looks like and what it looks like after applying the RandomFlip and RandomRotation.\n\ndata_augmentation = tf.keras.Sequential([\n  layers.RandomFlip(\"horizontal_and_vertical\"),\n  layers.RandomRotation(0.2),\n])\n\n# Add the image to a batch.\nimage = tf.cast(tf.expand_dims(image, 0), tf.float32)\n\nplt.figure(figsize=(10, 10))\nfor i in range(3):\n  augmented_image = data_augmentation(image)\n  ax = plt.subplot(1, 3, i + 1)\n  plt.imshow(augmented_image[0].numpy().astype(\"uint8\"))\n  plt.axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\nNow, we are able to apply what we used above and implement it into our new model. We will be adding RandomFlip and RandomRotation into our model and adding as the first two layers.\n\nmodel2 = models.Sequential([\n    layers.RandomFlip(\"horizontal_and_vertical\", input_shape=(150, 150, 3)),  # Horizontal flip augmentation\n    layers.RandomRotation(0.2),  # Random rotation augmentation\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(10), # number of classes\n    layers.Dropout(0.5)\n])\n\nLet’s generate a summary of our model.\n\nmodel2.summary()\n\nModel: \"sequential_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n random_flip_1 (RandomFlip)  (None, 150, 150, 3)       0         \n                                                                 \n random_rotation_1 (RandomR  (None, 150, 150, 3)       0         \n otation)                                                        \n                                                                 \n conv2d_3 (Conv2D)           (None, 148, 148, 32)      896       \n                                                                 \n max_pooling2d_2 (MaxPoolin  (None, 74, 74, 32)        0         \n g2D)                                                            \n                                                                 \n conv2d_4 (Conv2D)           (None, 72, 72, 32)        9248      \n                                                                 \n max_pooling2d_3 (MaxPoolin  (None, 36, 36, 32)        0         \n g2D)                                                            \n                                                                 \n conv2d_5 (Conv2D)           (None, 34, 34, 64)        18496     \n                                                                 \n flatten_1 (Flatten)         (None, 73984)             0         \n                                                                 \n dense_2 (Dense)             (None, 64)                4735040   \n                                                                 \n dense_3 (Dense)             (None, 10)                650       \n                                                                 \n dropout_1 (Dropout)         (None, 10)                0         \n                                                                 \n=================================================================\nTotal params: 4764330 (18.17 MB)\nTrainable params: 4764330 (18.17 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\nmodel2.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nhistory = model2.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 7s 35ms/step - loss: 16.2815 - accuracy: 0.4927 - val_loss: 0.7275 - val_accuracy: 0.5645\nEpoch 2/20\n146/146 [==============================] - 5s 33ms/step - loss: 1.4435 - accuracy: 0.5153 - val_loss: 0.7084 - val_accuracy: 0.6002\nEpoch 3/20\n146/146 [==============================] - 5s 33ms/step - loss: 1.4261 - accuracy: 0.5258 - val_loss: 0.6892 - val_accuracy: 0.6230\nEpoch 4/20\n146/146 [==============================] - 5s 34ms/step - loss: 1.4301 - accuracy: 0.5301 - val_loss: 0.6647 - val_accuracy: 0.6298\nEpoch 5/20\n146/146 [==============================] - 5s 33ms/step - loss: 1.4000 - accuracy: 0.5357 - val_loss: 0.6676 - val_accuracy: 0.6496\nEpoch 6/20\n146/146 [==============================] - 5s 35ms/step - loss: 1.4242 - accuracy: 0.5303 - val_loss: 0.6705 - val_accuracy: 0.6273\nEpoch 7/20\n146/146 [==============================] - 5s 33ms/step - loss: 1.3946 - accuracy: 0.5433 - val_loss: 0.6153 - val_accuracy: 0.6763\nEpoch 8/20\n146/146 [==============================] - 5s 33ms/step - loss: 1.3932 - accuracy: 0.5449 - val_loss: 0.6748 - val_accuracy: 0.6642\nEpoch 9/20\n146/146 [==============================] - 5s 33ms/step - loss: 1.3823 - accuracy: 0.5474 - val_loss: 0.6370 - val_accuracy: 0.6638\nEpoch 10/20\n146/146 [==============================] - 5s 35ms/step - loss: 1.3912 - accuracy: 0.5519 - val_loss: 0.6344 - val_accuracy: 0.7042\nEpoch 11/20\n146/146 [==============================] - 5s 33ms/step - loss: 1.3657 - accuracy: 0.5582 - val_loss: 0.5666 - val_accuracy: 0.7231\nEpoch 12/20\n146/146 [==============================] - 5s 32ms/step - loss: 1.3605 - accuracy: 0.5591 - val_loss: 0.6062 - val_accuracy: 0.6905\nEpoch 13/20\n146/146 [==============================] - 5s 36ms/step - loss: 1.3702 - accuracy: 0.5579 - val_loss: 0.5941 - val_accuracy: 0.7171\nEpoch 14/20\n146/146 [==============================] - 5s 33ms/step - loss: 1.3565 - accuracy: 0.5617 - val_loss: 0.6071 - val_accuracy: 0.6849\nEpoch 15/20\n146/146 [==============================] - 5s 34ms/step - loss: 1.3436 - accuracy: 0.5607 - val_loss: 0.5813 - val_accuracy: 0.7025\nEpoch 16/20\n146/146 [==============================] - 5s 33ms/step - loss: 1.3461 - accuracy: 0.5702 - val_loss: 0.5873 - val_accuracy: 0.6909\nEpoch 17/20\n146/146 [==============================] - 5s 34ms/step - loss: 1.3337 - accuracy: 0.5717 - val_loss: 0.5837 - val_accuracy: 0.6978\nEpoch 18/20\n146/146 [==============================] - 5s 33ms/step - loss: 1.3337 - accuracy: 0.5745 - val_loss: 0.6679 - val_accuracy: 0.6698\nEpoch 19/20\n146/146 [==============================] - 5s 36ms/step - loss: 1.3535 - accuracy: 0.5665 - val_loss: 0.5826 - val_accuracy: 0.7128\nEpoch 20/20\n146/146 [==============================] - 5s 33ms/step - loss: 1.3238 - accuracy: 0.5780 - val_loss: 0.5905 - val_accuracy: 0.7167\n\n\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\nLooking at our plot, we see that the accuracy has increased by a lot. The accuracy of model 2 ranged from 54% to 71%. This was not a huge improvement compared to model 1. The highest accuracy was around 68%. However, there does not appear to have any overfitting in our model 2, which is really good.\n\n\nModel 3: Data Preprocessing\nIn this model, we will be edditing the RGB pixel values to be normalized between 0 and 1, or between -1 and 1. This will allow the model to be a lot faster since RGB pixel values range from 0 to 255. This will give the model a smaller range to work with.\n\ni = keras.Input(shape=(150, 150, 3))\n# The pixel values have the range of (0, 255), but many models will work better if rescaled to (-1, 1.)\n# outputs: `(inputs * scale) + offset`\nscale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\nx = scale_layer(i)\npreprocessor = keras.Model(inputs = i, outputs = x)\n\nNow that we have finished preprocesing our data. Let’s add it into our model. We should be putting preprocessor into the first layer of our model.\n\nmodel3 = models.Sequential([\n    preprocessor,\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dropout(0.2),\n    layers.Dense(10),\n    layers.Dropout(0.5)\n])\n\nLet’s generate the summary!\n\nmodel3.summary()\n\nModel: \"sequential_7\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n model (Functional)          (None, 150, 150, 3)       0         \n                                                                 \n conv2d_19 (Conv2D)          (None, 148, 148, 32)      896       \n                                                                 \n max_pooling2d_13 (MaxPooli  (None, 74, 74, 32)        0         \n ng2D)                                                           \n                                                                 \n conv2d_20 (Conv2D)          (None, 72, 72, 32)        9248      \n                                                                 \n max_pooling2d_14 (MaxPooli  (None, 36, 36, 32)        0         \n ng2D)                                                           \n                                                                 \n conv2d_21 (Conv2D)          (None, 34, 34, 64)        18496     \n                                                                 \n max_pooling2d_15 (MaxPooli  (None, 17, 17, 64)        0         \n ng2D)                                                           \n                                                                 \n conv2d_22 (Conv2D)          (None, 15, 15, 64)        36928     \n                                                                 \n flatten_7 (Flatten)         (None, 14400)             0         \n                                                                 \n dense_12 (Dense)            (None, 64)                921664    \n                                                                 \n dropout_8 (Dropout)         (None, 64)                0         \n                                                                 \n dense_13 (Dense)            (None, 10)                650       \n                                                                 \n dropout_9 (Dropout)         (None, 10)                0         \n                                                                 \n=================================================================\nTotal params: 987882 (3.77 MB)\nTrainable params: 987882 (3.77 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\nmodel3.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nhistory = model3.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 7s 32ms/step - loss: 1.4914 - accuracy: 0.4970 - val_loss: 0.7055 - val_accuracy: 0.6135\nEpoch 2/20\n146/146 [==============================] - 5s 31ms/step - loss: 1.3789 - accuracy: 0.5467 - val_loss: 0.6294 - val_accuracy: 0.6892\nEpoch 3/20\n146/146 [==============================] - 4s 30ms/step - loss: 1.3315 - accuracy: 0.5704 - val_loss: 0.5518 - val_accuracy: 0.7614\nEpoch 4/20\n146/146 [==============================] - 5s 33ms/step - loss: 1.2610 - accuracy: 0.6045 - val_loss: 0.5186 - val_accuracy: 0.7704\nEpoch 5/20\n146/146 [==============================] - 4s 30ms/step - loss: 1.2519 - accuracy: 0.6179 - val_loss: 0.5229 - val_accuracy: 0.7846\nEpoch 6/20\n146/146 [==============================] - 4s 30ms/step - loss: 1.2149 - accuracy: 0.6506 - val_loss: 0.4664 - val_accuracy: 0.7936\nEpoch 7/20\n146/146 [==============================] - 5s 31ms/step - loss: 1.2002 - accuracy: 0.6529 - val_loss: 0.4535 - val_accuracy: 0.7919\nEpoch 8/20\n146/146 [==============================] - 4s 30ms/step - loss: 1.1645 - accuracy: 0.6792 - val_loss: 0.4552 - val_accuracy: 0.8005\nEpoch 9/20\n146/146 [==============================] - 5s 33ms/step - loss: 1.1690 - accuracy: 0.6860 - val_loss: 0.4775 - val_accuracy: 0.8005\nEpoch 10/20\n146/146 [==============================] - 4s 31ms/step - loss: 1.1030 - accuracy: 0.7191 - val_loss: 0.4909 - val_accuracy: 0.7984\nEpoch 11/20\n146/146 [==============================] - 4s 31ms/step - loss: 1.0901 - accuracy: 0.7363 - val_loss: 0.5036 - val_accuracy: 0.8001\nEpoch 12/20\n146/146 [==============================] - 5s 31ms/step - loss: 1.0486 - accuracy: 0.7524 - val_loss: 0.5018 - val_accuracy: 0.8065\nEpoch 13/20\n146/146 [==============================] - 4s 31ms/step - loss: 1.0261 - accuracy: 0.7712 - val_loss: 0.5290 - val_accuracy: 0.8117\nEpoch 14/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.9942 - accuracy: 0.7913 - val_loss: 0.5876 - val_accuracy: 0.8091\nEpoch 15/20\n146/146 [==============================] - 4s 31ms/step - loss: 0.9628 - accuracy: 0.7994 - val_loss: 0.7362 - val_accuracy: 0.7726\nEpoch 16/20\n146/146 [==============================] - 4s 31ms/step - loss: 0.9785 - accuracy: 0.8109 - val_loss: 0.6217 - val_accuracy: 0.8022\nEpoch 17/20\n146/146 [==============================] - 5s 31ms/step - loss: 0.9539 - accuracy: 0.8178 - val_loss: 0.6739 - val_accuracy: 0.8022\nEpoch 18/20\n146/146 [==============================] - 4s 31ms/step - loss: 0.9067 - accuracy: 0.8304 - val_loss: 0.6746 - val_accuracy: 0.8173\nEpoch 19/20\n146/146 [==============================] - 4s 31ms/step - loss: 0.9170 - accuracy: 0.8291 - val_loss: 0.7120 - val_accuracy: 0.8134\nEpoch 20/20\n146/146 [==============================] - 5s 31ms/step - loss: 0.9189 - accuracy: 0.8376 - val_loss: 0.8578 - val_accuracy: 0.7966\n\n\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\nWe see that the validation accuracty now ranges from 76% to 81%. This is now a big improvement from model 1, where the highest accuracy was 81.73%. However, there might be some overfitting in model 3. Toward the middle of the epoch, we see that training began to surpass our validation accuracy. Let’s try our final model to see if we can get rid of this issue.\n\n\nModel 4: Transfer Learning\nIn this model, let’s try to use a pre-existing model that was already build to add onto it. We can do so by downloading MobileNetV3Large as shown below.\n\nIMG_SHAPE = (150, 150, 3)\nbase_model = keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')\nbase_model.trainable = False\n\ni = keras.Input(shape=IMG_SHAPE)\nx = base_model(i, training = False)\nbase_model_layer = keras.Model(inputs = i, outputs = x)\n\nWARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n\n\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_large_224_1.0_float_no_top_v2.h5\n12683000/12683000 [==============================] - 0s 0us/step\n\n\nLet’s now add the base_model_layer as well as the data augmentation layers from model 2 to our newest model. Be sure to also change the Dense layer to 2.\n\nmodel4 = models.Sequential([\n    layers.RandomFlip(\"horizontal_and_vertical\", input_shape=(150, 150, 3)),  # Horizontal flip augmentation\n    layers.RandomRotation(0.2),  # Random rotation augmentation,\n    base_model_layer,\n    layers.GlobalAveragePooling2D(),\n    layers.Dense(2),\n    layers.Dropout(0.5)\n])\n\nLet’s generate a summary of our model!\n\nmodel4.summary()\n\nModel: \"sequential_4\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n random_flip_2 (RandomFlip)  (None, 150, 150, 3)       0         \n                                                                 \n random_rotation_2 (RandomR  (None, 150, 150, 3)       0         \n otation)                                                        \n                                                                 \n model_1 (Functional)        (None, 5, 5, 960)         2996352   \n                                                                 \n global_average_pooling2d (  (None, 960)               0         \n GlobalAveragePooling2D)                                         \n                                                                 \n dense_6 (Dense)             (None, 2)                 1922      \n                                                                 \n dropout_3 (Dropout)         (None, 2)                 0         \n                                                                 \n=================================================================\nTotal params: 2998274 (11.44 MB)\nTrainable params: 1922 (7.51 KB)\nNon-trainable params: 2996352 (11.43 MB)\n_________________________________________________________________\n\n\n\nmodel4.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nhistory = model4.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 15s 65ms/step - loss: 0.5046 - accuracy: 0.7431 - val_loss: 0.1272 - val_accuracy: 0.9553\nEpoch 2/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.3615 - accuracy: 0.7959 - val_loss: 0.1073 - val_accuracy: 0.9635\nEpoch 3/20\n146/146 [==============================] - 7s 46ms/step - loss: 0.3352 - accuracy: 0.8169 - val_loss: 0.1011 - val_accuracy: 0.9647\nEpoch 4/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.3167 - accuracy: 0.8168 - val_loss: 0.0979 - val_accuracy: 0.9643\nEpoch 5/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.3228 - accuracy: 0.8173 - val_loss: 0.0881 - val_accuracy: 0.9673\nEpoch 6/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.3194 - accuracy: 0.8210 - val_loss: 0.0843 - val_accuracy: 0.9721\nEpoch 7/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.3102 - accuracy: 0.8193 - val_loss: 0.0895 - val_accuracy: 0.9669\nEpoch 8/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.3079 - accuracy: 0.8230 - val_loss: 0.0913 - val_accuracy: 0.9686\nEpoch 9/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.2891 - accuracy: 0.8290 - val_loss: 0.0933 - val_accuracy: 0.9643\nEpoch 10/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.3103 - accuracy: 0.8189 - val_loss: 0.0962 - val_accuracy: 0.9617\nEpoch 11/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.3084 - accuracy: 0.8218 - val_loss: 0.0903 - val_accuracy: 0.9673\nEpoch 12/20\n146/146 [==============================] - 7s 46ms/step - loss: 0.3048 - accuracy: 0.8251 - val_loss: 0.0865 - val_accuracy: 0.9690\nEpoch 13/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.2958 - accuracy: 0.8248 - val_loss: 0.0829 - val_accuracy: 0.9703\nEpoch 14/20\n146/146 [==============================] - 7s 46ms/step - loss: 0.3037 - accuracy: 0.8224 - val_loss: 0.0915 - val_accuracy: 0.9656\nEpoch 15/20\n146/146 [==============================] - 7s 46ms/step - loss: 0.2951 - accuracy: 0.8333 - val_loss: 0.0829 - val_accuracy: 0.9690\nEpoch 16/20\n146/146 [==============================] - 7s 46ms/step - loss: 0.2980 - accuracy: 0.8264 - val_loss: 0.0822 - val_accuracy: 0.9695\nEpoch 17/20\n146/146 [==============================] - 7s 46ms/step - loss: 0.2978 - accuracy: 0.8263 - val_loss: 0.0791 - val_accuracy: 0.9708\nEpoch 18/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.2915 - accuracy: 0.8294 - val_loss: 0.0800 - val_accuracy: 0.9669\nEpoch 19/20\n146/146 [==============================] - 7s 46ms/step - loss: 0.2938 - accuracy: 0.8260 - val_loss: 0.0826 - val_accuracy: 0.9708\nEpoch 20/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.2990 - accuracy: 0.8219 - val_loss: 0.0818 - val_accuracy: 0.9660\n\n\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\nWow! Look at the accuracy of the validation! We now have an accuracy ranging from 95% to 97%. This performed way better than model 1 where the highest accuracy we achieved was a 68%. Notice on the plot above, we also see that there is no overfitting present in model 4. The training accuracy never surpassed the validation accuracy.\n\n\nTesting Data\nSince model 4 preformed the best, lets try it on the testing dataset.\n\nmodel4.evaluate(test_ds)\n\n37/37 [==============================] - 4s 104ms/step - loss: 0.0987 - accuracy: 0.9609\n\n\n[0.0987427830696106, 0.9608770608901978]\n\n\nWe see that the accuracy on our testing data is still really high, 96%. That is super impressive!\n\n\nFinal Takeaways\nHope this tutorial was helpful in learning about image classifications using Keras!"
  },
  {
    "objectID": "posts/bruin/HW3.html",
    "href": "posts/bruin/HW3.html",
    "title": "Web Design Using Flask",
    "section": "",
    "text": "In this blog, you will be learning how to create a website using flask! The goal is to create a website that contains hyperlinks that will open a text box to allow users to put in a message as well as their name or handle. All of the information will be sorted into a database. There will also be a link to look at all the past messages."
  },
  {
    "objectID": "posts/bruin/HW3.html#get_message_db",
    "href": "posts/bruin/HW3.html#get_message_db",
    "title": "Web Design Using Flask",
    "section": "get_message_db()",
    "text": "get_message_db()\nThe purpose of get_message_db() is to connect or create a database that will store all the messages that is being submitted in our website. To do so, we will create a try and except case. If a database exists, the function will connect to the existing one. In the case that it does not exist, we will create a database.\n\n@app.route('/message') # directory to the message page\ndef get_message_db():\n\"\"\"\nCreating the database if one does not exist \nIf a database does exist, we will get the current one\n\"\"\"\n    try:\n        return g.message_db\n    except AttributeError:\n        # connecting to our database\n        g.message_db = sqlite3.connect(\"messages_db.sqlite\")\n        cursor = g.message_db.cursor()\n        # if table does not exist, create one\n        cursor.execute('''CREATE TABLE IF NOT EXISTS messages\n                          (id INTEGER PRIMARY KEY, handle TEXT, message TEXT)''')\n        g.message_db.commit()\n        return g.message_db\n\nIndentationError: expected an indented block after function definition on line 2 (926931727.py, line 3)"
  },
  {
    "objectID": "posts/bruin/HW3.html#insert_messagerequest",
    "href": "posts/bruin/HW3.html#insert_messagerequest",
    "title": "Web Design Using Flask",
    "section": "insert_message(request):",
    "text": "insert_message(request):\nThe purpose of this function is to take in the user inputs from the website. We are asking users for to submit a message as well as their name/handle. We will then get our database through calling the get_message_db() function and inserting what the user submitted to the site. We also have to make sure we close the connection to our database.\n\ndef insert_message(request):\n\"\"\"\nInputting the user's submissions into the database\n\"\"\"\n    handle = request.form['handle']\n    message = request.form['message']\n\n    con = get_message_db()\n    cur = con.cursor()\n    cur.execute('''INSERT INTO messages (handle, message) VALUES (?, ?)''', (handle, message))\n    \n    con.commit() # commit the messages\n    con.close() # CLOSE THE CONNECTION!"
  },
  {
    "objectID": "posts/bruin/HW3.html#submit",
    "href": "posts/bruin/HW3.html#submit",
    "title": "Web Design Using Flask",
    "section": "submit()",
    "text": "submit()\nThe purpose of this function is to ensure the html files are connected to the functions that we have defined. Above our function, we have included an @app.route(), which creates the add on to our base link. We have two different methods of accessing the site, one through POST and one through GET. The GET when the user tries to get to the submit page by clicking on the hyperlink in the base website. If the user is trying to post, we will use the insert_message() function and then thank the user for their submission.\n\n# Creating a route to the submit page\n@app.route('/submit', methods=['POST', 'GET'])\ndef submit():\n\"\"\"\nCalling the submit.html base on what the user does\nWill either open up the submit page or get the information\nfrom the user's submission\n\"\"\"\n    if request.method == 'GET':\n        return render_template(\"submit.html\")\n    else:\n        # calling insert_message() to input submission to database\n        insert_message(request) \n        # thank you note to user\n        thanks = \"Thank you for your submission!\"\n        return render_template(\"submit.html\", thanks = thanks)"
  },
  {
    "objectID": "posts/bruin/HW3.html#random_messagesn",
    "href": "posts/bruin/HW3.html#random_messagesn",
    "title": "Web Design Using Flask",
    "section": "random_messages(n)",
    "text": "random_messages(n)\nThe purpose of this function is to get the messages and names of the users that have posted on our site. To do so, we will need to establish a connection to our database. Once the connection has been made, we will get all of the messages and handles.\n\ndef random_messages(n):\n\"\"\"\nConnecting to our database and getting the handle and messages\nfrom each user\n\"\"\"\n    # connect to database\n    con = get_message_db()\n    cur = con.cursor()\n    # extracting information\n    cur.execute('''SELECT handle, message FROM messages ORDER BY RANDOM() LIMIT ?''', (n,))\n    messages = cur.fetchall()\n    # close our connection\n    cur.close()\n    return messages"
  },
  {
    "objectID": "posts/bruin/HW3.html#view",
    "href": "posts/bruin/HW3.html#view",
    "title": "Web Design Using Flask",
    "section": "view()",
    "text": "view()\nThe purpose of the view() function is to create a route form the base website that shows the different messages that have been submitted by previous users. To do so, we will all the random_messages(n) function to get the information we need. In this case, I have made it so that n equals 5. Therefore, we will be getting 5 random messages from our database. Then we will set a connection to the view.html to upload the messages we have extracted.\n\n# creating a new route from base website\n@app.route('/view')\ndef view():\n\"\"\"\nGetting 5 random messages from database and uploading\nit to the view.html\n\"\"\"\n    messages = random_messages(5)\n    return render_template(\"view.html\", messages=messages)\n        \nif __name__ == '__main__':\n    app.run(debug=True)\n\nThat is all for the app.py files! The next section will be talking about the html files in our templates folder."
  },
  {
    "objectID": "posts/bruin/HW2.html",
    "href": "posts/bruin/HW2.html",
    "title": "Web Scrapping TMDB",
    "section": "",
    "text": "In the blog, we will be learning how to web scrape through TMDB and create a csv file using the library scrapy."
  },
  {
    "objectID": "posts/bruin/HW2.html#parse-function",
    "href": "posts/bruin/HW2.html#parse-function",
    "title": "Web Scrapping TMDB",
    "section": "parse Function",
    "text": "parse Function\nInside our class, we will define another function called parse. This will bring us to the next page that contains the information on the Full Cast and Members.\n\ndef parse(self, response):\n    \"\"\"\n    Parsing through the initial website to get to the cast webpage\n    \"\"\"\n    # getting to the cast page\n    yield scrapy.Request(f\"{response.url}/cast\", \n                         callback = self.parse_full_credits)\n\nIn parse, you can see that we set the first parameter of scrapy.Request to f”{response.url}/cast”. This will take the url that we have selected in the init function, and add on /cast to get to the page with all the cast information."
  },
  {
    "objectID": "posts/bruin/HW2.html#parse_full_credits-function",
    "href": "posts/bruin/HW2.html#parse_full_credits-function",
    "title": "Web Scrapping TMDB",
    "section": "parse_full_credits Function",
    "text": "parse_full_credits Function\nFrom the cast information webpage, we will like to go to each individual actor’s webpage. To do so, we will define another function inside the class called parse_full_credits. What this function will do is go through each actor on the website and call the function parse_actor_page.\n\ndef parse_full_credits(self, response):\n    \"\"\"\n    At the cast webpage, this function will look through all the crew members and \n    get link to their own personal webpage \n    \"\"\"\n    # getting the information from the full cast and members\n    for info in response.css(\"ol.people.credits\")[0].css(\"li\") :\n        # getting the links to the individual actors\n        actor = info.css('a::attr(href)').get() \n        yield scrapy.Request(f\"https://www.themoviedb.org{actor}\", \n                             callback = self.parse_actor_page)\n\nIn parse_full_credits, to get to each individual actor’s page, we will use the following command: response.css(“ol.people.credits”)[0].css(“li”).  How do we know we will need to use this command? We will inspect the webpage.  In the webpage, when we hover over a line, we can see the corresponding section highlighted on the website. We will follow all the subcategories until we find the link to an cast member’s webpage.  To get to that section, we will use response.css(“ol.people.credits”)[0].css(“li”). We will put this under a for loop, which will allow us to parse through each individual actor under Cast.  After getting to a single actor, we will use ‘a::attr(href)’ to get the link to the actor’s page. We will then add on the link to https://www.themoviedb.org, by using the following code: f”https://www.themoviedb.org{actor}“. This will send the link to the next function to be parsed."
  },
  {
    "objectID": "posts/bruin/HW2.html#parse_actor_page-function",
    "href": "posts/bruin/HW2.html#parse_actor_page-function",
    "title": "Web Scrapping TMDB",
    "section": "parse_actor_page Function",
    "text": "parse_actor_page Function\nWe will now define another function inside the class called parse_actor_page. This will scrape through a specific actor’s page and find all the movies and tv shows the actor had an Acting role in.\n\ndef parse_actor_page(self, response):\n    \"\"\"\n    This function will start at the actor's webpage.\n    It will then take all the movies and TV shows this actor had an Acting role in \n    and insert it into a dictionary.\n    \"\"\"\n    # some actor acting informations are not aligned, therefore we will need \n    # to take that into account and use an if statement to find the correct header\n    header_pos = 0\n        \n    if response.css('h3.zero:contains(\"Acting\")') :\n        header_pos = 0\n    elif response.css('h3.one:contains(\"Acting\")') :\n        header_pos = 1\n    elif response.css('h3.two:contains(\"Acting\")'):\n        header_pos = 2\n        \n    actor_acting = response.css(\"table.card.credits\")[header_pos]\n        \n    for work in actor_acting.css(\"a.tooltip bdi::text\").getall():\n        yield {\"actor\" : response.css(\"h2.title a::text\").get(), \n               \"movie_or_TV_name\" : work}\n\nAgain, we will look through inspect to see exactly where the information we need is located. We need to be very careful in this section, as some cast webpages are out of order.  Specifically, when we look at David Holmes webpage, we see that Acting is not displayed first. Instead, Crew is displayed first on his page. Therefore, to figure out where the Acting section is for each actor, we will use an if statement.  Under inspect, we see that h3 contains all the information on the movies and tv shows that actors has been in and whether they were in Acting, Crew, or Production. Therefore, to find the specific h3 that has the information on the movies and tv shows an actor had an Acting role in, we will use the following command: ‘h3.zero:contains(“Acting”)’, ‘h3.one:contains(“Acting”)’ or ‘h3.two:contains(“Acting”)’. This will look through the text in h3 and see if Acting is in the line. If it is, we will then set header_pos.  Next, we will use the command response.css(“table.card.credits”)[header_pos] which will use the position of the header to get to the information on the movies and tv shows the actor has been in.  To get the movie and tv show name, we will use actor_acting.css(“a.tooltip bdi::text”).getall(). This will get the names of all the movies and tv show’s the actor has been in.  Now, we want to add the information into a dictionary. We will for loop through the result and add the titles to the corresponding actor.\nNow that we have completed our code, we can run our code by using the following terminal command:  scrapy crawl tmdb_spider -o movies.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone  If you would like to parse through another movie, simply replace the subdir to another movie’s link.  If you run into any issues, you will have to use an USER_AGENT to make the website not think you are a bot. You can add this command to the terminal command: -s USER_AGENT=’Mozilla/5.0 (Macintosh; Intel Mac OS X 11_5_2) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.2 Safari/605.1.15\nAfterwards, you should have a file generated that has all of the information you need."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "myblog",
    "section": "",
    "text": "Text Classification\n\n\n\n\n\n\nWeek 10\n\n\nHW6\n\n\n\n\n\n\n\n\n\nMar 11, 2024\n\n\nShaina Wang\n\n\n\n\n\n\n\n\n\n\n\n\nImage Classification\n\n\n\n\n\n\nWeek 9\n\n\nHW5\n\n\n\n\n\n\n\n\n\nMar 4, 2024\n\n\nShaina Wang\n\n\n\n\n\n\n\n\n\n\n\n\nHeat Diffusion\n\n\n\n\n\n\nWeek 7\n\n\nHW4\n\n\n\n\n\n\n\n\n\nFeb 23, 2024\n\n\nShaina Wang\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Design Using Flask\n\n\n\n\n\n\nWeek 6\n\n\nHW3\n\n\n\n\n\n\n\n\n\nFeb 12, 2024\n\n\nShaina Wang\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Scrapping TMDB\n\n\n\n\n\n\nWeek 5\n\n\nHW2\n\n\n\n\n\n\n\n\n\nFeb 12, 2024\n\n\nShaina Wang\n\n\n\n\n\n\n\n\n\n\n\n\nSQL Tutorial on Climate\n\n\n\n\n\n\nWeek 3\n\n\nHW1\n\n\n\n\n\n\n\n\n\nJan 29, 2024\n\n\nShaina Wang\n\n\n\n\n\n\n\n\n\n\n\n\nContructing data visualization of the Palmer Penguin data set\n\n\n\n\n\n\nWeek 1\n\n\nHW0\n\n\n\n\n\n\n\n\n\nJan 20, 2024\n\n\nShaina Wang\n\n\n\n\n\n\nNo matching items"
  }
]