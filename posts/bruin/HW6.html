<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.541">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Shaina Wang">
<meta name="dcterms.date" content="2024-03-11">

<title>myblog - Fake News Classification</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">myblog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Fake News Classification</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Week 10</div>
                <div class="quarto-category">HW6</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Shaina Wang </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">March 11, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>In this blog, I will be covering how to create a fake news classifier using the library Keras. We will be using Google Colab for this demonstration.</p>
<p>First, let’s begin by importing all the necessary libraries.</p>
<div id="cell-2" class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> string</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> keras</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> utils, models, layers</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.layers <span class="im">import</span> TextVectorization</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> LabelEncoder</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># for embedding viz</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.express <span class="im">as</span> px</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.io <span class="im">as</span> pio</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>pio.templates.default <span class="op">=</span> <span class="st">"plotly_white"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>After you are done, we can now import the dataset. We will be using data from the following github: “https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true”. Make sure to pass in the dataset using pd.read_csv.</p>
<div id="cell-4" class="cell" data-outputid="018da1cc-17ea-43bf-e021-3a3af6830ee8">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>train_url <span class="op">=</span> <span class="st">"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.read_csv(train_url)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>data.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="93">

  <div id="df-af450b8d-3ffb-4904-82db-e0d0662a8210" class="colab-df-container">
    <div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Unnamed: 0</th>
<th data-quarto-table-cell-role="th">title</th>
<th data-quarto-table-cell-role="th">text</th>
<th data-quarto-table-cell-role="th">fake</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>17366</td>
<td>Merkel: Strong result for Austria's FPO 'big c...</td>
<td>German Chancellor Angela Merkel said on Monday...</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>5634</td>
<td>Trump says Pence will lead voter fraud panel</td>
<td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>17487</td>
<td>JUST IN: SUSPECTED LEAKER and “Close Confidant...</td>
<td>On December 5, 2017, Circa s Sara Carter warne...</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>12217</td>
<td>Thyssenkrupp has offered help to Argentina ove...</td>
<td>Germany s Thyssenkrupp, has offered assistance...</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>5535</td>
<td>Trump say appeals court decision on travel ban...</td>
<td>President Donald Trump on Thursday called the ...</td>
<td>0</td>
</tr>
</tbody>
</table>

</div>
    <div class="colab-df-buttons">

  <div class="colab-df-container">
    <button class="colab-df-convert" onclick="convertToInteractive('df-af450b8d-3ffb-4904-82db-e0d0662a8210')" title="Convert this dataframe to an interactive table." style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 -960 960 960">
    <path d="M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z"></path>
  </svg>
    </button>

  <style>
    .colab-df-container {
      display:flex;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    .colab-df-buttons div {
      margin-bottom: 4px;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

    <script>
      const buttonEl =
        document.querySelector('#df-af450b8d-3ffb-4904-82db-e0d0662a8210 button.colab-df-convert');
      buttonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';

      async function convertToInteractive(key) {
        const element = document.querySelector('#df-af450b8d-3ffb-4904-82db-e0d0662a8210');
        const dataTable =
          await google.colab.kernel.invokeFunction('convertToInteractive',
                                                    [key], {});
        if (!dataTable) return;

        const docLinkHtml = 'Like what you see? Visit the ' +
          '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
          + ' to learn more about interactive tables.';
        element.innerHTML = '';
        dataTable['output_type'] = 'display_data';
        await google.colab.output.renderOutput(dataTable, element);
        const docLink = document.createElement('div');
        docLink.innerHTML = docLinkHtml;
        element.appendChild(docLink);
      }
    </script>
  </div>


<div id="df-52f7a331-95d8-4e23-a2a8-bcd95a3c60e5">
  <button class="colab-df-quickchart" onclick="quickchart('df-52f7a331-95d8-4e23-a2a8-bcd95a3c60e5')" title="Suggest charts" style="display:none;">

<svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 0 24 24" width="24px">
    <g>
        <path d="M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z"></path>
    </g>
</svg>
  </button>

<style>
  .colab-df-quickchart {
      --bg-color: #E8F0FE;
      --fill-color: #1967D2;
      --hover-bg-color: #E2EBFA;
      --hover-fill-color: #174EA6;
      --disabled-fill-color: #AAA;
      --disabled-bg-color: #DDD;
  }

  [theme=dark] .colab-df-quickchart {
      --bg-color: #3B4455;
      --fill-color: #D2E3FC;
      --hover-bg-color: #434B5C;
      --hover-fill-color: #FFFFFF;
      --disabled-bg-color: #3B4455;
      --disabled-fill-color: #666;
  }

  .colab-df-quickchart {
    background-color: var(--bg-color);
    border: none;
    border-radius: 50%;
    cursor: pointer;
    display: none;
    fill: var(--fill-color);
    height: 32px;
    padding: 0;
    width: 32px;
  }

  .colab-df-quickchart:hover {
    background-color: var(--hover-bg-color);
    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);
    fill: var(--button-hover-fill-color);
  }

  .colab-df-quickchart-complete:disabled,
  .colab-df-quickchart-complete:disabled:hover {
    background-color: var(--disabled-bg-color);
    fill: var(--disabled-fill-color);
    box-shadow: none;
  }

  .colab-df-spinner {
    border: 2px solid var(--fill-color);
    border-color: transparent;
    border-bottom-color: var(--fill-color);
    animation:
      spin 1s steps(1) infinite;
  }

  @keyframes spin {
    0% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
      border-left-color: var(--fill-color);
    }
    20% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    30% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
      border-right-color: var(--fill-color);
    }
    40% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    60% {
      border-color: transparent;
      border-right-color: var(--fill-color);
    }
    80% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-bottom-color: var(--fill-color);
    }
    90% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
    }
  }
</style>

  <script>
    async function quickchart(key) {
      const quickchartButtonEl =
        document.querySelector('#' + key + ' button');
      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.
      quickchartButtonEl.classList.add('colab-df-spinner');
      try {
        const charts = await google.colab.kernel.invokeFunction(
            'suggestCharts', [key], {});
      } catch (error) {
        console.error('Error during call to suggestCharts:', error);
      }
      quickchartButtonEl.classList.remove('colab-df-spinner');
      quickchartButtonEl.classList.add('colab-df-quickchart-complete');
    }
    (() => {
      let quickchartButtonEl =
        document.querySelector('#df-52f7a331-95d8-4e23-a2a8-bcd95a3c60e5 button');
      quickchartButtonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';
    })();
  </script>
</div>
    </div>
  </div>
</div>
</div>
<p>When we first read in the data, we see that there is an extra column in addition to title, text, and fake. We can simply remove that column because that is not important and will not give us any helpful information.</p>
<div id="cell-6" class="cell" data-outputid="cc3f9ae4-c7d9-4706-c19d-49b554ae58e1">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>data  <span class="op">=</span> data[[<span class="st">"title"</span>, <span class="st">"text"</span>, <span class="st">"fake"</span>]]</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>data.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="94">

  <div id="df-2b50ea06-8506-4aa3-b85b-e1256ec98c67" class="colab-df-container">
    <div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">title</th>
<th data-quarto-table-cell-role="th">text</th>
<th data-quarto-table-cell-role="th">fake</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>Merkel: Strong result for Austria's FPO 'big c...</td>
<td>German Chancellor Angela Merkel said on Monday...</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>Trump says Pence will lead voter fraud panel</td>
<td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>JUST IN: SUSPECTED LEAKER and “Close Confidant...</td>
<td>On December 5, 2017, Circa s Sara Carter warne...</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>Thyssenkrupp has offered help to Argentina ove...</td>
<td>Germany s Thyssenkrupp, has offered assistance...</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>Trump say appeals court decision on travel ban...</td>
<td>President Donald Trump on Thursday called the ...</td>
<td>0</td>
</tr>
</tbody>
</table>

</div>
    <div class="colab-df-buttons">

  <div class="colab-df-container">
    <button class="colab-df-convert" onclick="convertToInteractive('df-2b50ea06-8506-4aa3-b85b-e1256ec98c67')" title="Convert this dataframe to an interactive table." style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 -960 960 960">
    <path d="M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z"></path>
  </svg>
    </button>

  <style>
    .colab-df-container {
      display:flex;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    .colab-df-buttons div {
      margin-bottom: 4px;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

    <script>
      const buttonEl =
        document.querySelector('#df-2b50ea06-8506-4aa3-b85b-e1256ec98c67 button.colab-df-convert');
      buttonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';

      async function convertToInteractive(key) {
        const element = document.querySelector('#df-2b50ea06-8506-4aa3-b85b-e1256ec98c67');
        const dataTable =
          await google.colab.kernel.invokeFunction('convertToInteractive',
                                                    [key], {});
        if (!dataTable) return;

        const docLinkHtml = 'Like what you see? Visit the ' +
          '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
          + ' to learn more about interactive tables.';
        element.innerHTML = '';
        dataTable['output_type'] = 'display_data';
        await google.colab.output.renderOutput(dataTable, element);
        const docLink = document.createElement('div');
        docLink.innerHTML = docLinkHtml;
        element.appendChild(docLink);
      }
    </script>
  </div>


<div id="df-d80fa0fe-0d3d-4937-b6e1-e6ec163b6c92">
  <button class="colab-df-quickchart" onclick="quickchart('df-d80fa0fe-0d3d-4937-b6e1-e6ec163b6c92')" title="Suggest charts" style="display:none;">

<svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 0 24 24" width="24px">
    <g>
        <path d="M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z"></path>
    </g>
</svg>
  </button>

<style>
  .colab-df-quickchart {
      --bg-color: #E8F0FE;
      --fill-color: #1967D2;
      --hover-bg-color: #E2EBFA;
      --hover-fill-color: #174EA6;
      --disabled-fill-color: #AAA;
      --disabled-bg-color: #DDD;
  }

  [theme=dark] .colab-df-quickchart {
      --bg-color: #3B4455;
      --fill-color: #D2E3FC;
      --hover-bg-color: #434B5C;
      --hover-fill-color: #FFFFFF;
      --disabled-bg-color: #3B4455;
      --disabled-fill-color: #666;
  }

  .colab-df-quickchart {
    background-color: var(--bg-color);
    border: none;
    border-radius: 50%;
    cursor: pointer;
    display: none;
    fill: var(--fill-color);
    height: 32px;
    padding: 0;
    width: 32px;
  }

  .colab-df-quickchart:hover {
    background-color: var(--hover-bg-color);
    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);
    fill: var(--button-hover-fill-color);
  }

  .colab-df-quickchart-complete:disabled,
  .colab-df-quickchart-complete:disabled:hover {
    background-color: var(--disabled-bg-color);
    fill: var(--disabled-fill-color);
    box-shadow: none;
  }

  .colab-df-spinner {
    border: 2px solid var(--fill-color);
    border-color: transparent;
    border-bottom-color: var(--fill-color);
    animation:
      spin 1s steps(1) infinite;
  }

  @keyframes spin {
    0% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
      border-left-color: var(--fill-color);
    }
    20% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    30% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
      border-right-color: var(--fill-color);
    }
    40% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    60% {
      border-color: transparent;
      border-right-color: var(--fill-color);
    }
    80% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-bottom-color: var(--fill-color);
    }
    90% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
    }
  }
</style>

  <script>
    async function quickchart(key) {
      const quickchartButtonEl =
        document.querySelector('#' + key + ' button');
      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.
      quickchartButtonEl.classList.add('colab-df-spinner');
      try {
        const charts = await google.colab.kernel.invokeFunction(
            'suggestCharts', [key], {});
      } catch (error) {
        console.error('Error during call to suggestCharts:', error);
      }
      quickchartButtonEl.classList.remove('colab-df-spinner');
      quickchartButtonEl.classList.add('colab-df-quickchart-complete');
    }
    (() => {
      let quickchartButtonEl =
        document.querySelector('#df-d80fa0fe-0d3d-4937-b6e1-e6ec163b6c92 button');
      quickchartButtonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';
    })();
  </script>
</div>
    </div>
  </div>
</div>
</div>
<p>We can now only see title, text, and fake on our dataset!</p>
<p>Next, let’s create a function to clean up our dataset. We want to get ride of all the stopword’s in our data as well as make all the letter’s lowercase. We want to also return a tf.data.Dataset, which consists of two inputs and one output. The two inputs should be title and text, while the output should be fake. In the function, you can see that we also included batch. Batch will help increase the speed of training especially since we are working with a large dataset.</p>
<div id="cell-8" class="cell" data-outputid="ca0cd676-8fd3-4223-b906-643e23b0d584">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.corpus <span class="im">import</span> stopwords</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">'stopwords'</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>stop <span class="op">=</span> stopwords.words(<span class="st">'english'</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_dataset(input_data) :</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>  <span class="co">"""</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co">  Function will make all text lowercase</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co">  Removes stopwords from title and text</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="co">  Returns a tf.data.Dataset with two inputs and one output</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co">  """</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>  input_data[<span class="st">'title_wo_stopwords'</span>] <span class="op">=</span> input_data[<span class="st">'title'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="st">' '</span>.join([word <span class="cf">for</span> word <span class="kw">in</span> x.split() <span class="cf">if</span> word <span class="kw">not</span> <span class="kw">in</span> stop]))</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>  input_data[<span class="st">'text_wo_stopwords'</span>] <span class="op">=</span> input_data[<span class="st">'text'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="st">' '</span>.join([word <span class="cf">for</span> word <span class="kw">in</span> x.split() <span class="cf">if</span> word <span class="kw">not</span> <span class="kw">in</span> stop]))</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>  data <span class="op">=</span> tf.data.Dataset.from_tensor_slices(({<span class="st">"title"</span>: input_data[<span class="st">'title_wo_stopwords'</span>],</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>                                              <span class="st">"text"</span>: input_data[<span class="st">'text_wo_stopwords'</span>]},</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>                                              {<span class="st">"fake"</span>: input_data[<span class="st">'fake'</span>]}))</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>  data <span class="op">=</span> data.<span class="bu">map</span>(<span class="kw">lambda</span> x, y: ((tf.strings.lower(x[<span class="st">"title"</span>]), tf.strings.lower(x[<span class="st">"text"</span>])), y))  <span class="co"># Lowercasing the text</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>  data <span class="op">=</span> data.batch(<span class="dv">100</span>)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> data</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>[nltk_data] Downloading package stopwords to /root/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!</code></pre>
</div>
</div>
<div id="cell-9" class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> make_dataset(data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>After we have passed our dataset through the function we have just created, we can split our data for training. We will leave 20% of the data to be used for validation and the remaining 80% for training.</p>
<div id="cell-11" class="cell" data-outputid="1e4ec172-9aee-4491-fed3-db659fe49a3a">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> data.shuffle(buffer_size <span class="op">=</span> <span class="bu">len</span>(data), reshuffle_each_iteration<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>train_size <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.8</span><span class="op">*</span><span class="bu">len</span>(data))</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>val_size   <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.2</span><span class="op">*</span><span class="bu">len</span>(data))</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>train <span class="op">=</span> data.take(train_size)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>val   <span class="op">=</span> data.skip(train_size).take(val_size)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(train), <span class="bu">len</span>(val)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="97">
<pre><code>(180, 45)</code></pre>
</div>
</div>
<p>Next, let us look at base rate. We can calculate the base rate by looking at the number of fake and real articles in our dataset. To do so, let’s create a for loop.</p>
<div id="cell-13" class="cell" data-outputid="5835ccc0-e917-4a82-dc3f-bf7dbd9eb108">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Counting real and fake articles</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>real <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>fake <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> batch <span class="kw">in</span> train.take(<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Getting the labels</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> batch[<span class="dv">1</span>]</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> label <span class="kw">in</span> labels[<span class="st">"fake"</span>].numpy():</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If label is 0, real article</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> label <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>            real <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If label is 1, fake news</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> label <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>            fake <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Real title/text:"</span>, real)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Fake title/text:"</span>, fake)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Real title/text: 8588
Fake title/text: 9361</code></pre>
</div>
</div>
<p>We see that we have a total of 8588 real articles and 9361 fake articles. This means that the base rate is approximately 50%. Let’s see if we can create models to bring the accurate up from 50%!</p>
<section id="text-vectorization" class="level1">
<h1>Text Vectorization</h1>
<p>We will be using the following code to prepare a text vectorization layer for the tensorflow models.</p>
<div id="cell-16" class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> string</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>size_vocabulary <span class="op">=</span> <span class="dv">2000</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> standardization(input_data):</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>  <span class="co">"""</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co">  cleaning the data making all text lower and removing special characters</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="co">  """</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>  lowercase <span class="op">=</span> tf.strings.lower(input_data)</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>  no_punctuation <span class="op">=</span> tf.strings.regex_replace(lowercase,</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>                                  <span class="st">'[</span><span class="sc">%s</span><span class="st">]'</span> <span class="op">%</span> re.escape(string.punctuation),<span class="st">''</span>)</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> no_punctuation</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>title_vectorize_layer <span class="op">=</span> tf.keras.layers.TextVectorization(</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>  standardize<span class="op">=</span>standardization,</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>  max_tokens<span class="op">=</span>size_vocabulary, <span class="co"># only consider this many words</span></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>  output_mode<span class="op">=</span><span class="st">'int'</span>,</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>  output_sequence_length<span class="op">=</span><span class="dv">500</span>)</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>title_vectorize_layer.adapt(train.<span class="bu">map</span>(<span class="kw">lambda</span> x, y: x[<span class="dv">0</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="model-1" class="level1">
<h1>Model 1</h1>
<p>Let’s start by creating a model using only the article title as an input. We will be utilizing the text vecorization created above.</p>
<div id="cell-18" class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>model_title_only <span class="op">=</span> tf.keras.Sequential([</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    title_vectorize_layer,</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    layers.Embedding(input_dim<span class="op">=</span>size_vocabulary, output_dim<span class="op">=</span><span class="dv">64</span>),</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    layers.GlobalAveragePooling1D(),</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    layers.Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">'relu'</span>),</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    layers.Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">'sigmoid'</span>, name <span class="op">=</span> <span class="st">"fake"</span>)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>train_combined <span class="op">=</span> train.<span class="bu">map</span>(<span class="kw">lambda</span> x, y: (x[<span class="dv">0</span>], y))</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>val_combined <span class="op">=</span> val.<span class="bu">map</span>(<span class="kw">lambda</span> x, y: (x[<span class="dv">0</span>], y))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-19" class="cell" data-outputid="a02089ad-7a96-4a58-aa81-11f74f03b6d3">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>model_title_only.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">'adam'</span>,</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>                          loss<span class="op">=</span><span class="st">'binary_crossentropy'</span>,</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>                          metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>history_title_only <span class="op">=</span> model_title_only.fit(train_combined, epochs<span class="op">=</span><span class="dv">20</span>, validation_data<span class="op">=</span>val_combined)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/20
180/180 [==============================] - 3s 11ms/step - loss: 0.6869 - accuracy: 0.5436 - val_loss: 0.6664 - val_accuracy: 0.5287
Epoch 2/20
180/180 [==============================] - 2s 9ms/step - loss: 0.5174 - accuracy: 0.8819 - val_loss: 0.3194 - val_accuracy: 0.9449
Epoch 3/20
180/180 [==============================] - 2s 10ms/step - loss: 0.2181 - accuracy: 0.9538 - val_loss: 0.1594 - val_accuracy: 0.9564
Epoch 4/20
180/180 [==============================] - 2s 10ms/step - loss: 0.1268 - accuracy: 0.9670 - val_loss: 0.1127 - val_accuracy: 0.9676
Epoch 5/20
180/180 [==============================] - 2s 10ms/step - loss: 0.0953 - accuracy: 0.9731 - val_loss: 0.0933 - val_accuracy: 0.9729
Epoch 6/20
180/180 [==============================] - 2s 10ms/step - loss: 0.0788 - accuracy: 0.9764 - val_loss: 0.0844 - val_accuracy: 0.9749
Epoch 7/20
180/180 [==============================] - 2s 9ms/step - loss: 0.0672 - accuracy: 0.9800 - val_loss: 0.0755 - val_accuracy: 0.9758
Epoch 8/20
180/180 [==============================] - 2s 10ms/step - loss: 0.0591 - accuracy: 0.9824 - val_loss: 0.0688 - val_accuracy: 0.9771
Epoch 9/20
180/180 [==============================] - 2s 9ms/step - loss: 0.0531 - accuracy: 0.9839 - val_loss: 0.0640 - val_accuracy: 0.9787
Epoch 10/20
180/180 [==============================] - 3s 14ms/step - loss: 0.0485 - accuracy: 0.9856 - val_loss: 0.0604 - val_accuracy: 0.9804
Epoch 11/20
180/180 [==============================] - 2s 10ms/step - loss: 0.0449 - accuracy: 0.9867 - val_loss: 0.0577 - val_accuracy: 0.9800
Epoch 12/20
180/180 [==============================] - 2s 10ms/step - loss: 0.0418 - accuracy: 0.9876 - val_loss: 0.0556 - val_accuracy: 0.9796
Epoch 13/20
180/180 [==============================] - 2s 10ms/step - loss: 0.0393 - accuracy: 0.9884 - val_loss: 0.0542 - val_accuracy: 0.9798
Epoch 14/20
180/180 [==============================] - 3s 13ms/step - loss: 0.0372 - accuracy: 0.9889 - val_loss: 0.0532 - val_accuracy: 0.9807
Epoch 15/20
180/180 [==============================] - 2s 10ms/step - loss: 0.0353 - accuracy: 0.9895 - val_loss: 0.0525 - val_accuracy: 0.9807
Epoch 16/20
180/180 [==============================] - 2s 10ms/step - loss: 0.0336 - accuracy: 0.9900 - val_loss: 0.0523 - val_accuracy: 0.9811
Epoch 17/20
180/180 [==============================] - 2s 10ms/step - loss: 0.0321 - accuracy: 0.9905 - val_loss: 0.0524 - val_accuracy: 0.9811
Epoch 18/20
180/180 [==============================] - 3s 12ms/step - loss: 0.0307 - accuracy: 0.9907 - val_loss: 0.0528 - val_accuracy: 0.9811
Epoch 19/20
180/180 [==============================] - 2s 9ms/step - loss: 0.0294 - accuracy: 0.9912 - val_loss: 0.0536 - val_accuracy: 0.9807
Epoch 20/20
180/180 [==============================] - 2s 9ms/step - loss: 0.0282 - accuracy: 0.9915 - val_loss: 0.0546 - val_accuracy: 0.9800</code></pre>
</div>
</div>
<p>Let’s generate a summary of our model.</p>
<div id="cell-21" class="cell" data-outputid="5ee8ab70-6f79-4219-82de-7c1a92149e2b">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>model_title_only.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "sequential_7"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 text_vectorization_3 (Text  (None, 500)               0         
 Vectorization)                                                  
                                                                 
 embedding_34 (Embedding)    (None, 500, 64)           128000    
                                                                 
 global_average_pooling1d_2  (None, 64)                0         
 3 (GlobalAveragePooling1D)                                      
                                                                 
 dense_31 (Dense)            (None, 64)                4160      
                                                                 
 fake (Dense)                (None, 1)                 65        
                                                                 
=================================================================
Total params: 132225 (516.50 KB)
Trainable params: 132225 (516.50 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________</code></pre>
</div>
</div>
<p>Looking at the plot below, we can see that the accuracy of our model is really high! However, in the previous blog, remember that if the training accuracy is higher than the validation accuracy, there is a possible of overfitting in our model. Let’s attempt a different model to see if overfitting will not occur.</p>
<div id="cell-23" class="cell" data-outputid="c45ae901-d0fe-4405-c2b0-63e2ddc17faf">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>plt.plot(history_title_only.history[<span class="st">"accuracy"</span>], label <span class="op">=</span> <span class="st">"training"</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>plt.plot(history_title_only.history[<span class="st">"val_accuracy"</span>], label <span class="op">=</span> <span class="st">"validation"</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>plt.gca().<span class="bu">set</span>(xlabel <span class="op">=</span> <span class="st">"epoch"</span>, ylabel <span class="op">=</span> <span class="st">"accuracy"</span>)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>plt.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="HW6_files/figure-html/cell-13-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Let’s also create a visualization of our model so we can see exactly the layers and the shapes.</p>
<div id="cell-25" class="cell" data-outputid="d06f83e7-2795-4b16-a676-6cf4d6f42fc8">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>utils.plot_model(model_title_only, <span class="st">"output_filename.png"</span>,</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>                       show_shapes<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>                       show_layer_names<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="105">
<div>
<figure class="figure">
<p><img src="HW6_files/figure-html/cell-14-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="model-2" class="level1">
<h1>Model 2</h1>
<p>Now, since we only used title for our previous model, let’s try to only use text for our second model. We will use the same text vectorization method as we did before.</p>
<div id="cell-27" class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>text_vectorize_layer <span class="op">=</span> tf.keras.layers.TextVectorization(</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    standardize<span class="op">=</span>standardization,</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    max_tokens<span class="op">=</span>size_vocabulary, <span class="co"># only consider this many words</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    output_mode<span class="op">=</span><span class="st">'int'</span>,</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    output_sequence_length<span class="op">=</span><span class="dv">500</span>)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>text_vectorize_layer.adapt(train.<span class="bu">map</span>(<span class="kw">lambda</span> x, y: x[<span class="dv">1</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we can start building our model using only text.</p>
<div id="cell-29" class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>model_text_only <span class="op">=</span> tf.keras.Sequential([</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    text_vectorize_layer,</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    layers.Embedding(input_dim<span class="op">=</span>size_vocabulary, output_dim<span class="op">=</span><span class="dv">64</span>),</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    layers.GlobalAveragePooling1D(),</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    layers.Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">'relu'</span>),</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    layers.Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">'sigmoid'</span>, name <span class="op">=</span> <span class="st">"fake"</span>)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>train_combined <span class="op">=</span> train.<span class="bu">map</span>(<span class="kw">lambda</span> x, y: (x[<span class="dv">1</span>], y))</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>val_combined <span class="op">=</span> val.<span class="bu">map</span>(<span class="kw">lambda</span> x, y: (x[<span class="dv">1</span>], y))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-30" class="cell" data-outputid="390148d0-63dc-4add-bdc6-ed378113168c">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>model_text_only.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">'adam'</span>,</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>                          loss<span class="op">=</span><span class="st">'binary_crossentropy'</span>,</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>                          metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>history_text_only <span class="op">=</span> model_text_only.fit(train_combined, epochs<span class="op">=</span><span class="dv">20</span>, validation_data<span class="op">=</span>val_combined)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/20
180/180 [==============================] - 4s 18ms/step - loss: 0.5115 - accuracy: 0.8108 - val_loss: 0.2521 - val_accuracy: 0.9282
Epoch 2/20
180/180 [==============================] - 3s 15ms/step - loss: 0.1789 - accuracy: 0.9518 - val_loss: 0.1418 - val_accuracy: 0.9671
Epoch 3/20
180/180 [==============================] - 3s 15ms/step - loss: 0.1157 - accuracy: 0.9708 - val_loss: 0.1123 - val_accuracy: 0.9744
Epoch 4/20
180/180 [==============================] - 4s 18ms/step - loss: 0.0886 - accuracy: 0.9778 - val_loss: 0.0984 - val_accuracy: 0.9756
Epoch 5/20
180/180 [==============================] - 3s 15ms/step - loss: 0.0717 - accuracy: 0.9818 - val_loss: 0.0900 - val_accuracy: 0.9767
Epoch 6/20
180/180 [==============================] - 4s 18ms/step - loss: 0.0597 - accuracy: 0.9860 - val_loss: 0.0848 - val_accuracy: 0.9776
Epoch 7/20
180/180 [==============================] - 3s 15ms/step - loss: 0.0506 - accuracy: 0.9887 - val_loss: 0.0816 - val_accuracy: 0.9789
Epoch 8/20
180/180 [==============================] - 4s 21ms/step - loss: 0.0433 - accuracy: 0.9904 - val_loss: 0.0800 - val_accuracy: 0.9787
Epoch 9/20
180/180 [==============================] - 3s 15ms/step - loss: 0.0372 - accuracy: 0.9921 - val_loss: 0.0795 - val_accuracy: 0.9784
Epoch 10/20
180/180 [==============================] - 3s 15ms/step - loss: 0.0320 - accuracy: 0.9938 - val_loss: 0.0797 - val_accuracy: 0.9778
Epoch 11/20
180/180 [==============================] - 4s 18ms/step - loss: 0.0278 - accuracy: 0.9947 - val_loss: 0.0804 - val_accuracy: 0.9771
Epoch 12/20
180/180 [==============================] - 3s 14ms/step - loss: 0.0243 - accuracy: 0.9953 - val_loss: 0.0815 - val_accuracy: 0.9784
Epoch 13/20
180/180 [==============================] - 4s 20ms/step - loss: 0.0212 - accuracy: 0.9960 - val_loss: 0.0827 - val_accuracy: 0.9793
Epoch 14/20
180/180 [==============================] - 3s 15ms/step - loss: 0.0183 - accuracy: 0.9969 - val_loss: 0.0850 - val_accuracy: 0.9793
Epoch 15/20
180/180 [==============================] - 4s 18ms/step - loss: 0.0159 - accuracy: 0.9975 - val_loss: 0.0882 - val_accuracy: 0.9796
Epoch 16/20
180/180 [==============================] - 3s 15ms/step - loss: 0.0139 - accuracy: 0.9977 - val_loss: 0.0919 - val_accuracy: 0.9800
Epoch 17/20
180/180 [==============================] - 4s 18ms/step - loss: 0.0122 - accuracy: 0.9984 - val_loss: 0.0956 - val_accuracy: 0.9800
Epoch 18/20
180/180 [==============================] - 4s 21ms/step - loss: 0.0105 - accuracy: 0.9986 - val_loss: 0.0999 - val_accuracy: 0.9802
Epoch 19/20
180/180 [==============================] - 4s 18ms/step - loss: 0.0088 - accuracy: 0.9988 - val_loss: 0.1050 - val_accuracy: 0.9807
Epoch 20/20
180/180 [==============================] - 3s 15ms/step - loss: 0.0077 - accuracy: 0.9991 - val_loss: 0.1102 - val_accuracy: 0.9809</code></pre>
</div>
</div>
<p>Now that we have completed our model, let’s output the summary.</p>
<div id="cell-32" class="cell" data-outputid="a067106c-3fb5-46f5-b193-7894b6e0b077">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>model_text_only.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "sequential_8"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 text_vectorization_4 (Text  (None, 500)               0         
 Vectorization)                                                  
                                                                 
 embedding_35 (Embedding)    (None, 500, 64)           128000    
                                                                 
 global_average_pooling1d_2  (None, 64)                0         
 4 (GlobalAveragePooling1D)                                      
                                                                 
 dense_32 (Dense)            (None, 64)                4160      
                                                                 
 fake (Dense)                (None, 1)                 65        
                                                                 
=================================================================
Total params: 132225 (516.50 KB)
Trainable params: 132225 (516.50 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________</code></pre>
</div>
</div>
<p>The accuracy of our model is still very high with accuracy close to 98%. However, in the plot, we can still observe overfitting. Let’s try a different model to see if we can prevent overfitting from occuring.</p>
<div id="cell-34" class="cell" data-outputid="d914923a-9f04-474e-b148-4a9c0c14b796">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>plt.plot(history_text_only.history[<span class="st">"accuracy"</span>], label <span class="op">=</span> <span class="st">"training"</span>)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>plt.plot(history_text_only.history[<span class="st">"val_accuracy"</span>], label <span class="op">=</span> <span class="st">"validation"</span>)</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>plt.gca().<span class="bu">set</span>(xlabel <span class="op">=</span> <span class="st">"epoch"</span>, ylabel <span class="op">=</span> <span class="st">"accuracy"</span>)</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>plt.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="HW6_files/figure-html/cell-19-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Let’s also create a visualization of our model so we can see all the different layers involved.</p>
<div id="cell-36" class="cell" data-outputid="fefc7b30-4917-452d-882d-b237d20abeca">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>utils.plot_model(model_text_only, <span class="st">"output_filename.png"</span>,</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>                       show_shapes<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>                       show_layer_names<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="111">
<div>
<figure class="figure">
<p><img src="HW6_files/figure-html/cell-20-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="model-3" class="level1">
<h1>Model 3</h1>
<p>Last but not least, let’s create a model that uses both title and text to predict fake news.</p>
<div id="cell-38" class="cell">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define title and text inputs</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>title_input <span class="op">=</span> tf.keras.Input(shape<span class="op">=</span>(<span class="dv">1</span>,), name<span class="op">=</span><span class="st">"title"</span>, dtype<span class="op">=</span><span class="st">"string"</span>)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>text_input <span class="op">=</span> tf.keras.Input(shape<span class="op">=</span>(<span class="dv">1</span>,), name<span class="op">=</span><span class="st">"text"</span>, dtype<span class="op">=</span><span class="st">"string"</span>)</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>title_features <span class="op">=</span> title_vectorize_layer(title_input)</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>text_features <span class="op">=</span> text_vectorize_layer(text_input)</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>title_features <span class="op">=</span> layers.Embedding(size_vocabulary, <span class="dv">3</span>)(title_features)</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>text_features <span class="op">=</span> layers.Embedding(size_vocabulary, <span class="dv">3</span>)(text_features)</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>title_features <span class="op">=</span> layers.Dropout(<span class="fl">0.2</span>)(title_features)</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>text_features <span class="op">=</span> layers.Dropout(<span class="fl">0.2</span>)(text_features)</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>title_features <span class="op">=</span> layers.GlobalAveragePooling1D()(title_features)</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>text_features <span class="op">=</span> layers.GlobalAveragePooling1D()(text_features)</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>title_features <span class="op">=</span> layers.Dropout(<span class="fl">0.2</span>)(title_features)</span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>text_features <span class="op">=</span> layers.Dropout(<span class="fl">0.2</span>)(text_features)</span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>title_features <span class="op">=</span> layers.Dense(<span class="dv">32</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(title_features)</span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>text_features <span class="op">=</span> layers.Dense(<span class="dv">32</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(text_features)</span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.concatenate([title_features, text_features], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a>fake_pred <span class="op">=</span> layers.Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">"sigmoid"</span>, name<span class="op">=</span><span class="st">"fake"</span>)(x)</span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-27"><a href="#cb27-27" aria-hidden="true" tabindex="-1"></a>combine_model <span class="op">=</span> tf.keras.Model(</span>
<span id="cb27-28"><a href="#cb27-28" aria-hidden="true" tabindex="-1"></a>    inputs<span class="op">=</span>[title_input, text_input],</span>
<span id="cb27-29"><a href="#cb27-29" aria-hidden="true" tabindex="-1"></a>    outputs<span class="op">=</span>fake_pred</span>
<span id="cb27-30"><a href="#cb27-30" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-39" class="cell" data-outputid="f1e1ab0f-5af6-451c-e920-38b526e3d6fe">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compile the model</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>combine_model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">'adam'</span>,</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>                          loss<span class="op">=</span><span class="st">'binary_crossentropy'</span>,</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>                          metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model</span></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>history_combine <span class="op">=</span> combine_model.fit(train, epochs<span class="op">=</span><span class="dv">20</span>, validation_data<span class="op">=</span>val)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/20
180/180 [==============================] - 5s 19ms/step - loss: 0.6795 - accuracy: 0.5638 - val_loss: 0.6381 - val_accuracy: 0.9069
Epoch 2/20
180/180 [==============================] - 4s 21ms/step - loss: 0.5254 - accuracy: 0.8853 - val_loss: 0.3861 - val_accuracy: 0.9402
Epoch 3/20
180/180 [==============================] - 4s 21ms/step - loss: 0.3209 - accuracy: 0.9290 - val_loss: 0.2414 - val_accuracy: 0.9536
Epoch 4/20
180/180 [==============================] - 4s 18ms/step - loss: 0.2191 - accuracy: 0.9522 - val_loss: 0.1682 - val_accuracy: 0.9669
Epoch 5/20
180/180 [==============================] - 4s 20ms/step - loss: 0.1583 - accuracy: 0.9673 - val_loss: 0.1219 - val_accuracy: 0.9736
Epoch 6/20
180/180 [==============================] - 4s 18ms/step - loss: 0.1158 - accuracy: 0.9760 - val_loss: 0.0921 - val_accuracy: 0.9764
Epoch 7/20
180/180 [==============================] - 4s 21ms/step - loss: 0.0879 - accuracy: 0.9822 - val_loss: 0.0709 - val_accuracy: 0.9804
Epoch 8/20
180/180 [==============================] - 4s 21ms/step - loss: 0.0711 - accuracy: 0.9841 - val_loss: 0.0592 - val_accuracy: 0.9829
Epoch 9/20
180/180 [==============================] - 4s 21ms/step - loss: 0.0586 - accuracy: 0.9867 - val_loss: 0.0478 - val_accuracy: 0.9856
Epoch 10/20
180/180 [==============================] - 4s 19ms/step - loss: 0.0495 - accuracy: 0.9889 - val_loss: 0.0397 - val_accuracy: 0.9876
Epoch 11/20
180/180 [==============================] - 5s 24ms/step - loss: 0.0425 - accuracy: 0.9905 - val_loss: 0.0345 - val_accuracy: 0.9898
Epoch 12/20
180/180 [==============================] - 5s 23ms/step - loss: 0.0356 - accuracy: 0.9918 - val_loss: 0.0307 - val_accuracy: 0.9913
Epoch 13/20
180/180 [==============================] - 5s 22ms/step - loss: 0.0328 - accuracy: 0.9925 - val_loss: 0.0279 - val_accuracy: 0.9922
Epoch 14/20
180/180 [==============================] - 5s 25ms/step - loss: 0.0283 - accuracy: 0.9936 - val_loss: 0.0262 - val_accuracy: 0.9920
Epoch 15/20
180/180 [==============================] - 5s 25ms/step - loss: 0.0261 - accuracy: 0.9933 - val_loss: 0.0240 - val_accuracy: 0.9927
Epoch 16/20
180/180 [==============================] - 5s 26ms/step - loss: 0.0229 - accuracy: 0.9947 - val_loss: 0.0210 - val_accuracy: 0.9933
Epoch 17/20
180/180 [==============================] - 6s 30ms/step - loss: 0.0210 - accuracy: 0.9948 - val_loss: 0.0203 - val_accuracy: 0.9936
Epoch 18/20
180/180 [==============================] - 4s 18ms/step - loss: 0.0190 - accuracy: 0.9954 - val_loss: 0.0172 - val_accuracy: 0.9953
Epoch 19/20
180/180 [==============================] - 4s 19ms/step - loss: 0.0167 - accuracy: 0.9960 - val_loss: 0.0165 - val_accuracy: 0.9953
Epoch 20/20
180/180 [==============================] - 5s 22ms/step - loss: 0.0159 - accuracy: 0.9964 - val_loss: 0.0176 - val_accuracy: 0.9942</code></pre>
</div>
</div>
<p>Wow! We can see that the accuracy of this model is close to 100%! We can see that the issue with overfitting has been greatly improve using this model.</p>
<div id="cell-41" class="cell" data-outputid="b053cf41-3007-4e83-dbf8-fb349d68726a">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>plt.plot(history_combine.history[<span class="st">"accuracy"</span>], label <span class="op">=</span> <span class="st">"training"</span>)</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>plt.plot(history_combine.history[<span class="st">"val_accuracy"</span>], label <span class="op">=</span> <span class="st">"validation"</span>)</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>plt.gca().<span class="bu">set</span>(xlabel <span class="op">=</span> <span class="st">"epoch"</span>, ylabel <span class="op">=</span> <span class="st">"accuracy"</span>)</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>plt.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="HW6_files/figure-html/cell-23-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Let’s create a visualization of our model to see all the layers involved. This one is more complex due to the use of two inputs.</p>
<div id="cell-43" class="cell" data-outputid="fa40bd83-143a-4738-9848-de73bc379086">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>utils.plot_model(combine_model, <span class="st">"output_filename.png"</span>,</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>                       show_shapes<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>                       show_layer_names<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="87">
<div>
<figure class="figure">
<p><img src="HW6_files/figure-html/cell-24-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="model-evaluation" class="level1">
<h1>Model Evaluation</h1>
<p>Since the best performing model is the one using both title and text, let’s try to run our testing dataset.</p>
<p>We will import the dataset from the following github: “https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true”. Make sure to clean the dataset to get rid of any unhelpful columns.</p>
<div id="cell-46" class="cell" data-outputid="46898b12-d38a-4ebd-9758-5771cc52e66f">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>test_url <span class="op">=</span> <span class="st">"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>test <span class="op">=</span> pd.read_csv(test_url)</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>test <span class="op">=</span> test[[<span class="st">"title"</span>, <span class="st">"text"</span>, <span class="st">"fake"</span>]]</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>test.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="45">

  <div id="df-92177fae-f83a-4709-acab-862748047089" class="colab-df-container">
    <div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">title</th>
<th data-quarto-table-cell-role="th">text</th>
<th data-quarto-table-cell-role="th">fake</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>CNN And MSNBC Destroy Trump, Black Out His Fa...</td>
<td>Donald Trump practically does something to cri...</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>Exclusive: Kremlin tells companies to deliver ...</td>
<td>The Kremlin wants good news. The Russian lead...</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>Golden State Warriors Coach Just WRECKED Trum...</td>
<td>On Saturday, the man we re forced to call Pre...</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>Putin opens monument to Stalin's victims, diss...</td>
<td>President Vladimir Putin inaugurated a monumen...</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>BREAKING: DNC HACKER FIRED For Bank Fraud…Blam...</td>
<td>Apparently breaking the law and scamming the g...</td>
<td>1</td>
</tr>
</tbody>
</table>

</div>
    <div class="colab-df-buttons">

  <div class="colab-df-container">
    <button class="colab-df-convert" onclick="convertToInteractive('df-92177fae-f83a-4709-acab-862748047089')" title="Convert this dataframe to an interactive table." style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 -960 960 960">
    <path d="M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z"></path>
  </svg>
    </button>

  <style>
    .colab-df-container {
      display:flex;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    .colab-df-buttons div {
      margin-bottom: 4px;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

    <script>
      const buttonEl =
        document.querySelector('#df-92177fae-f83a-4709-acab-862748047089 button.colab-df-convert');
      buttonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';

      async function convertToInteractive(key) {
        const element = document.querySelector('#df-92177fae-f83a-4709-acab-862748047089');
        const dataTable =
          await google.colab.kernel.invokeFunction('convertToInteractive',
                                                    [key], {});
        if (!dataTable) return;

        const docLinkHtml = 'Like what you see? Visit the ' +
          '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
          + ' to learn more about interactive tables.';
        element.innerHTML = '';
        dataTable['output_type'] = 'display_data';
        await google.colab.output.renderOutput(dataTable, element);
        const docLink = document.createElement('div');
        docLink.innerHTML = docLinkHtml;
        element.appendChild(docLink);
      }
    </script>
  </div>


<div id="df-aba8e9da-a296-46c0-a871-6dcfc2b9f76d">
  <button class="colab-df-quickchart" onclick="quickchart('df-aba8e9da-a296-46c0-a871-6dcfc2b9f76d')" title="Suggest charts" style="display:none;">

<svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 0 24 24" width="24px">
    <g>
        <path d="M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z"></path>
    </g>
</svg>
  </button>

<style>
  .colab-df-quickchart {
      --bg-color: #E8F0FE;
      --fill-color: #1967D2;
      --hover-bg-color: #E2EBFA;
      --hover-fill-color: #174EA6;
      --disabled-fill-color: #AAA;
      --disabled-bg-color: #DDD;
  }

  [theme=dark] .colab-df-quickchart {
      --bg-color: #3B4455;
      --fill-color: #D2E3FC;
      --hover-bg-color: #434B5C;
      --hover-fill-color: #FFFFFF;
      --disabled-bg-color: #3B4455;
      --disabled-fill-color: #666;
  }

  .colab-df-quickchart {
    background-color: var(--bg-color);
    border: none;
    border-radius: 50%;
    cursor: pointer;
    display: none;
    fill: var(--fill-color);
    height: 32px;
    padding: 0;
    width: 32px;
  }

  .colab-df-quickchart:hover {
    background-color: var(--hover-bg-color);
    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);
    fill: var(--button-hover-fill-color);
  }

  .colab-df-quickchart-complete:disabled,
  .colab-df-quickchart-complete:disabled:hover {
    background-color: var(--disabled-bg-color);
    fill: var(--disabled-fill-color);
    box-shadow: none;
  }

  .colab-df-spinner {
    border: 2px solid var(--fill-color);
    border-color: transparent;
    border-bottom-color: var(--fill-color);
    animation:
      spin 1s steps(1) infinite;
  }

  @keyframes spin {
    0% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
      border-left-color: var(--fill-color);
    }
    20% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    30% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
      border-right-color: var(--fill-color);
    }
    40% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    60% {
      border-color: transparent;
      border-right-color: var(--fill-color);
    }
    80% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-bottom-color: var(--fill-color);
    }
    90% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
    }
  }
</style>

  <script>
    async function quickchart(key) {
      const quickchartButtonEl =
        document.querySelector('#' + key + ' button');
      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.
      quickchartButtonEl.classList.add('colab-df-spinner');
      try {
        const charts = await google.colab.kernel.invokeFunction(
            'suggestCharts', [key], {});
      } catch (error) {
        console.error('Error during call to suggestCharts:', error);
      }
      quickchartButtonEl.classList.remove('colab-df-spinner');
      quickchartButtonEl.classList.add('colab-df-quickchart-complete');
    }
    (() => {
      let quickchartButtonEl =
        document.querySelector('#df-aba8e9da-a296-46c0-a871-6dcfc2b9f76d button');
      quickchartButtonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';
    })();
  </script>
</div>
    </div>
  </div>
</div>
</div>
<p>Remember the function we created a while ago, make sure to use that function to clean our testing dataset.</p>
<div id="cell-48" class="cell">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>data2 <span class="op">=</span> make_dataset(test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next, we will be using evaluate() to evaluate how our model does on the testing dataset. We can see that the accuracy is still very high with 98.9% accuracy!</p>
<div id="cell-50" class="cell" data-outputid="3e47102b-fb21-4da0-8658-1f389b84e59e">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>combine_model.evaluate(data2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>225/225 [==============================] - 2s 10ms/step - loss: 0.0327 - accuracy: 0.9890</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="47">
<pre><code>[0.032745443284511566, 0.9889527559280396]</code></pre>
</div>
</div>
</section>
<section id="embedding-visualization" class="level1">
<h1>Embedding Visualization</h1>
<p>The point of embedding is to place words on certain vectors and place words with similar meanings in close proximity to each other. The following code generates a dataframe of the position of each word. We can use the following dataframe to plot the words and see its relationship with other words.</p>
<div id="cell-52" class="cell" data-outputid="d5040f13-824c-454f-d3ce-ca0605fa35e2">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> combine_model.get_layer(<span class="st">'embedding_26'</span>).get_weights()[<span class="dv">0</span>] <span class="co"># get the weights from the embedding layer</span></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>vocab <span class="op">=</span> title_vectorize_layer.get_vocabulary()                <span class="co"># get the vocabulary from our data prep for later</span></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> pca.fit_transform(weights)</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>embedding_df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'word'</span> : vocab,</span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'x0'</span>   : weights[:,<span class="dv">0</span>],</span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">'x1'</span>   : weights[:,<span class="dv">1</span>]</span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a>embedding_df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="86">

  <div id="df-b12a15b6-9c75-4f5b-992a-7c3e2d778e40" class="colab-df-container">
    <div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">word</th>
<th data-quarto-table-cell-role="th">x0</th>
<th data-quarto-table-cell-role="th">x1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td></td>
<td>-0.412692</td>
<td>0.080047</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>[UNK]</td>
<td>4.514205</td>
<td>0.258109</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>trump</td>
<td>2.201340</td>
<td>-0.022787</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>video</td>
<td>0.416556</td>
<td>0.115865</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>to</td>
<td>11.038136</td>
<td>0.340465</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1995</td>
<td>swiss</td>
<td>2.376967</td>
<td>0.062208</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1996</td>
<td>swing</td>
<td>1.032635</td>
<td>-0.028573</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1997</td>
<td>stock</td>
<td>4.701135</td>
<td>0.010318</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1998</td>
<td>standing</td>
<td>-0.248115</td>
<td>-0.108937</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1999</td>
<td>solution</td>
<td>-0.374023</td>
<td>-0.061095</td>
</tr>
</tbody>
</table>

<p>2000 rows × 3 columns</p>
</div>
    <div class="colab-df-buttons">

  <div class="colab-df-container">
    <button class="colab-df-convert" onclick="convertToInteractive('df-b12a15b6-9c75-4f5b-992a-7c3e2d778e40')" title="Convert this dataframe to an interactive table." style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 -960 960 960">
    <path d="M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z"></path>
  </svg>
    </button>

  <style>
    .colab-df-container {
      display:flex;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    .colab-df-buttons div {
      margin-bottom: 4px;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

    <script>
      const buttonEl =
        document.querySelector('#df-b12a15b6-9c75-4f5b-992a-7c3e2d778e40 button.colab-df-convert');
      buttonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';

      async function convertToInteractive(key) {
        const element = document.querySelector('#df-b12a15b6-9c75-4f5b-992a-7c3e2d778e40');
        const dataTable =
          await google.colab.kernel.invokeFunction('convertToInteractive',
                                                    [key], {});
        if (!dataTable) return;

        const docLinkHtml = 'Like what you see? Visit the ' +
          '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
          + ' to learn more about interactive tables.';
        element.innerHTML = '';
        dataTable['output_type'] = 'display_data';
        await google.colab.output.renderOutput(dataTable, element);
        const docLink = document.createElement('div');
        docLink.innerHTML = docLinkHtml;
        element.appendChild(docLink);
      }
    </script>
  </div>


<div id="df-e073d412-9f41-498b-8cd2-553120d28ab2">
  <button class="colab-df-quickchart" onclick="quickchart('df-e073d412-9f41-498b-8cd2-553120d28ab2')" title="Suggest charts" style="display:none;">

<svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 0 24 24" width="24px">
    <g>
        <path d="M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z"></path>
    </g>
</svg>
  </button>

<style>
  .colab-df-quickchart {
      --bg-color: #E8F0FE;
      --fill-color: #1967D2;
      --hover-bg-color: #E2EBFA;
      --hover-fill-color: #174EA6;
      --disabled-fill-color: #AAA;
      --disabled-bg-color: #DDD;
  }

  [theme=dark] .colab-df-quickchart {
      --bg-color: #3B4455;
      --fill-color: #D2E3FC;
      --hover-bg-color: #434B5C;
      --hover-fill-color: #FFFFFF;
      --disabled-bg-color: #3B4455;
      --disabled-fill-color: #666;
  }

  .colab-df-quickchart {
    background-color: var(--bg-color);
    border: none;
    border-radius: 50%;
    cursor: pointer;
    display: none;
    fill: var(--fill-color);
    height: 32px;
    padding: 0;
    width: 32px;
  }

  .colab-df-quickchart:hover {
    background-color: var(--hover-bg-color);
    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);
    fill: var(--button-hover-fill-color);
  }

  .colab-df-quickchart-complete:disabled,
  .colab-df-quickchart-complete:disabled:hover {
    background-color: var(--disabled-bg-color);
    fill: var(--disabled-fill-color);
    box-shadow: none;
  }

  .colab-df-spinner {
    border: 2px solid var(--fill-color);
    border-color: transparent;
    border-bottom-color: var(--fill-color);
    animation:
      spin 1s steps(1) infinite;
  }

  @keyframes spin {
    0% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
      border-left-color: var(--fill-color);
    }
    20% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    30% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
      border-right-color: var(--fill-color);
    }
    40% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    60% {
      border-color: transparent;
      border-right-color: var(--fill-color);
    }
    80% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-bottom-color: var(--fill-color);
    }
    90% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
    }
  }
</style>

  <script>
    async function quickchart(key) {
      const quickchartButtonEl =
        document.querySelector('#' + key + ' button');
      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.
      quickchartButtonEl.classList.add('colab-df-spinner');
      try {
        const charts = await google.colab.kernel.invokeFunction(
            'suggestCharts', [key], {});
      } catch (error) {
        console.error('Error during call to suggestCharts:', error);
      }
      quickchartButtonEl.classList.remove('colab-df-spinner');
      quickchartButtonEl.classList.add('colab-df-quickchart-complete');
    }
    (() => {
      let quickchartButtonEl =
        document.querySelector('#df-e073d412-9f41-498b-8cd2-553120d28ab2 button');
      quickchartButtonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';
    })();
  </script>
</div>
    </div>
  </div>
</div>
</div>
<p>Below, let’s generate an interactive plot that will allow people to hover over a dot and look at the words to see the associations with each other. For example, if you hover over the point x0 = -1.689893 and x1 = 0.2999115, you will encounter the word illegal. If you move your cursor around illegal, you will find other words like shrieff, holds, undercover, fighting, criticizes, and record. These words are greatly related to each other which shows that our embedding is working properly. We often see these words in articles related to crimes or the government. Feel free to play around the plot to find other associations!</p>
<div id="cell-54" class="cell" data-outputid="a7305ce6-8483-4248-bc53-d932ec35a8d3">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.express <span class="im">as</span> px</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> px.scatter(embedding_df,</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>                 x <span class="op">=</span> <span class="st">"x0"</span>,</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>                 y <span class="op">=</span> <span class="st">"x1"</span>,</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>                 size <span class="op">=</span> <span class="bu">list</span>(np.ones(<span class="bu">len</span>(embedding_df))),</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>                 size_max <span class="op">=</span> <span class="dv">3</span>,</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>                 hover_name <span class="op">=</span> <span class="st">"word"</span>)</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>fig.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<meta charset="utf-8">

    <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-2.24.1.min.js"></script>                <div id="44f208d0-7ab6-4908-bdae-9bbb2d165f55" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("44f208d0-7ab6-4908-bdae-9bbb2d165f55")) {                    Plotly.newPlot(                        "44f208d0-7ab6-4908-bdae-9bbb2d165f55",                        [{"hovertemplate":"\u003cb\u003e%{hovertext}\u003c\u002fb\u003e\u003cbr\u003e\u003cbr\u003ex0=%{x}\u003cbr\u003ex1=%{y}\u003cbr\u003esize=%{marker.size}\u003cextra\u003e\u003c\u002fextra\u003e","hovertext":["","[UNK]","trump","video","to","the","us","for","of","in","says","on","a","and","is","obama","with","watch","house","hillary","after","new","his","about","white","president","clinton","trump\u2019s","just","at","bill","by","from","state","russia","this","out","he","north","who","republican","over","court","it","her","senate","election","are","will","donald","breaking","black","news","as","vote","media","korea","calls","be","not","how","why","police","him","that","republicans","tax","\u2013","gop","muslim","you","campaign","trumps","was","obama\u2019s","may","up","china","has","democrats","one","deal","iran","down","russian","gets","fbi","tweets","former","what","party","people","attack","government","back","america","have","eu","security","pm","congress","talks","first","un","against","judge","syria","fox","top","chief","democrat","twitter","speech","cnn","ban","senator","say","plan","they","war","law","leader","it\u2019s","tells","south","minister","report","no","sanders","their","probe","would","during","man","could","shows","liberal","when","official","military","makes","make","million","get","two","brexit","more","tweet","take","she","presidential","foreign","goes","supreme","racist","wants","factbox","americans","illegal","gun","like","american","into","if","cruz","time","putin","host","them","sanctions","islamic","border","wow","governor","rights","obamacare","nuclear","want","syrian","response","off","hillary\u2019s","healthcare","support","fight","attacks","rally","poll","day","woman","political","all","women","help","debate","bernie","being","supporters","an","visit","show","policy","meet","go","because","ryan","he\u2019s","next","your","must","crisis","win","urges","turkey","trade","race","mexico","german","don\u2019t","our","national","call","budget","killed","antitrump","world","uk","states","saudi","travel","refugees","leaders","gives","won\u2019t","school","officials","got","big","right","lawmakers","claims","old","candidate","see","room","panel","conservative","meeting","department","warns","students","opposition","immigration","details","while","so","asks","stop","sources","left","death","before","takes","secretary","climate","we","press","hilarious","group","ties","fake","comey","most","cops","head","air","going","did","can","years","ted","caught","aid","violence","here\u2019s","emails","democratic","case","administration","work","voters","than","texas","protesters","general","free","tillerson","pick","justice","defense","rules","reason","really","washington","should","reporter","move","mayor","huge","end","way","use","still","plans","live","email","kill","college","business","supporter","open","now","interview","arrested","shocking","wall","team","seeks","money","federal","exclusive","do","made","director","city","change","but","boiler","presidency","only","leftist","john","iraq","can\u2019t","ahead","year","paul","myanmar","major","last","army","reform","conservatives","speaker","nominee","lives","list","give","voter","threatens","post","merkel","high","george","push","protest","need","shooting","sexual","secret","said","i","ep","dead","britain","york","puerto","order","lawmaker","key","forces","investigation","health","germany","fire","never","lie","image","tv","terrorists","lol","latest","korean","dnc","attorney","times","peace","lawyer","jerusalem","florida","even","doesn\u2019t","cut","told","own","office","funding","family","decision","threat","social","another","week","votes","story","legal","job","face","control","bomb","behind","truth","run","rico","keep","isis","home","coalition","2016","release","ready","flag","cuba","busted","british","billion","terrorist","ruling","rohingya","blasts","bid","amid","trying","pence","mccain","kremlin","boom","agency","adviser","\u201ci","statement","muslims","message","know","ever","chair","catalan","power","needs","msnbc","great","california","been","asked","using","terror","orders","or","lead","japan","force","destroys","best","bad","seek","radical","lies","intelligence","had","called","senators","refugee","march","macron","kills","believe","aide","slams","set","internet","independence","hate","fired","congressman","chicago","voting","tries","special","sean","israel","action","tell","rep","likely","full","france","cuts","again","thing","pay","parliament","jobs","iraqi","flynn","discuss","defends","charges","bush","audio","young","sign","protests","near","hold","found","despite","cop","admits","violent","used","source","rule","missile","inauguration","very","under","turkish","three","review","matter","leave","hollywood","chinas","calling","benghazi","10","\u201cthe","wins","son","possible","men","kids","himself","french","fraud","clinton\u2019s","claim","bombshell","backs","3","start","real","away","act","trip","spokesman","sex","sessions","saying","reports","proves","nyc","nfl","making","hit","arrest","angry","without","vows","threats","scandal","rubio","reveals","refuses","questions","public","london","ivanka","forced","exposes","didn\u2019t","destroy","days","corruption","cia","blames","assault","ad","yet","working","were","service","rape","least","images","hurricane","groups","disgusting","dc","ambassador","wife","united","these","stunning","shut","shot","seen","released","rant","much","jr","issues","girl","gave","female","epic","elections","does","denies","approves","announces","think","rips","repeal","rejects","lying","king","jail","hits","hack","good","evidence","warren","victims","uses","strike","senior","pressure","pope","photo","navy","moore","michelle","catalonia","alien","activist","transgender","they\u2019re","street","sees","sarah","role","question","pelosi","kurdish","joe","fed","facebook","epa","committee","brutal","ben","awesome","al","1","union","test","program","planned","middle","loses","erdogan","comments","barack","5","water","victory","thousands","target","pentagon","passes","ny","murder","kellyanne","kelly","history","hilariously","five","executive","criminal","citizens","chris","children","activists","xi","workers","whoa","warning","town","too","summit","strikes","spain","second","running","paris","massive","look","let","future","flashback","final","every","energy","debt","crowd","conference","biden","\u201cwe","viral","trial","russias","michigan","member","liberals","friend","envoy","drops","demands","boy","abortion","yemen","university","terrorism","return","name","millions","life","labor","issue","hell","hard","global","getting","gay","friday","faces","effort","east","doj","defend","close","child","challenge","car","agree","accuses","4","2018","venezuela","transition","thinks","student","signs","releases","puts","other","mike","migrants","members","melania","mcconnell","leaves","join","intel","illegals","hopes","four","dem","country","conway","convention","cabinet","break","talk","star","speak","since","put","perfect","oil","offer","nato","megyn","letter","laws","exposed","detroit","destroyed","brilliant","bank","arabia","announcement","aliens","2","yr","weapons","turn","sunday","stage","she\u2019s","protect","picks","part","little","line","killing","immigrants","immigrant","guest","dangerous","congressional","chairman","carson","canada","block","battle","vp","step","stand","some","responds","percent","oregon","oops","nancy","might","michael","fear","economic","company","come","chinese","breaks","australia","anchor","afghan","ukraine","staff","spicer","spending","sheriff","ria","prison","philippines","parties","offers","number","names","nafta","mattis","manager","hacking","funds","foundation","finally","fighting","fans","europe","elizabeth","deputy","around","arms","allies","agenda","accused","911","vietnam","urge","troops","tried","taiwan","stay","something","sht","remarks","owner","ohio","mass","literally","heads","food","explains","drug","dems","church","ceo","words","turkeys","taking","private","politics","picture","moscow","militants","insane","guns","frances","firm","find","far","due","cyber","crooked","conspiracy","confirms","christmas","capital","better","alabama","actually","tough","suspected","soros","schools","raise","prince","play","night","lawsuit","johnson","israeli","irma","irish","human","hearing","hannity","fck","event","embarrassing","economy","criticism","civil","bring","boycott","asia","20","where","system","steve","steps","palin","mueller","illinois","highlights","eyes","eus","crazy","coal","care","bans","baltimore","ask","any","allow","\u2018the","waters","taxes","spy","speaks","sea","request","reelection","points","northern","mom","meltdown","meets","love","lose","lebanon","jeanine","james","ireland","iowa","henningsen","guy","finds","finance","demand","dad","comment","brutally","boost","book","between","bathroom","asking","african","afghanistan","actor","absolutely","zimbabwe","wrong","worst","west","voted","videos","update","tucker","syrias","shuts","roy","rich","revealed","reutersipsos","referendum","perfectly","pass","pakistan","mark","launches","humiliates","germanys","fund","flint","fires","everyone","ends","early","comes","cities","charge","agencies","100","wikileaks","well","victim","unhinged","try","train","totally","sue","strong","streets","sent","red","praises","pastor","moves","maxine","launch","journalists","information","industry","girls","game","employees","education","declares","daughter","council","coming","christie","candidates","bundy","become","bangladesh","allegations","15","vs","vice","uks","took","there","testimony","targets","send","same","russians","resign","reporters","progress","o\u2019reilly","opens","obamas","nbc","my","mugabe","mocks","mexican","held","happened","front","financial","feds","experts","crime","corporate","communist","cnn\u2019s","charged","carolina","appeals","aides","you\u2019re","worse","veteran","today","thought","things","telling","soon","sends","record","prosecutor","potential","polls","palestinian","months","market","less","its","hurt","hope","hacked","firing","ethics","drop","done","disaster","companies","class","choice","carlson","blame","attempt","arrests","antifa","anthem","agents","address","6","50","winning","wanted","tuesday","sets","sanctuary","rnc","results","relations","proof","professor","powerful","patrick","parenthood","nothing","nomination","moment","loss","joins","iranian","he\u2019ll","hammers","giving","eastern","documents","diplomatic","confederate","christian","britains","borders","zimbabwes","women\u2019s","who\u2019s","wearing","virginia","thug","then","taxpayer","surprise","soldiers","returns","protester","place","parents","pact","netanyahu","nations","missing","manafort","looks","leaked","kurds","journalist","japans","international","important","ground","furious","freedom","explain","embassy","electoral","draws","desperate","delivers","continue","check","attacked","armed","answer","ally","8","13","trump\u201d","threaten","third","term","tensions","talking","storm","spanish","socialist","screenshots","rightwing","read","raises","racism","quits","pushes","players","overhaul","murdered","mosque","low","long","jailed","inside","india","hotel","hosts","homeland","gas","fix","egypt","disturbing","detained","dept","continues","changes","central","blocks","america\u2019s","already","across","access","abe","12","veterans","thursday","throws","super","stupid","somali","sick","save","replace","region","reaction","quit","promises","prime","person","paid","officer","nra","news\u2019","lied","koreas","kim","hysterical","hot","hands","green","golf","focus","father","failed","enough","dollars","doing","dispute","data","dallas","cooperation","community","center","brazil","blow","base","baby","australian","asylum","appeal","airport","admit","abuse","you\u2019ll","went","treasury","total","threatened","teen","suicide","study","sec","romney","risk","residents","remove","rebels","qatar","popular","philippine","outside","nyt","militant","migrant","lost","libya","kkk","kerry","kenya","jimmy","injured","hispanic","here","harassment","hand","gowdy","elected","drive","december","counsel","clintons","cannot","ca","body","board","bills","bannon","backing","amazing","2017","17","11","w","visits","visa","undercover","turns","trumprussia","trey","thugs","suspends","supports","supporting","station","shooter","riots","removed","relationship","reid","records","reach","propaganda","process","problem","probes","policies","plot","phone","options","movie","mother","monday","mays","many","longer","legislation","leaks","lavrov","irs","husband","hateful","half","guilty","fail","exit","efforts","donations","diplomats","delay","deals","cover","condemns","committed","citizen","cites","campus","buy","bus","bizarre","april","amnesty","advisor","18","word","weeks","unity","threatening","testify","teacher","suspect","six","seven","reuters","resigns","recount","ratings","pulls","prove","others","met","marriage","looms","leading","leadership","kasich","joy","jeff","isn\u2019t","hypocrisy","guess","gov","files","fears","families","eric","endorses","endorsement","dialogue","deep","credit","crackdown","commerce","charity","blacks","billionaire","biggest","avoid","attend","attempts","attacking","assad","alleged","ago","\u201cyou","\u201cif","welfare","unreal","store","stance","snl","serious","seeking","search","scam","ridiculous","regime","reasons","putting","priceless","price","position","politico","pledge","pathetic","newspaper","nazi","month","missiles","ministry","maher","losing","living","libyan","kansas","hundreds","humiliated","harvey","hariri","forward","fan","false","failure","european","entire","dossier","decide","crimes","countries","concerned","burn","beating","banned","arizona","appears","agrees","africas","won","watchdog","warned","usa","treatment","through","taxpayers","taken","sweden","susan","sudan","starts","shoot","seth","sell","schumer","san","remain","religious","relief","presidents","path","past","nation","mi","me","links","late","kushner","kimmel","judges","jones","investment","hear","having","harry","gorsuch","giuliani","facts","defending","david","critical","coup","conflict","concerns","clash","chuck","christians","charlottesville","chaos","camp","breitbart","beijing","audience","allowed","agent","ads","account","accept","abc","\u201cthis","\u201cracist\u201d","wisconsin","weighs","waiting","wage","unlikely","thanks","swedish","suggests","stephen","spying","speaking","showing","sentence","scott","sales","road","rice","resignation","reforms","proposes","posts","poland","pledges","paying","outrageous","online","once","nsa","neil","mock","mic","lady","islamist","increase","idea","huckabee","hopeful","holds","happen","given","fuel","fit","farright","expects","environmental","donors","dirty","dinner","dies","die","compares","caucus","cash","brother","briefing","blacklivesmatter","berkeley","amendment","alert","accidentally","7","\u201cnot","\u201ci\u2019m","zika","whining","whines","whether","we\u2019re","wasn\u2019t","van","tests","strategy","situation","several","rock","rise","reportedly","regional","reality","raqqa","radio","protrump","plane","pipeline","phony","orlando","oklahoma","nuts","numbers","nightmare","network","militia","memorial","leaving","leads","land","jersey","jeb","it\u201d","hypocrite","hunt","historic","happy","god","gingrich","fellow","facing","expert","exactly","emergency","duterte","defeat","courts","cost","controversial","consider","collusion","broke","blood","blaming","aren\u2019t","apart","aim","9","30","\u201cit\u2019s","\u2014","worker","wire","what\u2019s","western","wednesday","vegas","trumpcare","trudeau","trash","tom","themselves","tech","straight","sports","spent","small","respond","rescue","proposal","priebus","president\u201d","point","planning","opening","offered","nearly","mnuchin","memo","meddling","majority","lets","june","italy","irans","instead","info","indonesian","indian","including","halt","graft","google","gonna","feel","famous","expected","establishment","discussed","deadline","dark","critics","criticizes","crash","corrupt","congo","commission","commander","colbert","clear","chelsea","brazils","boss","blast","becoming","ballot","baghdad","arab","anyone","agreement","accusations","60","25","\u201che","\u2018white","zealand","which","truck","tower","toward","taps","tape","surveillance","southern","someone","shutdown","shreds","shouldn\u2019t","shares","sending","scotus","safe","robert","retirement","responsible","regulations","rate","outrage","nine","needed","nails","n","myanmars","movement","mn","male","mainstream","local","lee","lawyers","la","killer","japanese","jackson","i\u2019m","independent","incident","hospital","hiding","helping","harvard","gulf","georgia","flee","feud","falls","eye","explodes","enforcement","easy","cuban","couple","convicted","complete","cold","coast","closer","challenges","cbs","cause","captures","brussels","brings","bombing","blocked","begins","beautiful","america\u201d","africa","actress","actions","24","2015","14","zuma","zone","within","withdrawal","witch","willing","welcomes","warming","viewers","view","va","unveils","trust","tim","theory","that\u2019s","suspended","stealing","steal","split","soldier","site","single","silent","silence","short","sharpton","share","server","safety","rush","runs","rhetoric","resolve","quake","prosecutors","problems","photos","ordered","opposes","officers","nominate","nigeria","nc","nasty","moving","minutes","mind","mean","marco","maine","listen","limit","kurdistan","knows","kidding","joke","italian","invites","interest","insurance","influence","illegally","hours","horrible","helped","happens","hall","hacks","graham","gone","gold","gift","gender","gang","gains","form","flight","fcking","fair","expose","episode","egypts","drone","draft","door","domestic","diplomat","detains","declaration","deadly","create","course","costs","considering","closed","chance","ceasefire","cases","career","canadian","building","blows","billions","begin","becomes","beaten","banks","ballistic","appearance","among","allowing","again\u201d","\u201cwhite","worked","venezuelas","vatican","true","tears","teachers","tantrum","swiss","swing","stock","standing","solution"],"legendgroup":"","marker":{"color":"#636efa","size":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0],"sizemode":"area","sizeref":0.1111111111111111,"symbol":"circle"},"mode":"markers","name":"","showlegend":false,"x":[-0.4126916229724884,4.514204502105713,2.201340436935425,0.4165564179420471,11.038135528564453,-4.201103210449219,2.6093809604644775,-1.245924949645996,5.227529048919678,2.4383182525634766,6.43718147277832,2.4551608562469482,0.9879310131072998,-0.4552235007286072,0.792695164680481,5.654252052307129,0.29203537106513977,-0.47936898469924927,-2.5487685203552246,0.8659778237342834,5.246309757232666,3.161729574203491,0.09251729398965836,0.6625851988792419,-0.1574830859899521,2.4137721061706543,9.943937301635742,3.770995855331421,-1.966219186782837,-0.7904397249221802,3.5886011123657227,-0.4377734065055847,2.864201784133911,2.249680519104004,3.1023826599121094,-0.5730269551277161,-1.8187707662582397,5.539248466491699,1.1017060279846191,7.081197261810303,1.83530855178833,0.852146565914154,0.8652811050415039,-1.7381937503814697,3.3264100551605225,-0.6565526127815247,-0.43271538615226746,6.3215012550354,10.031848907470703,0.7273510098457336,-1.084385633468628,0.12518852949142456,-2.867957592010498,-3.831359386444092,3.731769561767578,0.6944745779037476,-0.05928024277091026,-2.426300287246704,2.064453125,3.096087694168091,0.380571573972702,-0.24793493747711182,1.277748942375183,2.5085439682006836,4.463709831237793,-1.7257040739059448,-3.5689690113067627,1.6191442012786865,9.244847297668457,-0.19759255647659302,-1.9485119581222534,-2.617849349975586,-1.2167401313781738,-0.2639341354370117,0.7933593988418579,4.248204708099365,5.381150722503662,-4.34771203994751,1.2673664093017578,-2.750790596008301,-5.684530735015869,-3.1801366806030273,0.7795262336730957,0.18066240847110748,3.1560676097869873,-0.8687054514884949,-2.7909014225006104,0.023982884362339973,-1.5077165365219116,0.41285809874534607,2.0119574069976807,5.399028301239014,2.143789768218994,-1.4094332456588745,2.112506628036499,-1.5931087732315063,1.6967891454696655,2.6743626594543457,1.8319045305252075,2.0871002674102783,-0.18975916504859924,-1.0271934270858765,0.5365062355995178,6.308278560638428,-2.060352087020874,-1.583956241607666,2.048466205596924,-1.5601037740707397,1.6676476001739502,-7.859410285949707,-1.125847578048706,-0.7822983860969543,0.04397189989686012,-1.2027220726013184,2.1050608158111572,-2.2362239360809326,-1.7448084354400635,2.860826015472412,-2.8065924644470215,2.160757303237915,-1.3945022821426392,-0.771263837814331,1.9272135496139526,-1.1948308944702148,2.015519142150879,1.6901570558547974,-3.885438919067383,0.4698377549648285,-0.43837201595306396,1.8623740673065186,-1.7917481660842896,-1.3479728698730469,1.0434916019439697,-1.0084887742996216,-4.312021255493164,-1.818681240081787,0.8755239248275757,4.1146087646484375,-2.5648558139801025,1.6538015604019165,-1.1275769472122192,-1.2566739320755005,-3.980886459350586,0.33377039432525635,-1.1686879396438599,-1.1399495601654053,-2.6988685131073,0.33130621910095215,1.6993850469589233,1.6693205833435059,1.4065577983856201,1.8048700094223022,1.7986716032028198,10.642892837524414,-0.1519569307565689,0.5502645969390869,-1.6893956661224365,-1.9421017169952393,-1.689893126487732,0.5030492544174194,1.1727328300476074,1.1173423528671265,1.2690447568893433,0.7084869742393494,1.2184860706329346,-1.186418056488037,-2.4683549404144287,1.511959433555603,-0.14426662027835846,-2.1469075679779053,1.9653950929641724,1.0179038047790527,4.228230953216553,0.6784318685531616,0.8256011605262756,-1.7991888523101807,-1.3445078134536743,1.0321542024612427,-0.4011147618293762,-1.6023837327957153,-0.22561414539813995,0.3097724914550781,-2.8923563957214355,-0.11352512240409851,2.9249677658081055,-0.1713326871395111,1.6700397729873657,-5.327579021453857,0.787606418132782,-0.22368884086608887,-3.339897632598877,-1.0637528896331787,-0.7442470788955688,-0.8197737336158752,-3.2121171951293945,-0.7433610558509827,-1.3761906623840332,-0.6020429134368896,-0.9136340618133545,0.6216009855270386,0.8472056984901428,-5.7693963050842285,0.19011510908603668,3.615591287612915,1.250396966934204,-0.4857194125652313,-1.9574569463729858,-2.372863292694092,1.5368942022323608,-0.836388111114502,0.6918445229530334,2.4960215091705322,-0.2138301581144333,0.6631954908370972,2.5698390007019043,5.97456169128418,-1.0707420110702515,0.44090184569358826,0.5123440027236938,0.4702517092227936,1.4271633625030518,0.11018633097410202,-1.7171318531036377,-0.5223917365074158,-0.3757209777832031,1.7333341836929321,2.467205762863159,1.4192758798599243,8.075172424316406,-2.240403413772583,-2.626718044281006,-1.9817332029342651,0.14715415239334106,2.5644712448120117,1.489783525466919,1.8225716352462769,-0.9655352830886841,-4.289729118347168,1.4341219663619995,-2.039794445037842,-3.7500174045562744,2.742486000061035,4.0141191482543945,1.8897823095321655,0.5761973857879639,3.68953800201416,-0.9284329414367676,2.229607582092285,0.9910121560096741,-3.312080144882202,1.1877262592315674,1.8576278686523438,-2.86967134475708,-1.5895087718963623,1.4566738605499268,-1.0189186334609985,1.4718915224075317,0.05237865820527077,-0.5572359561920166,2.1006104946136475,0.24557892978191376,2.976634979248047,-0.5934776663780212,-0.02823462337255478,0.08108271658420563,4.195725917816162,4.136599540710449,3.0326919555664062,1.5836261510849,-0.12805074453353882,1.1527785062789917,-5.158274173736572,-1.2110854387283325,0.163639634847641,-2.0490074157714844,-1.0547295808792114,0.0192232895642519,-1.4411792755126953,1.3467334508895874,-0.2127877175807953,-1.964267373085022,-0.1754622757434845,0.04163377359509468,6.501123905181885,-0.3178158700466156,4.078588008880615,1.2386038303375244,1.2848631143569946,-0.5545948147773743,-1.005110740661621,1.7708805799484253,2.258721113204956,1.8313369750976562,0.6477842330932617,-1.6686012744903564,2.381356954574585,0.030090775340795517,5.087611675262451,3.2402846813201904,3.177119731903076,0.5636600852012634,3.4614508152008057,0.9703588485717773,0.4108772873878479,11.781432151794434,-1.6343228816986084,-0.5299280285835266,2.3685033321380615,0.8072893023490906,3.647923707962036,1.4369312524795532,-1.8093805313110352,-0.39270228147506714,2.728600263595581,-1.1508771181106567,-1.6898243427276611,-1.3335082530975342,2.020608425140381,0.958504319190979,-0.2895662784576416,-3.4584481716156006,-2.419764518737793,4.6167778968811035,-0.8267455697059631,3.6875758171081543,-1.6689344644546509,1.5674158334732056,1.1214779615402222,3.2561795711517334,-2.736438035964966,-4.023036003112793,-1.1547892093658447,-1.6557806730270386,1.0180801153182983,-0.8775188326835632,0.025879381224513054,-0.8132338523864746,-5.578265190124512,1.5547486543655396,1.8802685737609863,-1.535004734992981,12.93157958984375,-1.5936719179153442,0.361582487821579,0.9861025214195251,2.274643898010254,0.1255766749382019,-1.7975598573684692,-2.070586919784546,-1.6538037061691284,-2.613675832748413,-0.7721495628356934,-0.5278476476669312,-0.20459790527820587,1.6533267498016357,0.5970371961593628,2.0924072265625,-1.274504542350769,-1.5390866994857788,1.7479504346847534,4.963944911956787,-0.012452748604118824,0.07588151097297668,-1.441139578819275,-0.9808204174041748,0.26420143246650696,-0.1646701693534851,2.1220202445983887,1.9221690893173218,0.9507833123207092,-3.4522175788879395,1.5678536891937256,-1.0723412036895752,-0.2260216772556305,0.7422522306442261,-4.879110336303711,1.0820380449295044,-2.8349668979644775,-1.0610936880111694,0.24930110573768616,1.1959320306777954,0.4953410029411316,1.0024462938308716,-1.2143996953964233,1.6931757926940918,0.7352632880210876,-3.042722463607788,-1.2087420225143433,-0.33420315384864807,0.9346383810043335,0.6537019610404968,2.101250171661377,-1.9019232988357544,-0.11353419721126556,-0.1730005443096161,1.1790781021118164,1.2124288082122803,-1.8343758583068848,3.840496301651001,1.144633173942566,0.20082899928092957,-0.3646083474159241,0.7978507280349731,1.7490570545196533,0.3456375300884247,-0.16408966481685638,-3.479879856109619,1.5013128519058228,1.137115478515625,0.4970875084400177,-1.9602419137954712,-0.512820303440094,1.8167411088943481,-1.62493896484375,-2.1687262058258057,-2.2691144943237305,-1.4890780448913574,-1.8668761253356934,1.8087728023529053,-0.7246644496917725,-1.2075953483581543,-1.3816049098968506,-2.865225315093994,8.219589233398438,-0.893304705619812,-1.1694419384002686,4.02660608291626,-0.12480182200670242,-1.7714073657989502,3.1813082695007324,6.302102088928223,4.306648254394531,-0.01929634064435959,-0.868910014629364,-1.259433388710022,-1.3725582361221313,0.26984548568725586,-2.636345863342285,0.3264337182044983,0.7342404723167419,2.2468042373657227,-0.8426641821861267,-2.8587632179260254,-3.53175687789917,-0.389365553855896,0.9582783579826355,0.1518799215555191,1.2563656568527222,0.9943798780441284,-2.0127182006835938,-1.706417202949524,8.629143714904785,-1.606594443321228,-0.025516964495182037,-1.0417680740356445,0.24206185340881348,-1.7480565309524536,-1.0278689861297607,2.367436170578003,1.6888505220413208,-1.9347002506256104,-1.003534197807312,2.786745071411133,0.24182823300361633,-1.1788524389266968,2.8024790287017822,-1.196089506149292,-1.5348972082138062,-1.6673455238342285,1.9629120826721191,7.1513590812683105,-0.35973304510116577,-3.3348708152770996,0.9631320238113403,-1.0811843872070312,-3.7519431114196777,0.916759192943573,0.42728427052497864,-3.0226657390594482,-1.5765639543533325,1.0157099962234497,1.920724630355835,-3.085537910461426,1.9579334259033203,0.8741820454597473,-1.8710530996322632,9.673236846923828,0.43777745962142944,-1.3833591938018799,-1.417609691619873,-1.758628487586975,-0.7470661401748657,0.8604113459587097,0.8181802034378052,0.2601742446422577,0.933185338973999,0.06862343847751617,-1.5646476745605469,1.957972764968872,-1.309224247932434,-0.36118650436401367,0.29864704608917236,0.30251964926719666,-1.0372073650360107,0.800812304019928,6.338209629058838,0.7967977523803711,-1.272780418395996,0.42074114084243774,1.5415189266204834,-1.8510633707046509,-2.416476011276245,-3.6726162433624268,0.769391655921936,0.9557450413703918,-0.5394003987312317,-2.9343461990356445,1.6657689809799194,-3.4731757640838623,0.6464904546737671,-0.144954651594162,0.023637525737285614,1.1157054901123047,-0.43397945165634155,1.8370589017868042,-2.0358080863952637,-0.21048817038536072,-0.0725850984454155,-4.153554916381836,-1.3092169761657715,1.8416074514389038,-0.5315207839012146,-1.8588911294937134,-1.1513066291809082,2.6565909385681152,-0.9635961651802063,1.428105115890503,-0.5817645192146301,-0.8722206950187683,0.18414384126663208,-1.752726435661316,0.8280499577522278,1.5674370527267456,1.1225827932357788,5.218118190765381,0.3381099998950958,0.7038446068763733,1.4116528034210205,-0.19211134314537048,-1.9003405570983887,-0.8395517468452454,-0.010192387737333775,2.034552812576294,1.9011471271514893,0.9359951019287109,1.5450630187988281,-1.8409191370010376,0.6854443550109863,-0.05285261943936348,1.3196619749069214,0.35260728001594543,2.6947362422943115,-1.7808421850204468,-0.17752528190612793,-3.729379892349243,1.0649425983428955,-1.318625569343567,3.1273787021636963,-2.5924975872039795,0.7302268743515015,-3.357433557510376,1.404388666152954,2.827218770980835,0.891149640083313,0.4809638261795044,1.022853136062622,-0.5282272100448608,-0.06828296184539795,-1.457562804222107,-0.8546313047409058,-0.2322002649307251,0.8087212443351746,1.4548708200454712,-0.6940408945083618,-1.23050057888031,-2.72587251663208,-1.374257206916809,2.2155098915100098,-2.792494297027588,-0.6445690989494324,-1.5609931945800781,-2.0378994941711426,1.5150458812713623,3.1886515617370605,5.869515419006348,-2.0261411666870117,1.97886323928833,-2.943408489227295,1.2476445436477661,-2.477921962738037,1.6734100580215454,0.8610978722572327,-0.6741070747375488,0.422463059425354,1.3682854175567627,-2.7767491340637207,2.079691171646118,2.6910107135772705,-1.4693957567214966,-1.260080099105835,2.248330593109131,-3.218275547027588,3.6886067390441895,-0.5651102066040039,0.2011086493730545,-1.176735758781433,-1.0093196630477905,0.4227448105812073,1.1770256757736206,0.8249721527099609,-0.31122371554374695,4.19107723236084,0.6724858283996582,2.747227191925049,-0.4716304838657379,-1.4829169511795044,-0.4122830927371979,-1.4524739980697632,2.432648181915283,1.3695722818374634,1.045668363571167,-0.7094298601150513,-3.1721274852752686,6.227900505065918,1.191102147102356,2.205733060836792,1.430141568183899,0.38869509100914,0.22809439897537231,0.13385285437107086,2.0185158252716064,-2.605443000793457,-0.38992124795913696,-2.711108922958374,-1.6706385612487793,-0.3422819972038269,-0.13831467926502228,0.8459239602088928,-2.0678584575653076,11.544014930725098,-0.0611128993332386,-0.05049457028508186,0.47838836908340454,-2.6099720001220703,3.855255365371704,-2.373614549636841,-1.6000068187713623,0.11576707661151886,-0.8362792134284973,1.5256891250610352,1.5133559703826904,-0.46127548813819885,-3.051600217819214,2.2953553199768066,-1.6002861261367798,1.5625784397125244,-0.43632543087005615,1.3909114599227905,0.07654689997434616,-1.6541749238967896,-1.7980363368988037,0.8088213801383972,-0.8799564838409424,-2.81215238571167,-0.6289267539978027,-0.28914159536361694,-1.7231426239013672,-0.8572246432304382,1.3336148262023926,-2.44903302192688,-0.6112988591194153,0.7204211950302124,-1.2685627937316895,-1.508219599723816,-0.4884263873100281,-2.2790894508361816,4.137608528137207,-2.3637871742248535,-0.8696398138999939,-1.7673653364181519,0.9712420105934143,-1.0224475860595703,0.6079810261726379,-0.3471149206161499,-2.1636452674865723,-0.608004629611969,1.2870312929153442,2.464521646499634,-0.3832932710647583,0.7587398290634155,-2.7280936241149902,-0.7014804482460022,-0.39036276936531067,0.5002782344818115,-0.9409542679786682,-0.9941247701644897,3.1789627075195312,1.5276973247528076,-0.05689755454659462,-0.15134859085083008,-0.3007781505584717,-0.9598101377487183,1.4432287216186523,0.3490768373012543,-0.9045408368110657,-0.08331625163555145,-1.4528610706329346,2.5305933952331543,1.6730226278305054,-0.9140428304672241,2.48240327835083,-1.5818504095077515,-0.26057982444763184,0.11114160716533661,-1.9706287384033203,-1.8849536180496216,1.2962737083435059,-0.3980194330215454,0.10489793121814728,1.4120409488677979,-1.6347965002059937,-0.6431499719619751,0.5764924883842468,-0.4557773172855377,-0.8364858031272888,-2.716855049133301,1.7750167846679688,0.6314100623130798,0.774553120136261,-3.8326759338378906,0.792194128036499,-1.4944945573806763,-1.2041624784469604,-0.5333255529403687,-0.6400811076164246,-1.2987451553344727,-0.9644588232040405,-0.23541146516799927,-0.43006014823913574,-1.589848518371582,-0.611559271812439,-0.7348280549049377,-0.2864443361759186,0.0886370912194252,-0.3696829378604889,-1.2976962327957153,0.10576732456684113,-2.4096624851226807,0.2901957035064697,-2.477370500564575,1.9059280157089233,-0.0595247745513916,-1.9777449369430542,-0.00010754666436696425,1.9672513008117676,0.8642931580543518,-0.16865353286266327,-0.3575509488582611,-2.3082642555236816,-0.2743661105632782,-0.28068962693214417,-1.6240483522415161,1.719223141670227,1.478657603263855,1.4602124691009521,1.4794551134109497,2.890303373336792,2.9561233520507812,-2.0065436363220215,-0.5989026427268982,-0.4572068452835083,0.9996570944786072,-1.5557454824447632,-1.0933321714401245,-3.26967453956604,0.20622843503952026,-0.7572212219238281,1.929918885231018,-0.9801332950592041,-0.24566295742988586,0.027490880340337753,-0.03195082023739815,-1.3781591653823853,0.18830156326293945,-4.880488395690918,-2.408094644546509,0.17743684351444244,0.11913049221038818,1.3940975666046143,-0.04239299148321152,0.4255821406841278,-0.6989845037460327,-0.33558863401412964,0.8619825839996338,0.7222419381141663,-1.2305686473846436,0.8826488852500916,0.09695844352245331,0.5121442079544067,1.2197062969207764,-0.09970135241746902,-2.600933074951172,-0.3980139493942261,0.08567650616168976,0.048879239708185196,1.098624348640442,-0.18605658411979675,-0.10449941456317902,0.9665399789810181,-2.5194296836853027,1.0541276931762695,-0.9423325657844543,-2.5421559810638428,-1.0084534883499146,-2.134077310562134,0.8951990008354187,3.655165433883667,-0.5614617466926575,0.4103550910949707,2.146193504333496,-1.6645327806472778,0.9592546820640564,-2.349701166152954,-2.235583782196045,-0.35157153010368347,-1.6416559219360352,-1.891711950302124,-0.19559065997600555,0.06648234277963638,1.014378309249878,-0.5731534361839294,-0.3196721374988556,-0.21281428635120392,-0.42526891827583313,0.4331025779247284,-0.8866890668869019,-2.7031185626983643,-0.4013146162033081,-1.3624393939971924,-0.2522822618484497,0.9530341625213623,1.4054323434829712,-1.9391425848007202,1.5604381561279297,-1.7369438409805298,1.4221984148025513,2.7758302688598633,0.71588534116745,-0.6754179000854492,0.7864822745323181,1.8631751537322998,-0.5664824843406677,0.9799053072929382,1.9208638668060303,-1.6589341163635254,2.1793320178985596,-1.8463019132614136,-1.4222732782363892,-1.2054595947265625,4.837652206420898,2.8851513862609863,-1.440905213356018,-0.3965338170528412,-0.24365438520908356,-1.5454187393188477,-1.1112221479415894,-2.3722147941589355,5.353757381439209,0.6017788648605347,0.9947550892829895,0.0593995526432991,1.325360894203186,1.8881206512451172,0.3780100643634796,0.677692174911499,-3.106487512588501,0.4422284960746765,0.261665403842926,0.8391976356506348,0.2178482860326767,-1.6112395524978638,-0.14993198215961456,0.30673229694366455,-2.328101396560669,-0.9822947978973389,-1.995137095451355,3.931185245513916,4.549898624420166,1.7869313955307007,-0.16198627650737762,0.18448948860168457,-0.0666995644569397,0.028604285791516304,2.769726037979126,-3.7967936992645264,0.017436904832720757,0.08314138650894165,1.801387906074524,7.720043182373047,-2.744969129562378,0.4114530086517334,-0.8068200349807739,3.4959592819213867,-2.9589059352874756,-0.3710439205169678,0.7896077036857605,0.5321788787841797,0.48683294653892517,1.7818859815597534,-1.2397924661636353,1.1206738948822021,-1.7138676643371582,-1.0592844486236572,-0.06173115223646164,-2.982086658477783,-0.40942203998565674,-1.9557418823242188,0.08734872192144394,0.18726271390914917,3.6302599906921387,-1.5582853555679321,-0.8768122792243958,-0.7374959588050842,-0.8536479473114014,-0.2794388234615326,-1.6666529178619385,0.29052144289016724,1.4947353601455688,-0.8434819579124451,-2.7967846393585205,0.08726295828819275,0.4311172366142273,-1.2255043983459473,-1.38856041431427,0.43461841344833374,-1.6089890003204346,-0.6250351667404175,-0.43980997800827026,5.438361644744873,-2.370666742324829,2.4416697025299072,1.334165096282959,0.9662783145904541,-1.68075692653656,-2.8742661476135254,-1.1644107103347778,-0.3296718895435333,-2.216887950897217,0.3225765824317932,0.9970679879188538,1.0169223546981812,0.7008087038993835,-0.29336443543434143,-3.1360063552856445,-0.9385256171226501,-2.0158233642578125,-1.7092584371566772,-1.1629022359848022,0.603611171245575,-0.6738083362579346,2.0695650577545166,3.587447166442871,-0.6114279627799988,2.781425714492798,-0.9308595657348633,1.335098385810852,0.994155764579773,0.08433516323566437,-1.6305763721466064,-1.1632499694824219,-1.7294046878814697,2.581611156463623,1.6542068719863892,0.8062939643859863,0.1485964059829712,-2.0720834732055664,0.22516308724880219,-0.6622250080108643,0.43758904933929443,0.386599063873291,0.6192392706871033,-0.8520848155021667,-0.015985846519470215,1.5948657989501953,0.5140547156333923,-2.433823585510254,-1.6874024868011475,6.2354936599731445,-0.060449860990047455,-4.1411638259887695,-1.5252636671066284,-1.0010406970977783,1.8234026432037354,0.7576184272766113,-0.2904253304004669,1.053285837173462,1.6680371761322021,-2.0150227546691895,3.0586180686950684,1.9916355609893799,0.19119977951049805,-0.3883376717567444,-0.10249295085668564,-1.0104087591171265,1.463610291481018,-0.11347747594118118,-0.2071211189031601,-0.2833584249019623,-2.041670083999634,-1.3733258247375488,0.494582861661911,-0.3839196264743805,1.7384434938430786,0.1826077550649643,-0.1588965803384781,0.3977043032646179,5.854877471923828,-1.7410308122634888,-1.3058226108551025,-0.9933998584747314,-1.1464840173721313,2.444709539413452,-1.1086978912353516,-0.13006195425987244,-0.2560560405254364,-0.8636187314987183,-1.5177695751190186,2.2234385013580322,-0.2940065562725067,-1.7674336433410645,2.4181418418884277,-1.3244057893753052,1.1734414100646973,-0.14124710857868195,0.5030307769775391,-0.05199946463108063,-0.09560124576091766,-1.3212882280349731,7.118120193481445,-2.482163906097412,0.5169393420219421,1.2234715223312378,-0.25559547543525696,2.9322926998138428,-0.11114627867937088,3.2001309394836426,-1.425274133682251,0.6336883902549744,1.6868990659713745,-0.6352441906929016,0.21736079454421997,-1.3740140199661255,1.2291611433029175,-0.607947587966919,1.3556867837905884,0.4393414556980133,-2.9266178607940674,0.19452816247940063,-0.9043981432914734,-1.3830267190933228,-1.5841068029403687,1.9858697652816772,-1.858829140663147,1.8335685729980469,-1.4454469680786133,-0.6667991876602173,0.1556018739938736,-0.8632345199584961,3.199312686920166,-3.0168542861938477,2.421955108642578,-0.25464898347854614,3.413557529449463,-0.07267118245363235,0.11649102717638016,-1.5244685411453247,0.9023750424385071,-0.8869962692260742,4.293290615081787,-0.26466768980026245,-1.6650094985961914,0.6344625353813171,-2.0458931922912598,1.0798090696334839,-1.7154710292816162,-1.0357319116592407,-0.34439629316329956,-1.6409804821014404,-0.7115128040313721,-1.4387001991271973,0.6448261141777039,1.6886934041976929,-1.925355076789856,-1.6030889749526978,-0.33171752095222473,0.42839235067367554,-0.21476298570632935,1.5397469997406006,0.4623199999332428,-1.6381405591964722,-1.0300520658493042,8.090343475341797,-1.0837777853012085,-1.2608990669250488,-0.512640118598938,0.9398011565208435,-1.3981314897537231,0.6026759147644043,-1.568787693977356,0.4311314523220062,2.3445305824279785,-0.28753629326820374,-1.7232365608215332,-0.03209640458226204,-2.9948084354400635,0.4169676601886749,0.09780202060937881,0.7413853406906128,-0.6662713289260864,-0.41921499371528625,-0.7361195087432861,-0.14185209572315216,-2.6069295406341553,-1.1121947765350342,-1.742814064025879,-0.6553170084953308,-1.0004932880401611,-0.8326088190078735,-2.7301201820373535,-1.9893184900283813,-1.1535015106201172,-0.8915966153144836,-0.8620468974113464,-0.007118698675185442,-1.5638902187347412,8.549805641174316,-0.9131694436073303,1.5778920650482178,-0.08132985234260559,-4.244309902191162,-0.13895004987716675,-1.1007033586502075,-0.9584058523178101,0.8171672821044922,-0.7843348979949951,2.7008883953094482,0.3190213143825531,-1.1646672487258911,-0.3795529305934906,1.4013862609863281,-0.44473615288734436,-2.820873498916626,0.006850795820355415,-1.3978723287582397,-0.4059794247150421,-0.9966478943824768,0.60068279504776,-1.211686372756958,-2.4956552982330322,1.5401804447174072,-0.7949087619781494,-1.2276393175125122,1.2199686765670776,-0.3917025625705719,-2.224783182144165,4.916191101074219,-0.5516659617424011,-1.1810826063156128,0.18537768721580505,-3.441711902618408,0.3843705654144287,-0.8497280478477478,-2.337404251098633,0.7463443875312805,-1.9213120937347412,-0.019301142543554306,1.9290469884872437,-0.20372748374938965,-1.0745315551757812,-1.188355803489685,-2.174067497253418,-2.411600351333618,-1.3594963550567627,-1.3032329082489014,3.9150753021240234,0.25925713777542114,0.725395143032074,0.4747764766216278,0.2042764276266098,0.3168828785419464,-0.16882340610027313,0.3136557638645172,-0.7821446061134338,-1.0599865913391113,-0.014872794970870018,-1.25669527053833,1.3380173444747925,-0.7613933086395264,-3.8691811561584473,2.9581594467163086,-0.3653809130191803,-0.4143165946006775,-0.7284279465675354,-2.1290388107299805,-3.458407163619995,-2.7332499027252197,-0.8743547797203064,3.015855312347412,0.9796146750450134,-1.5516608953475952,0.3624173104763031,-2.3164801597595215,2.4036285877227783,1.1705132722854614,-0.5455815196037292,-2.4559218883514404,-0.15297259390354156,1.3415688276290894,-1.1018954515457153,0.9779529571533203,-0.23772075772285461,-0.14496473968029022,-2.299729585647583,-1.215951919555664,0.9051223993301392,-1.0186057090759277,0.555249035358429,5.545913219451904,-0.2214042842388153,-1.440967082977295,-0.7847660183906555,-2.172415256500244,-0.2514992356300354,-1.063993215560913,1.310641884803772,-0.31981557607650757,-2.4938809871673584,1.1072219610214233,-2.7604637145996094,-0.19646912813186646,0.9125801920890808,-1.4499969482421875,1.0929334163665771,-3.477302312850952,-1.2113547325134277,-2.480590343475342,-1.0663621425628662,0.8514803647994995,1.4004807472229004,-0.37540382146835327,1.2584418058395386,0.44819530844688416,3.0755910873413086,0.38696712255477905,4.0131731033325195,-0.8269098401069641,-1.801666498184204,-1.5448347330093384,-2.4182283878326416,1.3341689109802246,2.9800214767456055,-0.2707075774669647,-0.5784804224967957,0.20845885574817657,-0.9341582655906677,-0.5468148589134216,2.3161487579345703,0.471278578042984,0.5187970995903015,0.5491898059844971,-1.3052787780761719,-0.2789805829524994,-0.4508060812950134,-2.6584315299987793,-2.054248094558716,0.6713407635688782,-0.38080400228500366,0.6104239225387573,-0.031647954136133194,2.199390172958374,1.7883306741714478,-1.5468875169754028,1.4233136177062988,2.390549421310425,-0.4737952947616577,-1.3333007097244263,-1.0202267169952393,-2.114994525909424,-2.0606935024261475,0.5505120158195496,1.6302995681762695,-0.2470138967037201,-2.40714955329895,-1.1674925088882446,-1.469799280166626,-0.052817195653915405,-1.144209861755371,2.1700148582458496,-0.5382607579231262,-0.32487761974334717,-1.437450647354126,-2.1894471645355225,1.271671175956726,0.6776050925254822,-0.7890728712081909,5.6202073097229,-0.34077754616737366,8.706500053405762,-2.7285399436950684,-1.603176474571228,2.300198554992676,1.9147793054580688,-2.3607187271118164,-0.5403349995613098,-0.478810578584671,-1.6871460676193237,-0.012029399164021015,-3.191464900970459,-0.8458343148231506,-2.2959539890289307,2.4049253463745117,-0.18318721652030945,-1.6935310363769531,-0.49598023295402527,-2.302067995071411,-1.2120606899261475,1.2563328742980957,-0.7934291958808899,-1.2487794160842896,-2.145045280456543,-0.3561726212501526,-3.174468994140625,-0.5987757444381714,-1.5468062162399292,-0.39835911989212036,1.000091314315796,-0.5512329936027527,-1.5645281076431274,0.02484028972685337,0.5032944679260254,1.2131818532943726,0.0632871463894844,3.154080867767334,-0.7907707691192627,1.0587923526763916,-0.20932362973690033,0.6819322109222412,-0.019918378442525864,0.20582568645477295,-1.2121728658676147,7.359872817993164,0.03574218228459358,-0.674537718296051,1.4844270944595337,-2.8591744899749756,-0.24022439122200012,0.8029060363769531,0.1909157931804657,-1.9289438724517822,0.7204780578613281,-0.3623501658439636,-2.193955183029175,2.9601473808288574,-0.23956695199012756,-0.5937477350234985,0.07795583456754684,-0.3671358525753021,-0.3780892789363861,-0.4021965265274048,3.012455940246582,0.6404145956039429,-2.0387699604034424,-1.9566081762313843,-4.595430850982666,0.12156187742948532,2.1918041706085205,-2.135763168334961,1.4062001705169678,-2.559281587600708,-0.3335244953632355,0.9610165953636169,-1.6115946769714355,-0.8911693096160889,-0.07588610053062439,-1.680092692375183,-0.4619079530239105,-0.6218850612640381,1.5827761888504028,-1.3740437030792236,-2.339869976043701,-2.5729446411132812,-1.2471126317977905,-7.149811744689941,-1.179000735282898,0.36955732107162476,-1.526046633720398,3.5617828369140625,0.8906856775283813,0.7647688984870911,1.7759439945220947,0.6770845055580139,1.0130139589309692,4.934362411499023,-1.5106509923934937,-2.501269817352295,-1.5254124402999878,-0.9491108059883118,3.018089771270752,0.5710774660110474,3.148010730743408,-0.42077988386154175,-3.2905547618865967,3.019284963607788,0.22397546470165253,0.3323575556278229,0.5112428069114685,-0.7144317626953125,-2.2304022312164307,-0.43351539969444275,1.0836443901062012,3.6647088527679443,0.27024999260902405,0.48296093940734863,-0.4198133647441864,0.7675262093544006,-1.0715514421463013,0.23870599269866943,-1.8215234279632568,-0.3981037139892578,-0.16920962929725647,-0.7467479109764099,0.20206880569458008,-0.002273975405842066,-1.788756012916565,0.35382601618766785,0.1383884996175766,-1.7204464673995972,-1.325061559677124,-0.3536122739315033,-1.2713065147399902,0.9894593358039856,-0.958947479724884,1.3814231157302856,-0.37826260924339294,7.390936374664307,-0.6168436408042908,-0.06570407003164291,-1.0958350896835327,-1.9227800369262695,0.5664461255073547,-4.126997470855713,-1.7057772874832153,1.147822618484497,4.895805835723877,0.502360463142395,-0.15279339253902435,-1.0030653476715088,0.1916465163230896,0.7441128492355347,0.27848726511001587,0.35130900144577026,-0.3261820077896118,1.8404245376586914,-0.3061847984790802,0.9524356722831726,-0.3491772711277008,-0.2529062032699585,-1.4955527782440186,0.28526726365089417,-1.4684226512908936,0.151450514793396,0.021567489951848984,-1.1112053394317627,-1.7585963010787964,-0.27356868982315063,0.6712144613265991,0.4669351279735565,-1.918096661567688,0.09543977677822113,-0.373967707157135,-1.4821618795394897,-1.595978021621704,0.9429773092269897,0.4670545756816864,0.10011665523052216,-1.1699140071868896,-1.4213389158248901,-0.5252112150192261,-0.8509542942047119,-2.3508005142211914,-2.217132329940796,1.4699214696884155,-0.442288339138031,-1.280103325843811,0.18897485733032227,1.6773878335952759,-1.1291056871414185,-0.04266004636883736,-1.644282341003418,-1.4808579683303833,0.474290132522583,-0.6613855957984924,-0.2502445578575134,-0.5570675730705261,-0.8673990368843079,0.7632418274879456,0.27205565571784973,-0.4428441822528839,-1.671605110168457,-0.26848703622817993,-0.4526635706424713,1.5884361267089844,-2.238245725631714,-2.0342142581939697,1.1346560716629028,0.8109825253486633,0.07070156186819077,0.7715950012207031,-0.2778937518596649,0.13618063926696777,2.2592005729675293,-0.36649927496910095,-1.254396915435791,0.3034228980541229,0.6159879565238953,-0.3503926396369934,-1.0582973957061768,-1.2282047271728516,-0.8889654874801636,-0.5962289571762085,0.9351174831390381,-0.40471088886260986,-1.4518293142318726,-1.648988962173462,-2.1329269409179688,-1.7202746868133545,-2.1141819953918457,-0.2894560396671295,-1.6715434789657593,0.20814618468284607,-0.876399040222168,-1.8580878973007202,0.9363423585891724,-1.0018560886383057,-0.19788558781147003,-1.4238756895065308,0.07183416187763214,-0.23474465310573578,-2.5686118602752686,-2.9394145011901855,-0.7778609395027161,3.572441339492798,-1.115822196006775,0.9711669683456421,1.2733134031295776,0.08189983665943146,2.358959436416626,-0.9552849531173706,0.04897727444767952,-1.2896195650100708,-0.7894763350486755,-0.8006755113601685,-0.9133812785148621,-0.062183547765016556,-2.6554012298583984,-0.048119038343429565,-0.4167872667312622,0.3931817412376404,2.003138780593872,0.025667978450655937,2.022982597351074,-2.794497489929199,-0.4390004873275757,-0.33721116185188293,0.1221117451786995,0.8301476240158081,0.18935002386569977,-0.3181992769241333,-0.283921480178833,-0.7268338203430176,-1.9226312637329102,-0.35616037249565125,-0.47969502210617065,1.4370657205581665,-0.38343098759651184,-2.837512254714966,-0.16160091757774353,-2.1837282180786133,-0.8689142465591431,-0.32672804594039917,-1.9080506563186646,-0.2559746205806732,1.148103952407837,-0.6851395964622498,1.0715023279190063,-0.5416657328605652,-0.034513384103775024,-1.0832056999206543,-0.6280325055122375,-1.0163590908050537,-1.5267058610916138,1.1086682081222534,1.3010666370391846,-0.9916567802429199,-0.4707796275615692,2.180382013320923,-0.804837167263031,1.6596810817718506,-3.5365841388702393,-0.4630892276763916,0.9693955779075623,1.4738471508026123,-1.1173362731933594,-0.3675815463066101,2.0898804664611816,-1.0686429738998413,-0.16162461042404175,0.4085969030857086,-0.08812643587589264,-1.0253299474716187,2.1158666610717773,-1.823449730873108,-0.733421266078949,0.05046839639544487,-1.9275315999984741,-0.5685428380966187,-1.6563445329666138,-0.6172482371330261,0.0561019666492939,-0.37899693846702576,-0.3323589861392975,-1.4134560823440552,-1.169187307357788,-0.9484811425209045,0.435236394405365,-1.1738277673721313,5.186668395996094,-0.47060197591781616,1.5025888681411743,0.5515162944793701,0.569778561592102,-0.21427959203720093,0.9009957909584045,0.7195559144020081,1.6109232902526855,-0.473801851272583,-0.2074265033006668,0.29404252767562866,-0.9038199782371521,1.1514244079589844,-1.6847290992736816,-0.14120520651340485,-0.1639108657836914,2.3083767890930176,-2.8906805515289307,0.007180980406701565,-0.28620481491088867,0.24137994647026062,0.6962798833847046,0.15374131500720978,0.9040143489837646,-1.9575490951538086,-0.2656884491443634,0.47306767106056213,0.8770484328269958,-1.2393816709518433,-0.3988725543022156,0.24045401811599731,1.2948098182678223,1.0373010635375977,-2.3265085220336914,-0.880661129951477,-3.163947343826294,1.363277554512024,-1.9909518957138062,-0.309794545173645,-1.3167484998703003,1.3469462394714355,2.0289604663848877,-1.8274598121643066,-0.7715266942977905,1.0993578433990479,3.8765108585357666,-0.256178617477417,-0.802885115146637,0.05867256224155426,-1.0417059659957886,-1.0106937885284424,2.066790819168091,-2.2048532962799072,-1.5985760688781738,-0.7473490238189697,-1.6544485092163086,-1.0913372039794922,0.02777966298162937,-1.7668049335479736,-1.2811238765716553,-2.196761131286621,1.0229718685150146,-0.6317408084869385,-0.6075658798217773,0.5551788806915283,-2.3574702739715576,1.8312879800796509,-0.33596086502075195,1.5802136659622192,-1.0590912103652954,-0.1402990221977234,-0.4210309386253357,-1.2255408763885498,-0.5751221179962158,0.1623818278312683,-2.2832205295562744,-2.8629372119903564,-1.002299427986145,-1.5283739566802979,-1.361477017402649,-1.813640832901001,-0.3388311564922333,-0.7414246201515198,3.087148904800415,-0.49396637082099915,0.20206095278263092,1.0217983722686768,-0.8738589286804199,1.6171225309371948,-1.3904231786727905,0.03615768626332283,-0.595744252204895,-0.8874454498291016,-2.106886148452759,-0.795429527759552,-0.7316251993179321,-1.4865754842758179,0.09128230065107346,0.12284059822559357,-1.0588266849517822,0.4441768527030945,-4.589879035949707,-2.34090256690979,0.20030534267425537,-1.2907638549804688,-1.0555546283721924,0.14456354081630707,1.2877354621887207,-1.4920278787612915,0.11961767077445984,-0.40537524223327637,2.2677001953125,1.8526333570480347,-1.2104676961898804,-1.227502703666687,0.8397477865219116,0.7683325409889221,0.3594643771648407,-3.0956079959869385,-2.605076789855957,1.5294791460037231,-0.3004564046859741,-2.466944932937622,-0.6099533438682556,1.2772353887557983,-1.8068897724151611,-0.37631070613861084,1.0651473999023438,-0.4679594039916992,0.3529936969280243,2.079200506210327,-0.0436355285346508,-0.6732410192489624,-0.3740382194519043,-1.3803716897964478,1.6071237325668335,-0.34822019934654236,0.9377179741859436,0.6927573084831238,-1.022628664970398,-1.1362444162368774,0.3453879952430725,-1.364586353302002,-2.272489309310913,0.18365561962127686,2.147658586502075,-1.3054895401000977,2.1357076168060303,-0.28491589426994324,0.48412248492240906,-1.8006210327148438,-0.35170426964759827,-0.0543607659637928,-0.07571034133434296,-0.8514806032180786,0.5409597754478455,-2.015368938446045,-2.2101640701293945,0.29405590891838074,-0.32020336389541626,0.49084392189979553,-0.9682226181030273,-2.3499786853790283,-1.2350541353225708,0.0879359170794487,2.4769434928894043,1.9165823459625244,-0.7175482511520386,-1.1150883436203003,2.997817277908325,0.06219680234789848,2.190614700317383,-2.083462953567505,-1.3196423053741455,-0.342483788728714,-0.3094642460346222,-0.8395527601242065,-2.144526720046997,0.7764121890068054,0.0008806890109553933,-0.8346275687217712,0.3812846541404724,-0.6792272925376892,1.7080978155136108,-2.991227388381958,-2.66886830329895,-0.38605445623397827,-0.6992967128753662,1.0811687707901,-0.8853006958961487,-1.6812888383865356,-1.353052020072937,0.1375751942396164,0.7546644806861877,0.49934571981430054,1.2391958236694336,3.812870740890503,-2.1962671279907227,1.2247281074523926,2.544203996658325,-1.8904578685760498,0.10265333950519562,-1.2046096324920654,1.6039948463439941,2.86069393157959,-0.3697049021720886,1.2941032648086548,-0.16600869596004486,-0.016908258199691772,0.3912145495414734,-1.641771674156189,-0.23139706254005432,-0.11271821707487106,0.026850828900933266,-1.509600043296814,-0.2500000298023224,-0.15867537260055542,-0.8538815975189209,-0.3367430865764618,-0.2680688798427582,-0.8440436124801636,0.03533322364091873,-0.32640424370765686,-1.5931003093719482,1.216539978981018,0.27198532223701477,0.7553464770317078,-0.19966204464435577,-1.668746829032898,-1.0455230474472046,-0.6469347476959229,-0.7624735236167908,1.7598936557769775,-0.16373184323310852,-0.5577898621559143,0.2786221206188202,-0.43898651003837585,0.48278746008872986,-0.11744586378335953,-1.186222791671753,1.655806303024292,-1.1726694107055664,-1.0481805801391602,-1.817566990852356,-1.5634324550628662,-1.7068225145339966,-2.2859268188476562,-1.701348066329956,-2.310091972351074,-0.3810274600982666,-0.1105492040514946,-0.4593690037727356,1.8236712217330933,-0.9024597406387329,-0.16111721098423004,-1.7077547311782837,-0.589992105960846,0.05799414590001106,0.38950955867767334,-0.6674160361289978,-0.8095951080322266,0.32531648874282837,-0.7369950413703918,-0.39214831590652466,-0.49077779054641724,-0.09422826766967773,0.5047407150268555,-1.9517321586608887,-0.39988064765930176,-0.33934617042541504,0.037498388439416885,-1.544763445854187,2.9069385528564453,-0.6450715661048889,-1.1392872333526611,2.3996822834014893,-0.7641164660453796,-0.7429202795028687,-0.057923391461372375,-0.30243200063705444,-0.8947259187698364,-0.2701382637023926,-1.131911277770996,1.2557365894317627,-1.1539382934570312,-0.2806691527366638,-1.3271863460540771,-0.7212978005409241,-0.048300016671419144,0.10118163377046585,-0.8690924644470215,1.1386127471923828,-1.4793351888656616,-1.5135442018508911,0.07135006785392761,-1.1834625005722046,0.9680735468864441,0.29813113808631897,0.7114835977554321,-3.8112845420837402,1.1260409355163574,4.250321865081787,2.0941145420074463,0.19780568778514862,0.11652035266160965,-1.0144884586334229,1.7814170122146606,-0.13207729160785675,2.4450109004974365,0.2784784138202667,-0.006771680898964405,1.5088392496109009,1.6004103422164917,0.24628221988677979,-0.25714078545570374,-1.9221261739730835,0.8656426668167114,0.9507125616073608,5.680835723876953,-1.9444522857666016,-1.4084845781326294,1.265430212020874,0.6041169762611389,-0.19112877547740936,-1.0751643180847168,-1.2276369333267212,1.3306506872177124,-0.0750403106212616,3.179365396499634,0.7346416115760803,2.265760660171509,-0.4999571144580841,-0.44932520389556885,0.09968069195747375,0.40420231223106384,0.20829233527183533,1.7639379501342773,0.8398168087005615,0.27938807010650635,-2.812847137451172,-0.7747303247451782,-0.3097257912158966,2.1275031566619873,-0.45902007818222046,-0.5248798131942749,0.3124043345451355,0.9603634476661682,-0.32122063636779785,2.376966953277588,1.032634973526001,4.701135158538818,-0.24811463057994843,-0.3740227520465851],"xaxis":"x","y":[0.08004682511091232,0.2581094801425934,-0.022787364199757576,0.11586539447307587,0.3404651880264282,0.13877587020397186,0.002809178316965699,-0.029320964589715004,0.23866039514541626,0.09416169673204422,0.0030320577789098024,0.005320949014276266,0.07732073217630386,-0.03677183389663696,-0.06298241019248962,0.08178745955228806,0.02762743830680847,0.06667539477348328,0.16603870689868927,0.08278561383485794,-0.023437930271029472,0.07227206230163574,0.03414656221866608,0.07887625694274902,0.027796540409326553,-0.08809908479452133,0.4401950240135193,-0.013510900549590588,0.013899087905883789,0.021487411111593246,0.01174041535705328,0.03854789584875107,0.08569531887769699,-0.022769996896386147,0.11390959471464157,0.004631782881915569,0.019006550312042236,0.15960107743740082,0.056216761469841,0.24857527017593384,-0.04885849729180336,0.012423090636730194,0.04379298537969589,-0.11907333880662918,-0.11962392926216125,0.030872508883476257,0.006819779518991709,0.07838110625743866,0.31457796692848206,0.020106561481952667,-0.03276936337351799,0.07475467771291733,-0.14617325365543365,0.1440313160419464,0.20275574922561646,0.033594761043787,0.0003417323168832809,-0.0872066542506218,0.0035568547900766134,0.0696972906589508,0.08395678550004959,0.052616532891988754,-0.007519831880927086,-0.0338129848241806,0.09380943328142166,0.06140247359871864,-0.05822521448135376,0.00865860190242529,0.3508128821849823,-0.011836343444883823,-0.07476894557476044,-0.07479386776685715,-0.10324703902006149,0.06106938421726227,-0.16868199408054352,0.09946698695421219,0.04655396565794945,-0.06562523543834686,-0.047080669552087784,0.046678755432367325,0.048941150307655334,0.08338315039873123,-0.10673636943101883,0.17855624854564667,0.11190921068191528,-0.06272692233324051,-0.05662764981389046,-0.09479771554470062,-0.046370286494493484,0.020624255761504173,0.02837860770523548,0.13847948610782623,-0.1454184651374817,-0.051729701459407806,0.08872709423303604,0.13468298316001892,0.08019959926605225,0.052061960101127625,0.02359316498041153,0.005211601033806801,-0.0843532606959343,0.22645530104637146,0.05425076559185982,0.1377311795949936,-0.058579329401254654,0.04460322856903076,-0.03245242312550545,0.08796302229166031,-0.1322849839925766,0.10103777796030045,-0.05319397896528244,0.0658315122127533,0.035379741340875626,0.1353912651538849,-0.07334419339895248,0.18351876735687256,0.16526418924331665,-0.05535050854086876,-0.09457509219646454,0.007374193985015154,0.053806040436029434,-0.046718161553144455,-0.03430067375302315,0.06631772965192795,0.05988765135407448,0.16764667630195618,0.13812701404094696,0.05085182934999466,-0.023774949833750725,-0.05354246869683266,0.0442064143717289,-0.002933693351224065,-0.097112737596035,0.0015344335697591305,-0.07045230269432068,-0.05287259444594383,-0.10002095252275467,0.09739159047603607,-0.08027073740959167,0.0021151879336684942,-0.04366384074091911,-0.006080476101487875,-0.07385210692882538,0.06665199995040894,-0.010155903175473213,0.15009446442127228,-0.09820893406867981,0.14802660048007965,-0.03605034574866295,-0.06382250040769577,0.164453387260437,-0.019608095288276672,0.06961701065301895,0.34521302580833435,0.0717117115855217,0.004702763166278601,0.022701971232891083,0.24962376058101654,0.2999115288257599,-0.01724160462617874,0.12019498646259308,-0.07279147952795029,-0.06699575483798981,0.0483877919614315,0.05095723643898964,0.056782808154821396,0.007544154301285744,0.010026045143604279,-0.04758189246058464,-0.126411572098732,-0.02216486446559429,0.013477573171257973,0.01760423742234707,-0.11250031739473343,-0.0652129054069519,-0.06398189812898636,0.09356245398521423,-0.024689676240086555,-0.08567869663238525,0.09108112752437592,-0.03059368208050728,-0.09374354034662247,0.27016597986221313,-0.031513042747974396,0.23679421842098236,-0.10038107633590698,0.08754173666238785,-0.22978973388671875,0.030095556750893593,0.02843085676431656,0.24165228009223938,-0.01048397459089756,-0.028759382665157318,-0.06511881947517395,-0.04926888272166252,0.06950212270021439,0.06557579338550568,-0.0619836188852787,-0.04772695153951645,0.024821432307362556,-0.06087389588356018,-0.041788499802351,0.026106085628271103,-0.11976449191570282,-0.008624028414487839,0.04749318212270737,0.08238667994737625,-0.13167113065719604,0.006268836557865143,-0.11400747299194336,-0.01814034953713417,-0.053129784762859344,-0.02144624851644039,-0.029731810092926025,-0.08417733013629913,0.012974673882126808,0.10581473261117935,-0.0002649026282597333,0.04673871025443077,-0.028481295332312584,0.02808539941906929,0.02226637862622738,0.05729188770055771,-0.06183062121272087,-0.05038189888000488,0.05631934106349945,-0.04311053454875946,-0.1408185213804245,0.18162615597248077,-0.11642242968082428,-0.14223943650722504,-0.02787069045007229,-0.1098410114645958,-0.03563838452100754,0.04087129607796669,-0.03746654838323593,-0.06570176035165787,-0.03105689026415348,-0.0198442991822958,-0.04268237203359604,0.08536979556083679,-0.15693819522857666,0.0018360435497015715,0.05566992983222008,-0.111341692507267,-0.11398673057556152,0.029345888644456863,0.04376920312643051,-0.016678044572472572,-0.13928763568401337,-0.031725723296403885,0.038742367178201675,-0.04489181190729141,-0.10402530431747437,0.0190870501101017,-0.01465167198330164,0.004179208539426327,-0.09926708787679672,0.017770638689398766,-0.03325180336833,0.15579630434513092,-0.07584661990404129,-0.016862552613019943,-0.0013480738271027803,-0.03043167106807232,-0.03506099805235863,0.01301566045731306,0.023508267477154732,0.03349967673420906,0.045906346291303635,-0.1343340128660202,-0.024181846529245377,0.027540214359760284,0.021050412207841873,-0.08673778921365738,0.14529961347579956,-0.032251276075839996,-0.02939128316938877,-0.01495309453457594,-0.03142146021127701,-0.05816509947180748,-0.08034707605838776,-0.047418829053640366,0.1773771047592163,0.20862486958503723,0.17226716876029968,0.03898743912577629,-0.08254554122686386,-0.014138588681817055,0.03639960289001465,-0.06042866036295891,0.04059363529086113,0.09923006594181061,-0.06226840615272522,-0.04139181971549988,0.029605280607938766,-0.050170134752988815,0.22113661468029022,0.18399286270141602,0.04714579880237579,-0.06351740658283234,0.01346411183476448,-0.09643712639808655,0.0062287310138344765,0.3695586323738098,-0.009381764568388462,-0.06265495717525482,0.029418298974633217,0.022073302417993546,0.013055402785539627,0.04406265914440155,0.06739513576030731,0.09878215938806534,0.005364575423300266,-0.08632375299930573,-0.025808481499552727,0.035943105816841125,0.12285806238651276,0.005269091110676527,-0.025715000927448273,-0.17001448571681976,0.034110866487026215,0.0726240873336792,-0.005840077996253967,-0.014055734500288963,-0.054502297192811966,-0.10875450074672699,0.06297218799591064,0.14024943113327026,0.04797771945595741,0.035027794539928436,0.03477043658494949,0.03802601620554924,-0.12568530440330505,-0.046977508813142776,0.01879763975739479,0.05859275534749031,0.3715069591999054,-0.13417048752307892,-0.0490712970495224,0.08214648067951202,0.47252869606018066,0.09531920403242111,-0.09048682451248169,-0.00321478140540421,-0.005952292587608099,-0.08984938263893127,-0.028414176777005196,0.24483221769332886,-0.13596342504024506,0.11892275512218475,-0.053920213133096695,-0.035943176597356796,-0.06895618885755539,0.02696007303893566,-0.21525563299655914,0.1465112715959549,0.007831971161067486,0.00810443889349699,0.16265304386615753,0.0689292922616005,-0.09510351717472076,0.03247511014342308,0.0445655956864357,0.13714665174484253,0.007873866707086563,-0.07445890456438065,-0.006961209233850241,-0.06617888063192368,-0.04741138592362404,0.14263169467449188,-0.06241678074002266,-0.09532932192087173,0.00778069207444787,-0.035275932401418686,0.0607021264731884,-0.08446779102087021,-0.03497179597616196,-0.03720017522573471,-0.003406810574233532,-0.07214540243148804,-0.0939701497554779,-0.1382177770137787,-0.021334659308195114,0.0776626393198967,-0.18661345541477203,0.004941620398312807,0.030216427519917488,-0.01499305572360754,-0.05613192543387413,0.03201577067375183,0.44875654578208923,0.1666264832019806,0.1688268631696701,-0.02049005776643753,0.05813999101519585,0.025228921324014664,-0.05354281887412071,0.1163100153207779,-0.05897240340709686,-4.769708539242856e-05,-0.03884848579764366,-0.06926804780960083,0.05526971444487572,-0.07091844081878662,0.06392057240009308,0.3482179641723633,-0.10202935338020325,0.006116815377026796,0.028924817219376564,-0.037403449416160583,0.07141309231519699,0.04362040013074875,-0.0880025252699852,-0.13709470629692078,-0.02684197574853897,-0.054542288184165955,-0.04089144617319107,-0.06706051528453827,0.04944106191396713,-0.0547972097992897,-0.09102004766464233,0.049386315047740936,0.25170978903770447,0.002376655349507928,0.05309198424220085,0.029437271878123283,-0.06371055543422699,0.2436954826116562,-0.12690302729606628,0.18298910558223724,0.03317601978778839,-0.04078959301114082,0.0946226716041565,0.05703058838844299,0.005533786024898291,0.006132929585874081,-0.0008872051257640123,-0.04571288824081421,-0.1250416487455368,-0.06665623933076859,-0.0195530503988266,0.07238514721393585,-0.11361894756555557,-0.04779958724975586,-0.0930245891213417,-0.10873933881521225,-0.05385338142514229,0.09030956029891968,0.24997608363628387,0.09675803035497665,0.2695038914680481,0.09630826860666275,0.10505183786153793,0.07982334494590759,-0.1441337764263153,-0.11718736588954926,-0.011397448368370533,-0.03920511156320572,0.12543773651123047,0.21216055750846863,0.054996464401483536,0.07505901902914047,-0.05507408827543259,-0.08087605983018875,0.05228010565042496,0.09906282275915146,-0.10436247289180756,0.07128164917230606,0.11748123168945312,0.013636278919875622,0.0948595404624939,-0.12328815460205078,-0.011580144986510277,-0.10081088542938232,-0.04049323499202728,0.033920858055353165,0.015513049438595772,-0.01600688509643078,0.07926078885793686,0.036646999418735504,-0.012578385882079601,-0.11419621109962463,-0.06093888729810715,-0.17061364650726318,0.046372588723897934,0.3574894368648529,-0.050427768379449844,0.21720464527606964,-0.007497474551200867,0.22362558543682098,-0.03936266526579857,-0.06394671648740768,0.08976005762815475,-0.10992326587438583,0.10573089122772217,0.10104400664567947,0.06836733967065811,-0.010730867274105549,0.04841065779328346,0.0878453478217125,-0.105928935110569,-0.06801991909742355,-0.05617453530430794,-0.07661647349596024,0.1843971312046051,-0.05601188540458679,0.09589895606040955,-0.015362009406089783,-0.16772212088108063,0.2505173981189728,0.0038300587330013514,0.22968778014183044,-0.044498056173324585,-0.041118375957012177,-0.016533231362700462,0.10515487194061279,0.05639050900936127,0.21012289822101593,0.06948281079530716,-0.01025918684899807,-0.0540902353823185,0.0890992283821106,-0.010915374383330345,-0.05661766231060028,-0.012043113820254803,-0.07091707736253738,-0.0681006908416748,-0.048800546675920486,0.14132705330848694,-0.11343105882406235,0.014866632409393787,-0.0018766515422612429,0.20195263624191284,-0.041626766324043274,-0.007842320017516613,0.006388331763446331,0.11552722752094269,-0.017091110348701477,-0.013369093649089336,0.13502804934978485,-0.1396283209323883,-0.04016594961285591,-0.05583209916949272,0.06661083549261093,-0.079491525888443,0.0050405459478497505,0.08908982574939728,-0.05542483180761337,0.09338290244340897,-0.007707405835390091,0.01756991632282734,0.032352663576602936,0.07290830463171005,-0.1280255913734436,-0.017546214163303375,0.09570274502038956,-0.09648147225379944,-0.1074758768081665,0.0062482538633048534,0.3868105411529541,-0.1646423041820526,0.025385888293385506,0.07855900377035141,0.14070139825344086,-0.042692750692367554,0.05111931636929512,0.01638837344944477,-0.11280354112386703,-0.03830740600824356,0.0679226815700531,-0.019841637462377548,-0.0050132786855101585,-0.003834990318864584,-0.079358771443367,0.03894630819559097,0.019431620836257935,-0.08296933025121689,-0.007533653639256954,0.05320656672120094,-0.03355305269360542,-0.0424228273332119,-0.011743206530809402,0.06310834735631943,0.054687052965164185,0.1512223184108734,0.01943756826221943,-0.09773092716932297,-0.18696114420890808,-0.03209376335144043,0.17144091427326202,0.2030331939458847,-0.044647466391325,0.11269429326057434,0.15837492048740387,0.014893446117639542,0.0998077467083931,0.0028159618377685547,-0.10420212149620056,0.029912522062659264,-0.13247287273406982,-0.12727567553520203,0.04648836702108383,-0.08595738559961319,-0.1359921395778656,0.14348682761192322,-0.043454479426145554,-0.049776095896959305,0.10089355707168579,0.011424751952290535,0.016674181446433067,0.16755008697509766,0.18076196312904358,0.009787483140826225,0.016901109367609024,0.05809911713004112,0.05778907239437103,-0.08944475650787354,-0.11578555405139923,-0.031191207468509674,-0.014673993922770023,0.07689832895994186,-0.10396091639995575,-0.006477463990449905,-0.2175016552209854,-0.08994971960783005,0.09049712121486664,-0.072626031935215,-0.02341669239103794,0.005483950022608042,-0.044893622398376465,-0.021146554499864578,-0.14335601031780243,0.03668392077088356,-0.12083261460065842,0.17029646039009094,-0.1815267652273178,-0.04093898460268974,-0.12124767154455185,0.07463840395212173,-0.059466082602739334,0.17261892557144165,-0.0372454933822155,-0.07925001531839371,0.15421529114246368,-0.0034283632412552834,-0.03634323552250862,-0.06351267546415329,0.03384476155042648,0.47418278455734253,-0.0037377746775746346,-0.0968504250049591,-0.07437239587306976,0.044093240052461624,-0.013473113998770714,0.08780775219202042,0.02825544960796833,-0.10391191393136978,0.14937321841716766,-0.04892384633421898,-0.12395703792572021,-0.0036605747882276773,-0.08568920940160751,-0.054661720991134644,0.40391233563423157,-0.12097898125648499,0.015155339613556862,0.017893003299832344,-0.13545648753643036,0.49386656284332275,-0.09555842727422714,-0.073512963950634,-0.02272162213921547,-0.017212722450494766,0.025462916120886803,-0.04735329747200012,0.18053942918777466,0.036452360451221466,-0.12475351989269257,-0.023942437022924423,0.002294838661327958,-0.055196262896060944,0.08502618223428726,-0.09285019338130951,0.026920245960354805,0.03993001580238342,-0.22711271047592163,0.14246907830238342,-0.052722301334142685,-0.00939252134412527,-0.05283783748745918,0.00952377449721098,0.03563828393816948,0.03535252436995506,-0.04461643844842911,-0.018944241106510162,0.11126544326543808,-0.09803162515163422,-0.047229278832674026,-0.1534319519996643,0.16138726472854614,-0.07878030836582184,-0.06885411590337753,0.027809692546725273,-0.09691199660301208,0.10465563833713531,0.05227542668581009,-0.11065959930419922,-0.012397024780511856,-0.10251569002866745,-0.0007412339909933507,0.09050094336271286,0.028523780405521393,0.050513558089733124,-0.044949304312467575,-0.036324676126241684,-0.03380720689892769,-0.12864266335964203,0.054252322763204575,0.033490683883428574,0.04063361883163452,-0.04783954843878746,0.11693847924470901,0.03799450024962425,0.08181358128786087,-0.05710674077272415,-0.029317887499928474,-0.02640414610505104,-0.06496936827898026,-0.026529865339398384,0.015643958002328873,0.012544247321784496,-0.03336707130074501,-0.07439973950386047,-0.05370849743485451,0.03517059609293938,0.12439364194869995,-0.04598841443657875,-0.03151825815439224,0.10912975668907166,-0.08482345938682556,0.13247299194335938,0.05690646544098854,-0.0641612559556961,-0.15395788848400116,0.12683987617492676,0.04873661696910858,0.015809249132871628,0.035870544612407684,0.1222354844212532,-0.04070824384689331,0.13848969340324402,0.02295052818953991,-0.0618550069630146,-0.048692405223846436,-0.033039357513189316,0.12951882183551788,0.052974000573158264,-0.013946077786386013,0.03234619274735451,-0.022338293492794037,-0.04803602024912834,0.05599885433912277,-0.11394049972295761,0.15864533185958862,-0.11085515469312668,0.07660036534070969,-0.03819536045193672,0.22235140204429626,0.12079428136348724,-0.08586588501930237,0.0641024112701416,-0.0893377736210823,-0.002117825672030449,0.013105104677379131,-0.08457696437835693,-0.15510377287864685,0.07365202903747559,-0.05486910417675972,-0.006035011727362871,0.021930022165179253,-0.04878109320998192,0.018055051565170288,-0.12582287192344666,0.027101712301373482,0.16724543273448944,0.054670196026563644,-0.1662984937429428,-0.04634511098265648,-0.043104711920022964,-0.12146880477666855,-0.10778999328613281,0.18433146178722382,0.020581979304552078,0.40781381726264954,-0.02879265323281288,-0.09078219532966614,-0.06524046510457993,-0.077786885201931,-0.007788928225636482,-0.06311526149511337,-0.07459414750337601,0.028056861832737923,-0.15458448231220245,-0.05791972950100899,0.11489569395780563,-0.046813517808914185,-0.038489244878292084,0.022023364901542664,0.03439633175730705,0.01041394378989935,0.14888300001621246,-0.04250463470816612,0.008594845421612263,-0.13329316675662994,-0.1549232006072998,-0.08956621587276459,-0.008461939170956612,-0.07960787415504456,0.05285393074154854,-0.1141672432422638,0.06797175109386444,0.1065891832113266,-0.01194562017917633,0.11169525235891342,0.09082503616809845,-0.0013084812089800835,0.04415891692042351,-0.18181410431861877,-0.041905101388692856,-0.03697796165943146,-0.11157834529876709,0.11546429991722107,0.07069966942071915,-0.05597817152738571,0.08888338506221771,0.19914624094963074,-0.0993732213973999,0.042150892317295074,0.0565062053501606,-0.008157218806445599,-0.16058677434921265,-0.022648947313427925,-0.06206066161394119,0.054087720811367035,-0.015492990612983704,0.060995932668447495,-0.022813783958554268,0.0411934070289135,0.027506669983267784,0.01077469065785408,-0.15571057796478271,0.11772584170103073,-0.15985706448554993,0.0986289530992508,0.067299485206604,-0.06855504214763641,0.03643446043133736,-0.06392761319875717,-0.08942849934101105,0.08906403183937073,0.3308553099632263,0.009051398374140263,-0.05141017958521843,-0.11485037207603455,0.019836625084280968,0.0190335251390934,0.27521803975105286,-0.09271574765443802,0.03037264570593834,-0.04707687720656395,0.07352907210588455,0.08314987272024155,-0.052137427031993866,-0.007811917923390865,0.050059810280799866,0.3119734823703766,-0.061925966292619705,-0.08721676468849182,-0.09121900051832199,0.025085540488362312,-0.06304781138896942,0.020878871902823448,-0.07377404719591141,-0.06414084136486053,-0.018380379304289818,0.02518833614885807,-0.08952831476926804,-0.10836032032966614,-0.13229335844516754,-0.028091510757803917,-0.034814827144145966,0.015638558194041252,0.09059204906225204,0.027180908247828484,-0.03850751370191574,0.03931957110762596,0.11185331642627716,-0.05881282314658165,-0.038577549159526825,-0.08399707078933716,-0.014743836596608162,-0.07200891524553299,-0.002619741251692176,0.28181663155555725,0.03991067782044411,-0.15104925632476807,-0.1200346127152443,0.24448595941066742,0.07811317592859268,0.07324926555156708,-0.03474181890487671,-0.01410360261797905,0.12104243040084839,-0.012226906605064869,-0.16338083148002625,-0.14804431796073914,-0.05869339406490326,0.02001245506107807,-0.02373398281633854,-0.09875896573066711,-0.07977914065122604,0.05249996483325958,0.006850845646113157,0.014135674573481083,-0.06691272556781769,0.02531592920422554,-0.03267133608460426,0.033951349556446075,0.06599581986665726,0.032124392688274384,0.012454358860850334,0.06498806178569794,-0.007722504902631044,-0.005324073601514101,0.10098908841609955,-0.06916934996843338,-0.14676938951015472,-0.019327005371451378,0.016865646466612816,-0.06941527873277664,0.17229892313480377,-0.031900666654109955,0.13763847947120667,-0.06451062858104706,-0.08781703561544418,0.09180216491222382,-0.06880713254213333,0.15853241086006165,-0.043613094836473465,0.03035193495452404,-0.024339353665709496,-0.03072962909936905,0.17642726004123688,0.10328024625778198,0.009061935357749462,-0.07423709332942963,0.013348417356610298,-0.04436371102929115,0.07904919236898422,-0.1990717351436615,-0.19889479875564575,0.056498974561691284,-0.0061327614821493626,0.021679067984223366,0.08227480947971344,0.0883805975317955,0.19041608273983002,-0.19432619214057922,0.1175784170627594,-0.01065466832369566,-0.08752012252807617,0.11265160143375397,0.06450386345386505,-0.02020338363945484,-0.19928593933582306,0.07546461373567581,-0.11643697321414948,-0.05993732437491417,-0.013002918101847172,-0.054511163383722305,0.09002408385276794,0.0053484817035496235,-0.03665979951620102,-0.05251764506101608,0.18114706873893738,-0.029134340584278107,0.010195210576057434,-0.23935599625110626,-0.0866289883852005,0.011487229727208614,0.03252345323562622,-0.09572461247444153,-0.051927998661994934,-0.05041435733437538,-0.05319215729832649,-0.006784459576010704,0.12932054698467255,0.03056078590452671,0.059939440339803696,-0.05337045341730118,0.040916990488767624,-0.09878160059452057,0.04138651490211487,-0.0632658302783966,-0.10926591604948044,0.06724836677312851,-0.08487991243600845,0.05987963825464249,-0.1783229559659958,0.018836382776498795,0.019947338849306107,-0.029391726478934288,0.0537668913602829,-0.013111254200339317,-0.12115097045898438,-0.020230012014508247,-0.0373469814658165,0.07033925503492355,-0.0023990608751773834,-0.11522246897220612,0.0017991631757467985,0.008484606631100178,-0.006475473288446665,0.064663365483284,0.033076558262109756,0.10412788391113281,0.08532790839672089,-0.08157386630773544,-0.03351030871272087,0.17215488851070404,-0.08821242302656174,0.012499126605689526,-0.023314526304602623,-0.06611786782741547,-0.007100187707692385,0.005923101678490639,-0.06195494532585144,-0.018449025228619576,-0.025526873767375946,-0.0015566571382805705,0.07259859144687653,-0.028808237984776497,-0.08461035788059235,-0.01791089028120041,0.0114045524969697,-0.09448471665382385,-0.1656416952610016,0.22951681911945343,0.15228408575057983,-0.045085083693265915,-0.022652046754956245,0.03263029083609581,-0.16286371648311615,-0.0021994614508002996,-0.031085660681128502,0.19091074168682098,-0.09674622863531113,0.03817903995513916,0.04126635938882828,-0.13851316273212433,-0.05028166621923447,-0.18804670870304108,-0.01756604202091694,0.04014861583709717,-0.005819153506308794,0.01932264119386673,-0.024233194068074226,0.039916373789310455,-0.13539130985736847,-0.04180476814508438,-0.06528442353010178,0.07626035064458847,0.06486921012401581,0.04919441044330597,0.03270382061600685,-0.05348457768559456,0.057526394724845886,0.009025823324918747,-0.09020968526601791,0.04071103036403656,-0.025760367512702942,-0.0053306580521166325,-0.07431302219629288,-0.13162514567375183,0.08561825752258301,-0.04660281166434288,-0.14087975025177002,0.09302465617656708,0.017578426748514175,0.05462590977549553,-0.1612982600927353,-0.06471133977174759,-0.13638287782669067,0.09509401768445969,0.04143545776605606,-0.07566025108098984,-0.034691378474235535,0.04509440064430237,0.12064854800701141,0.12708711624145508,-0.02278672344982624,0.09946421533823013,0.0729258581995964,-0.07154088467359543,-0.023727456107735634,0.001851538079790771,-0.059053245931863785,-0.11678962409496307,0.13784103095531464,0.2108825445175171,0.20651793479919434,0.0468367263674736,0.07368409633636475,0.018270334228873253,-0.1320003718137741,-0.008531780913472176,0.0036959864664822817,-0.0363561175763607,-0.11722593754529953,0.10396981984376907,0.05368814244866371,0.019904525950551033,-0.09282199293375015,0.28498920798301697,-0.21790407598018646,-0.06787551939487457,-0.11297252029180527,0.04802940785884857,-0.0606132447719574,0.030685871839523315,-0.008844446390867233,0.08811833709478378,0.02922428958117962,-0.004174470901489258,-0.08301252871751785,-0.018450215458869934,0.006603133399039507,0.14145256578922272,-0.10118964314460754,0.08105728030204773,-0.04185087978839874,-0.002591792494058609,-0.03846074268221855,0.1466870754957199,0.2287532389163971,-0.04550332576036453,0.025955190882086754,-0.06809064000844955,-0.1159169152379036,-0.019432462751865387,0.015268342569470406,0.028701666742563248,-0.07674234360456467,0.026187298819422722,-0.17989598214626312,0.05474190041422844,-0.06967814266681671,0.024465030059218407,-0.030601702630519867,-0.004103201441466808,0.012252927757799625,-0.08113763481378555,0.03414105623960495,0.017011987045407295,0.039484091103076935,-0.09325046837329865,0.020046312361955643,-0.07178452610969543,-0.02433129958808422,0.11464346200227737,0.0800277441740036,-0.12404078990221024,0.06662482768297195,0.06972386687994003,0.097887322306633,-0.09579908847808838,-0.03394577279686928,-0.11815284192562103,-0.010412665084004402,-0.01090833730995655,-0.01090050209313631,-0.0199272520840168,-0.12418543547391891,0.05165568366646767,-0.06303030997514725,-0.19877801835536957,0.07252051681280136,0.0941566452383995,-0.009365387260913849,-0.023622829467058182,0.02991170436143875,0.014198721386492252,-0.04712355136871338,0.0478045754134655,-0.041007522493600845,-0.07539337128400803,0.01608888991177082,-0.024335000663995743,-0.02754945307970047,-0.09743790328502655,0.008607330732047558,-0.04605037719011307,-0.03365488350391388,-0.0867781713604927,-0.0377214252948761,0.016053028404712677,-0.012646617367863655,0.11697380244731903,-0.14397640526294708,-0.006201886106282473,0.010110038332641125,-0.06900658458471298,0.03964364156126976,-0.018035460263490677,0.14799968898296356,-0.08676005899906158,-0.06022431328892708,-0.15462170541286469,0.03186216950416565,-0.09258393198251724,-0.06134559586644173,0.1970280110836029,-0.0976090207695961,-0.03360594063997269,-0.05559728294610977,-0.025445524603128433,0.0380612388253212,-0.05392742156982422,-0.0403079129755497,-0.08198147267103195,-0.03767542541027069,0.10158815234899521,0.18302182853221893,-0.07868816703557968,-0.08040163666009903,-0.13572978973388672,-0.023621754720807076,0.05440818518400192,0.062360506504774094,-0.006890390533953905,0.11547642201185226,-0.08784282952547073,0.052669256925582886,-0.1254415065050125,-0.018588315695524216,0.060828614979982376,-0.0015571474796161056,0.04745319113135338,-0.04190187528729439,-0.05805445462465286,0.11252903938293457,0.036366529762744904,0.1795947104692459,0.013579395599663258,-0.06535720080137253,-0.0074750641360878944,0.03654928877949715,0.0509357713162899,-0.01856977678835392,-0.15882231295108795,-0.03611903265118599,-0.1941857784986496,-0.09730861335992813,-0.04122335836291313,-0.10385336726903915,0.06894125789403915,-0.04079804569482803,0.04684526100754738,0.04488866776227951,-0.10428764671087265,-0.07129161804914474,-0.005634790286421776,0.0677243247628212,0.11574926972389221,-0.031433332711458206,-0.2104305773973465,-0.08328507840633392,-0.1855982393026352,-0.01942610554397106,0.01715126633644104,-0.023725101724267006,-0.030157066881656647,-0.09394031763076782,0.048766884952783585,-0.01721636950969696,-0.004801795817911625,-0.1600893884897232,-0.07578226178884506,-0.05629797652363777,-0.06267797946929932,0.0008346876129508018,-0.11588652431964874,-0.005787745118141174,0.03234848007559776,-0.040456622838974,0.0456693209707737,0.16933634877204895,0.0004299389256630093,0.06899620592594147,0.008823469281196594,-0.014388947747647762,-0.027025295421481133,-0.053347375243902206,0.018964670598506927,-0.014994933269917965,-0.008407790213823318,-0.04764730855822563,0.08885404467582703,-0.05281303822994232,0.06529665738344193,0.08091703057289124,-0.1928299367427826,-0.09110894054174423,0.02357814647257328,0.14329878985881805,-0.03155592828989029,0.16609856486320496,0.23826797306537628,-0.02431485429406166,-0.13985438644886017,-0.116068534553051,0.04400825873017311,0.06277243793010712,-0.04586843028664589,-0.004851579200476408,0.0002512199862394482,0.06224219501018524,0.008177138864994049,-0.024714186787605286,-0.027348848059773445,-0.08805704116821289,0.006689266301691532,-0.05033829063177109,0.02112826332449913,0.016066784039139748,-0.07926064729690552,-0.05806596204638481,0.11828073114156723,-0.016640106216073036,-0.09058874845504761,0.052944768220186234,0.008253587409853935,0.06966014951467514,-0.0029383893124759197,-0.2075580209493637,0.11455245316028595,0.0898372158408165,-0.1356627643108368,-0.08391532301902771,-0.03958955779671669,-0.003434529062360525,0.025716427713632584,0.07530216872692108,-0.0723612830042839,-0.04507311433553696,-0.037182245403528214,-0.06089261546730995,-0.10862916707992554,0.07589361816644669,0.23529210686683655,0.032562144100666046,0.0784803256392479,-0.021732136607170105,-0.02801516465842724,-0.09314311295747757,-0.1368088573217392,-0.053891003131866455,0.09279458224773407,0.005903009790927172,0.0015916058328002691,0.2982845604419708,0.05358976125717163,-0.07810638099908829,0.01965891569852829,-0.0674280896782875,-0.010917611420154572,0.03939175605773926,-0.033984631299972534,-0.12811221182346344,-0.046354684978723526,0.03817376866936684,0.050515349954366684,0.41272276639938354,-0.09913021326065063,0.07045108079910278,0.24038152396678925,-0.1251499205827713,-0.05310053005814552,0.050285667181015015,-0.04356565698981285,0.09834320843219757,-0.009815366007387638,-0.020416591316461563,0.007514864671975374,-0.042607709765434265,-0.03485199436545372,-0.0972328707575798,0.0029633031226694584,-0.13111823797225952,-0.0171565692871809,0.011985155753791332,0.32957619428634644,0.11189785599708557,0.056121859699487686,-0.029664792120456696,-0.01778552122414112,-0.16976584494113922,-0.016873687505722046,-0.040066804736852646,-0.08650567382574081,-0.014348415657877922,-0.04180300608277321,0.015141202136874199,0.049185510724782944,0.16368678212165833,-0.05674640089273453,-0.13261771202087402,-0.10331571847200394,-0.05018475279211998,-0.054160792380571365,0.23004484176635742,0.004834665451198816,-0.04919916018843651,-0.022098993882536888,0.036990225315093994,-0.0021313398610800505,-0.07161875814199448,-0.015789099037647247,-0.15611864626407623,0.014057986438274384,-0.08841557055711746,-0.06772731244564056,-0.10409406572580338,-0.02034669555723667,0.06719792634248734,-0.17464910447597504,-0.008494967594742775,-0.08209706097841263,-0.06739050894975662,0.023665888234972954,-0.013579907827079296,0.0032913621980696917,0.1970912218093872,-0.056944962590932846,-0.03190311789512634,0.08530700951814651,0.23397758603096008,0.06084482744336128,-0.019471779465675354,-0.12710212171077728,0.0006803992437198758,-0.043596770614385605,-0.027528919279575348,0.2488735169172287,0.05081911012530327,-0.017781300470232964,0.04152693226933479,0.07834926247596741,-0.1555292159318924,0.36679965257644653,0.009184292517602444,0.07962006330490112,-0.0041119614616036415,-0.08348194509744644,-0.09580966830253601,0.013659440912306309,-0.016074320301413536,-0.06323657184839249,0.04063156992197037,-0.22051078081130981,-0.08642902970314026,-0.1918719857931137,-0.03990974277257919,-0.06052428483963013,0.04313096031546593,-0.02651323936879635,0.005764254834502935,-0.037829551845788956,0.06496971845626831,-0.07797519862651825,-0.06663957983255386,-0.08563723415136337,-0.08451006561517715,-0.008354640565812588,-0.006217326037585735,-0.10855037719011307,0.13616424798965454,-0.05660031735897064,-0.023048287257552147,0.04790718853473663,0.07970954477787018,-0.08299845457077026,-0.06945282220840454,-0.009556801989674568,-0.07632972300052643,0.18517079949378967,0.03609292209148407,0.04776777699589729,0.25378918647766113,0.08576634526252747,0.041327279061079025,-0.06161780282855034,0.03145606070756912,-0.10802903771400452,-0.13729432225227356,-0.036929503083229065,-0.08635752648115158,0.07103539258241653,0.1388072520494461,-0.12328192591667175,-0.016099700704216957,-0.07809499651193619,-0.039504632353782654,0.1936035007238388,0.033389076590538025,-0.08984874933958054,-0.018999939784407616,-0.019056640565395355,-0.13248960673809052,0.09415893256664276,-0.086515873670578,0.019537724554538727,0.20619139075279236,-0.058879464864730835,-0.09039871394634247,-0.08458834141492844,-0.040986429899930954,-0.027515478432178497,0.03785727545619011,-0.07186970859766006,-0.06079050526022911,0.0887603908777237,-0.19400084018707275,-0.05725187063217163,-0.04461418464779854,-0.028137721121311188,0.0574350468814373,-0.008625530637800694,-0.03935737907886505,-0.14038260281085968,-0.0440957210958004,-0.03137657791376114,-0.028521373867988586,0.0777471661567688,0.08978238701820374,0.07637715339660645,-0.1168828159570694,0.043885502964258194,0.019369641318917274,-0.09737968444824219,0.008892687037587166,-0.08527754247188568,0.08351335674524307,-0.061277374625205994,0.015976393595337868,-0.028498247265815735,-0.059350576251745224,0.0017189470818266273,0.07754158228635788,0.007512236945331097,-0.0017722417833283544,0.0827493891119957,0.04442746192216873,-0.007311681751161814,-0.09635990858078003,-0.07798927277326584,-0.06396298855543137,-0.0639018788933754,0.07502728700637817,-0.051409050822257996,-0.015331488102674484,0.025059370324015617,-0.08308826386928558,0.04012291505932808,-0.10796957463026047,-0.06045196205377579,-0.14206379652023315,-0.019625011831521988,-0.07654055953025818,0.030176851898431778,0.0850914865732193,-0.028085336089134216,-0.04585660994052887,-0.05274362862110138,0.04659620672464371,-0.01746137998998165,-0.08824951201677322,-0.017228849232196808,0.05862165987491608,-0.10769324749708176,0.006940824445337057,-0.03424237295985222,-0.2025800496339798,-0.0054212030954658985,0.4208749532699585,-0.036734022200107574,-0.093352310359478,0.15173853933811188,-0.05716178938746452,0.08399812877178192,-0.050465404987335205,-0.03855709359049797,0.045618508011102676,-0.09308257699012756,-0.01707630231976509,-0.05089091882109642,-0.0033121772576123476,0.015636948868632317,0.0021152119152247906,-0.17484381794929504,0.05231829732656479,-0.15735355019569397,0.10551352053880692,-0.020032601431012154,-0.056038521230220795,-0.09586802870035172,-0.027132151648402214,-0.18360963463783264,-0.055202122777700424,0.01151347253471613,-0.08154945820569992,0.04577777162194252,-0.013012297451496124,-0.08033628761768341,-0.14073710143566132,-0.12738804519176483,-0.19551074504852295,0.03920073062181473,0.09041299670934677,0.041791658848524094,0.3223516345024109,0.06852352619171143,0.005966433323919773,0.07301270961761475,0.0518772155046463,0.021227410063147545,-0.014845051802694798,-0.03495582938194275,-0.009044948033988476,-0.0006673896568827331,-0.08545970171689987,0.003182042855769396,0.08374302834272385,0.06122567132115364,0.04841676726937294,0.0625295415520668,-0.021804092451930046,0.07825885713100433,-0.055699847638607025,-0.020953824743628502,-0.0029167416505515575,-0.047735393047332764,-0.16416816413402557,-0.18556198477745056,-0.09531629085540771,-0.05533771216869354,-0.15350984036922455,0.02738161012530327,-0.04215452820062637,0.04325719550251961,0.027426190674304962,0.013415450230240822,-0.028195109218358994,0.3206183612346649,-0.028851792216300964,-0.09737399965524673,-0.09120029956102371,0.017887916415929794,-0.07401571422815323,0.025855276733636856,-0.021984579041600227,-0.07054142653942108,-0.08122829347848892,-0.09314867854118347,0.18252156674861908,0.016872907057404518,-0.15133167803287506,-0.0568472184240818,-0.09475152939558029,0.05422516167163849,0.026930028572678566,0.04935454949736595,-0.029324525967240334,0.1275828331708908,-0.005881867837160826,-0.0556773878633976,-0.09382947534322739,0.007249752525240183,-0.13147857785224915,-0.057413533329963684,-0.040038105100393295,0.0436578094959259,-0.01787722297012806,0.04344319552183151,-0.05897042155265808,0.023023126646876335,0.09281323105096817,-0.05349283292889595,0.0009808398317545652,-0.06168893724679947,-0.01976424641907215,0.0010014430154114962,0.09473570436239243,-0.08077968657016754,0.07346595078706741,0.0460074245929718,-0.14769448339939117,0.0737505853176117,-0.0007674726657569408,0.013722648844122887,-0.09533820301294327,-0.10552067309617996,0.028491893783211708,-0.027031617239117622,-0.14501309394836426,0.02306964062154293,0.05990643426775932,-0.02914370410144329,0.10299909859895706,-0.02315310575067997,-0.05294135957956314,0.17267398536205292,-0.01542719453573227,0.025353601202368736,0.2564004957675934,0.01306444313377142,0.07729858160018921,-0.08196264505386353,-0.0653633251786232,-0.006566840223968029,-0.050502579659223557,-0.040083739906549454,-0.18929632008075714,-0.05755871534347534,-0.05323997884988785,0.04335111752152443,-0.046834275126457214,0.008232763037085533,0.08836334943771362,0.05993432551622391,-0.018265947699546814,-0.02957945689558983,-0.10070355236530304,-0.12821173667907715,-0.026279449462890625,-0.059849467128515244,0.1788741648197174,0.3176395893096924,0.0496215857565403,-0.06598283350467682,-0.05764280632138252,0.13370297849178314,-0.06451714783906937,-0.019940558820962906,0.13310417532920837,-0.07342956215143204,-0.0268584955483675,-0.07637452334165573,-0.014990611933171749,-0.10983847081661224,-0.09153260290622711,-0.1444176584482193,0.09587737172842026,0.04836821183562279,-0.12584882974624634,-0.04873083904385567,-0.1479986608028412,-0.10684238374233246,-0.10999183356761932,0.011124085634946823,-0.0282376017421484,0.28511881828308105,-0.05921337008476257,-0.09634725749492645,0.07733943313360214,-0.06602592766284943,-0.019874433055520058,-0.02892814576625824,0.0409993939101696,-0.042916689068078995,0.1041204184293747,0.030708467587828636,0.006364605855196714,-0.12222491949796677,-0.0721156895160675,0.021505985409021378,0.05313647538423538,-0.11370021104812622,0.1290479451417923,0.05996648967266083,0.07003705203533173,-0.026445215567946434,-0.025188591331243515,0.027609091252088547,-0.05763077735900879,-0.0070006828755140305,-0.03521911799907684,-0.015813807025551796,0.04061852768063545,-0.06388915330171585,-0.0367124117910862,-0.044950202107429504,0.0563582144677639,0.5378522872924805,-0.07787445187568665,-0.03466305509209633,-0.0865752249956131,0.08163068443536758,-0.07130755484104156,0.042052965611219406,-0.02673480100929737,-0.02349715866148472,-0.2808012366294861,-0.005055882968008518,0.004831287078559399,-0.012310279533267021,-0.06168877333402634,0.018168052658438683,0.04755449295043945,-0.10190392285585403,-0.011743101291358471,-0.05376902222633362,-0.032665301114320755,0.16636456549167633,0.09193313866853714,-0.051022808998823166,-0.058271896094083786,-0.1259547919034958,0.004176498856395483,-0.14795126020908356,0.07473985850811005,0.2068423330783844,-0.023030679672956467,-0.008996649645268917,0.01891319453716278,-0.014109820127487183,0.2296396940946579,0.12834911048412323,-0.10135027766227722,-0.1383959949016571,-0.08592869341373444,-0.03557049483060837,0.10677294433116913,-0.025845425203442574,-0.0026111588813364506,-0.014295419678092003,0.043559730052948,0.028127361088991165,0.04268142953515053,-0.038240209221839905,0.06482172757387161,0.13867470622062683,-0.13047906756401062,-0.1061287373304367,0.0255489032715559,-0.15481668710708618,-0.0257935281842947,-0.06285705417394638,-0.06128256767988205,-0.11010999977588654,0.05385884270071983,-0.056397438049316406,-0.08065029978752136,-0.01980900578200817,-0.08200642466545105,-0.034772198647260666,-0.031694769859313965,0.009063227102160454,0.011296750046312809,0.12907341122627258,-0.03987974673509598,-0.09616129845380783,-0.1620377153158188,-0.06039143353700638,0.08653394877910614,0.023198911920189857,-0.0021148051600903273,0.061863671988248825,-0.0648416206240654,0.045535437762737274,-0.09109807014465332,-0.10438203066587448,-0.025979066267609596,-0.10430324822664261,0.026085613295435905,0.009302270598709583,-0.03139485791325569,-0.031459975987672806,0.03204478695988655,-0.007886730134487152,-0.074825219810009,-0.022724544629454613,-0.048743922263383865,0.057665422558784485,0.052124906331300735,-0.02940080314874649,-0.12456553429365158,-0.02307555079460144,-0.12550613284111023,0.007697340101003647,-0.09636501222848892,0.039489299058914185,0.038662251085042953,-0.04559241607785225,-0.05492233484983444,0.015314140357077122,0.021714147180318832,-0.0027468956541270018,0.022347167134284973,-0.0231619942933321,-0.026675675064325333,-0.12477743625640869,-0.019113020971417427,0.16699405014514923,0.10175330936908722,0.005296226590871811,-0.04287338629364967,0.052484530955553055,-0.00164170959033072,0.005189888644963503,0.0162319578230381,0.00991574302315712,0.05581507831811905,0.03905785083770752,-0.032367534935474396,-0.07074800878763199,0.36560139060020447,0.09283116459846497,0.08051680028438568,-0.06857030093669891,-0.0219067744910717,-0.1648995727300644,0.012518363073468208,0.05757420137524605,-0.053027063608169556,-0.03100462816655636,0.06462093442678452,0.025603817775845528,0.024655772373080254,0.13402611017227173,-0.13975469768047333,0.07819811254739761,-0.12447323650121689,-0.07685327529907227,0.0022833191324025393,0.29467928409576416,-0.039598118513822556,0.009065555408596992,-0.02178005501627922,0.0265868678689003,-0.08852717280387878,-0.04622838646173477,0.01847134716808796,0.03282683715224266,-0.21282553672790527,-0.010480216704308987,-0.03703397884964943,-0.07426051795482635,-0.02232634462416172,-0.1852114200592041,-0.07130204141139984,-0.03797203302383423,0.03302086144685745,-0.10697482526302338,0.08352141082286835,-0.05626203492283821,-0.018699098378419876,-0.04256357252597809,-0.1539473533630371,-0.005080328322947025,0.05777829885482788,0.10950426757335663,-0.02093614637851715,-0.05315851792693138,0.12753169238567352,-0.04959316551685333,0.03523288294672966,-0.08636139333248138,-0.039380159229040146,-0.015509934164583683,-0.07857764512300491,-0.015974555164575577,-0.10326231271028519,-0.11488386243581772,-0.060700882226228714,-0.13497160375118256,0.01051998045295477,-0.04020218178629875,-0.06833125650882721,-0.08953677862882614,-0.03992399573326111,-0.050939254462718964,0.07792530208826065,-0.10539019852876663,0.062208421528339386,-0.028572626411914825,0.010318024083971977,-0.10893700271844864,-0.061095379292964935],"yaxis":"y","type":"scattergl"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"white","polar":{"bgcolor":"white","angularaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""},"radialaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""}},"ternary":{"bgcolor":"white","aaxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"baxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"caxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8","gridwidth":2},"yaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8","gridwidth":2},"zaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"white","subunitcolor":"#C8D4E3","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"x0"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"x1"}},"legend":{"tracegroupgap":0,"itemsizing":"constant"},"margin":{"t":60}},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('44f208d0-7ab6-4908-bdae-9bbb2d165f55');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                            </script>        </div>


</div>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="HW6_files/figure-html/cell-55-1-scatter.png" class="img-fluid figure-img"></p>
<figcaption>scatter.png</figcaption>
</figure>
</div>
</section>
<section id="final-takeaway" class="level1">
<h1>Final Takeaway</h1>
<p>That is all of creating a fake news classifier using Keras. Hope you found this tutorial interesting!</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>