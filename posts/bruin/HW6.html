<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.541">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Shaina Wang">
<meta name="dcterms.date" content="2024-03-11">

<title>myblog - Text Classification</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">myblog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Text Classification</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Week 10</div>
                <div class="quarto-category">HW6</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Shaina Wang </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">March 11, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>In this blog, I will be covering how to create a fake news classifier using the library Keras. We will be using Google Colab for this demonstration.</p>
<p>First, let’s begin by importing all the necessary libraries.</p>
<p>The current version of Keras we have is 2.15. However, for this text classification model, we would like to use Keras 3. To do so, let’s update the library!</p>
<div id="cell-3" class="cell" data-outputid="f5a91128-0cd6-4d9d-9820-3761d5212e22" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install keras <span class="op">--</span>upgrade</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.1.1)
Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)
Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.25.2)
Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)
Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.7)
Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.9.0)
Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.10.0)
Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.2.0)
Requirement already satisfied: typing-extensions&gt;=4.0.0 in /usr/local/lib/python3.10/dist-packages (from optree-&gt;keras) (4.10.0)
Requirement already satisfied: markdown-it-py&gt;=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (3.0.0)
Requirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (2.16.1)
Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;keras) (0.1.2)</code></pre>
</div>
</div>
<div id="cell-4" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> string</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> keras</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> utils, models, layers</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.layers <span class="im">import</span> TextVectorization</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> LabelEncoder</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.express <span class="im">as</span> px</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.io <span class="im">as</span> pio</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>pio.templates.default <span class="op">=</span> <span class="st">"plotly_white"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-5" class="cell" data-outputid="b114e171-8dc9-4357-e576-f69445f164f0" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(keras.__version__)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>3.1.1</code></pre>
</div>
</div>
<p>We can see that we now have Keras 3 imported!</p>
<p>After you are done, we can now import the dataset. We will be using data from the following github: “https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true”. Make sure to pass in the dataset using pd.read_csv.</p>
<div id="cell-8" class="cell" data-outputid="2d8894ce-bfef-413e-8e45-0de6fd6be8aa" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>train_url <span class="op">=</span> <span class="st">"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.read_csv(train_url)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>data.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">

  <div id="df-ba18219e-5a4f-4dd7-8e29-db39c5f0cd89" class="colab-df-container">
    <div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Unnamed: 0</th>
<th data-quarto-table-cell-role="th">title</th>
<th data-quarto-table-cell-role="th">text</th>
<th data-quarto-table-cell-role="th">fake</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>17366</td>
<td>Merkel: Strong result for Austria's FPO 'big c...</td>
<td>German Chancellor Angela Merkel said on Monday...</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>5634</td>
<td>Trump says Pence will lead voter fraud panel</td>
<td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>17487</td>
<td>JUST IN: SUSPECTED LEAKER and “Close Confidant...</td>
<td>On December 5, 2017, Circa s Sara Carter warne...</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>12217</td>
<td>Thyssenkrupp has offered help to Argentina ove...</td>
<td>Germany s Thyssenkrupp, has offered assistance...</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>5535</td>
<td>Trump say appeals court decision on travel ban...</td>
<td>President Donald Trump on Thursday called the ...</td>
<td>0</td>
</tr>
</tbody>
</table>

</div>
    <div class="colab-df-buttons">

  <div class="colab-df-container">
    <button class="colab-df-convert" onclick="convertToInteractive('df-ba18219e-5a4f-4dd7-8e29-db39c5f0cd89')" title="Convert this dataframe to an interactive table." style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 -960 960 960">
    <path d="M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z"></path>
  </svg>
    </button>

  <style>
    .colab-df-container {
      display:flex;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    .colab-df-buttons div {
      margin-bottom: 4px;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

    <script>
      const buttonEl =
        document.querySelector('#df-ba18219e-5a4f-4dd7-8e29-db39c5f0cd89 button.colab-df-convert');
      buttonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';

      async function convertToInteractive(key) {
        const element = document.querySelector('#df-ba18219e-5a4f-4dd7-8e29-db39c5f0cd89');
        const dataTable =
          await google.colab.kernel.invokeFunction('convertToInteractive',
                                                    [key], {});
        if (!dataTable) return;

        const docLinkHtml = 'Like what you see? Visit the ' +
          '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
          + ' to learn more about interactive tables.';
        element.innerHTML = '';
        dataTable['output_type'] = 'display_data';
        await google.colab.output.renderOutput(dataTable, element);
        const docLink = document.createElement('div');
        docLink.innerHTML = docLinkHtml;
        element.appendChild(docLink);
      }
    </script>
  </div>


<div id="df-b7040a77-94fb-4359-92e8-8eff4fbf9749">
  <button class="colab-df-quickchart" onclick="quickchart('df-b7040a77-94fb-4359-92e8-8eff4fbf9749')" title="Suggest charts" style="display:none;">

<svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 0 24 24" width="24px">
    <g>
        <path d="M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z"></path>
    </g>
</svg>
  </button>

<style>
  .colab-df-quickchart {
      --bg-color: #E8F0FE;
      --fill-color: #1967D2;
      --hover-bg-color: #E2EBFA;
      --hover-fill-color: #174EA6;
      --disabled-fill-color: #AAA;
      --disabled-bg-color: #DDD;
  }

  [theme=dark] .colab-df-quickchart {
      --bg-color: #3B4455;
      --fill-color: #D2E3FC;
      --hover-bg-color: #434B5C;
      --hover-fill-color: #FFFFFF;
      --disabled-bg-color: #3B4455;
      --disabled-fill-color: #666;
  }

  .colab-df-quickchart {
    background-color: var(--bg-color);
    border: none;
    border-radius: 50%;
    cursor: pointer;
    display: none;
    fill: var(--fill-color);
    height: 32px;
    padding: 0;
    width: 32px;
  }

  .colab-df-quickchart:hover {
    background-color: var(--hover-bg-color);
    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);
    fill: var(--button-hover-fill-color);
  }

  .colab-df-quickchart-complete:disabled,
  .colab-df-quickchart-complete:disabled:hover {
    background-color: var(--disabled-bg-color);
    fill: var(--disabled-fill-color);
    box-shadow: none;
  }

  .colab-df-spinner {
    border: 2px solid var(--fill-color);
    border-color: transparent;
    border-bottom-color: var(--fill-color);
    animation:
      spin 1s steps(1) infinite;
  }

  @keyframes spin {
    0% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
      border-left-color: var(--fill-color);
    }
    20% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    30% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
      border-right-color: var(--fill-color);
    }
    40% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    60% {
      border-color: transparent;
      border-right-color: var(--fill-color);
    }
    80% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-bottom-color: var(--fill-color);
    }
    90% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
    }
  }
</style>

  <script>
    async function quickchart(key) {
      const quickchartButtonEl =
        document.querySelector('#' + key + ' button');
      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.
      quickchartButtonEl.classList.add('colab-df-spinner');
      try {
        const charts = await google.colab.kernel.invokeFunction(
            'suggestCharts', [key], {});
      } catch (error) {
        console.error('Error during call to suggestCharts:', error);
      }
      quickchartButtonEl.classList.remove('colab-df-spinner');
      quickchartButtonEl.classList.add('colab-df-quickchart-complete');
    }
    (() => {
      let quickchartButtonEl =
        document.querySelector('#df-b7040a77-94fb-4359-92e8-8eff4fbf9749 button');
      quickchartButtonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';
    })();
  </script>
</div>
    </div>
  </div>
</div>
</div>
<p>When we first read in the data, we see that there is an extra column in addition to title, text, and fake. We can simply remove that column because that is not important and will not give us any helpful information.</p>
<div id="cell-10" class="cell" data-outputid="8b2f44c6-4a88-401e-89e5-988e7f64e707" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>data  <span class="op">=</span> data[[<span class="st">"title"</span>, <span class="st">"text"</span>, <span class="st">"fake"</span>]]</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>data.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">

  <div id="df-f0a05e1b-bbb5-4c51-bbbd-aea524e363ad" class="colab-df-container">
    <div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">title</th>
<th data-quarto-table-cell-role="th">text</th>
<th data-quarto-table-cell-role="th">fake</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>Merkel: Strong result for Austria's FPO 'big c...</td>
<td>German Chancellor Angela Merkel said on Monday...</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>Trump says Pence will lead voter fraud panel</td>
<td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>JUST IN: SUSPECTED LEAKER and “Close Confidant...</td>
<td>On December 5, 2017, Circa s Sara Carter warne...</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>Thyssenkrupp has offered help to Argentina ove...</td>
<td>Germany s Thyssenkrupp, has offered assistance...</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>Trump say appeals court decision on travel ban...</td>
<td>President Donald Trump on Thursday called the ...</td>
<td>0</td>
</tr>
</tbody>
</table>

</div>
    <div class="colab-df-buttons">

  <div class="colab-df-container">
    <button class="colab-df-convert" onclick="convertToInteractive('df-f0a05e1b-bbb5-4c51-bbbd-aea524e363ad')" title="Convert this dataframe to an interactive table." style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 -960 960 960">
    <path d="M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z"></path>
  </svg>
    </button>

  <style>
    .colab-df-container {
      display:flex;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    .colab-df-buttons div {
      margin-bottom: 4px;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

    <script>
      const buttonEl =
        document.querySelector('#df-f0a05e1b-bbb5-4c51-bbbd-aea524e363ad button.colab-df-convert');
      buttonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';

      async function convertToInteractive(key) {
        const element = document.querySelector('#df-f0a05e1b-bbb5-4c51-bbbd-aea524e363ad');
        const dataTable =
          await google.colab.kernel.invokeFunction('convertToInteractive',
                                                    [key], {});
        if (!dataTable) return;

        const docLinkHtml = 'Like what you see? Visit the ' +
          '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
          + ' to learn more about interactive tables.';
        element.innerHTML = '';
        dataTable['output_type'] = 'display_data';
        await google.colab.output.renderOutput(dataTable, element);
        const docLink = document.createElement('div');
        docLink.innerHTML = docLinkHtml;
        element.appendChild(docLink);
      }
    </script>
  </div>


<div id="df-07c62891-ba4d-49c9-9729-b3fe59537f0e">
  <button class="colab-df-quickchart" onclick="quickchart('df-07c62891-ba4d-49c9-9729-b3fe59537f0e')" title="Suggest charts" style="display:none;">

<svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 0 24 24" width="24px">
    <g>
        <path d="M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z"></path>
    </g>
</svg>
  </button>

<style>
  .colab-df-quickchart {
      --bg-color: #E8F0FE;
      --fill-color: #1967D2;
      --hover-bg-color: #E2EBFA;
      --hover-fill-color: #174EA6;
      --disabled-fill-color: #AAA;
      --disabled-bg-color: #DDD;
  }

  [theme=dark] .colab-df-quickchart {
      --bg-color: #3B4455;
      --fill-color: #D2E3FC;
      --hover-bg-color: #434B5C;
      --hover-fill-color: #FFFFFF;
      --disabled-bg-color: #3B4455;
      --disabled-fill-color: #666;
  }

  .colab-df-quickchart {
    background-color: var(--bg-color);
    border: none;
    border-radius: 50%;
    cursor: pointer;
    display: none;
    fill: var(--fill-color);
    height: 32px;
    padding: 0;
    width: 32px;
  }

  .colab-df-quickchart:hover {
    background-color: var(--hover-bg-color);
    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);
    fill: var(--button-hover-fill-color);
  }

  .colab-df-quickchart-complete:disabled,
  .colab-df-quickchart-complete:disabled:hover {
    background-color: var(--disabled-bg-color);
    fill: var(--disabled-fill-color);
    box-shadow: none;
  }

  .colab-df-spinner {
    border: 2px solid var(--fill-color);
    border-color: transparent;
    border-bottom-color: var(--fill-color);
    animation:
      spin 1s steps(1) infinite;
  }

  @keyframes spin {
    0% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
      border-left-color: var(--fill-color);
    }
    20% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    30% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
      border-right-color: var(--fill-color);
    }
    40% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    60% {
      border-color: transparent;
      border-right-color: var(--fill-color);
    }
    80% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-bottom-color: var(--fill-color);
    }
    90% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
    }
  }
</style>

  <script>
    async function quickchart(key) {
      const quickchartButtonEl =
        document.querySelector('#' + key + ' button');
      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.
      quickchartButtonEl.classList.add('colab-df-spinner');
      try {
        const charts = await google.colab.kernel.invokeFunction(
            'suggestCharts', [key], {});
      } catch (error) {
        console.error('Error during call to suggestCharts:', error);
      }
      quickchartButtonEl.classList.remove('colab-df-spinner');
      quickchartButtonEl.classList.add('colab-df-quickchart-complete');
    }
    (() => {
      let quickchartButtonEl =
        document.querySelector('#df-07c62891-ba4d-49c9-9729-b3fe59537f0e button');
      quickchartButtonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';
    })();
  </script>
</div>
    </div>
  </div>
</div>
</div>
<p>We can now only see title, text, and fake on our dataset!</p>
<p>Next, let’s create a function to clean up our dataset. We want to get ride of all the stopword’s in our data as well as make all the letter’s lowercase. We want to also return a tf.data.Dataset, which consists of two inputs and one output. The two inputs should be title and text, while the output should be fake. In the function, you can see that we also included batch. Batch will help increase the speed of training especially since we are working with a large dataset.</p>
<div id="cell-12" class="cell" data-outputid="1274dfc7-afe0-4839-c120-c0785e9e7119" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.corpus <span class="im">import</span> stopwords</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">'stopwords'</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>stop <span class="op">=</span> stopwords.words(<span class="st">'english'</span>)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_dataset(input_data) :</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>  <span class="co">"""</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="co">  Function will make all text lowercase</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="co">  Removes stopwords from title and text</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a><span class="co">  Returns a tf.data.Dataset with two inputs and one output</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a><span class="co">  """</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>  input_data[<span class="st">'title_wo_stopwords'</span>] <span class="op">=</span> input_data[<span class="st">'title'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="st">' '</span>.join([word <span class="cf">for</span> word <span class="kw">in</span> x.split() <span class="cf">if</span> word <span class="kw">not</span> <span class="kw">in</span> stop]))</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>  input_data[<span class="st">'text_wo_stopwords'</span>] <span class="op">=</span> input_data[<span class="st">'text'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="st">' '</span>.join([word <span class="cf">for</span> word <span class="kw">in</span> x.split() <span class="cf">if</span> word <span class="kw">not</span> <span class="kw">in</span> stop]))</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>  data <span class="op">=</span> tf.data.Dataset.from_tensor_slices(({<span class="st">"title"</span>: input_data[<span class="st">'title_wo_stopwords'</span>],</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>                                              <span class="st">"text"</span>: input_data[<span class="st">'text_wo_stopwords'</span>]},</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>                                              {<span class="st">"fake"</span>: input_data[<span class="st">'fake'</span>]}))</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>  data <span class="op">=</span> data.<span class="bu">map</span>(<span class="kw">lambda</span> x, y: ((tf.strings.lower(x[<span class="st">"title"</span>]), tf.strings.lower(x[<span class="st">"text"</span>])), y))  <span class="co"># Lowercasing the text</span></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>  data <span class="op">=</span> data.batch(<span class="dv">100</span>)</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> data</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>[nltk_data] Downloading package stopwords to /root/nltk_data...
[nltk_data]   Unzipping corpora/stopwords.zip.</code></pre>
</div>
</div>
<div id="cell-13" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> make_dataset(data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>After we have passed our dataset through the function we have just created, we can split our data for training. We will leave 20% of the data to be used for validation and the remaining 80% for training.</p>
<div id="cell-15" class="cell" data-outputid="2cde5dff-04cc-4fbe-a961-86fb5f11008b" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> data.shuffle(buffer_size <span class="op">=</span> <span class="bu">len</span>(data), reshuffle_each_iteration<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>train_size <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.8</span><span class="op">*</span><span class="bu">len</span>(data))</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>val_size   <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.2</span><span class="op">*</span><span class="bu">len</span>(data))</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>train <span class="op">=</span> data.take(train_size)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>val   <span class="op">=</span> data.skip(train_size).take(val_size)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(train), <span class="bu">len</span>(val)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>(180, 45)</code></pre>
</div>
</div>
<p>Next, let us look at base rate. We can calculate the base rate by looking at the number of fake and real articles in our dataset. To do so, let’s create a for loop.</p>
<div id="cell-17" class="cell" data-outputid="3a49dcd1-76dc-4e08-d1e6-f764f843fc10">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Counting real and fake articles</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>real <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>fake <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> batch <span class="kw">in</span> train.take(<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Getting the labels</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> batch[<span class="dv">1</span>]</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> label <span class="kw">in</span> labels[<span class="st">"fake"</span>].numpy():</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If label is 0, real article</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> label <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>            real <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If label is 1, fake news</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> label <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>            fake <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Real title/text:"</span>, real)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Fake title/text:"</span>, fake)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Real title/text: 8534
Fake title/text: 9415</code></pre>
</div>
</div>
<p>We see that we have a total of 8588 real articles and 9361 fake articles. This means that the base rate is approximately 50%. Let’s see if we can create models to bring the accurate up from 50%!</p>
<section id="text-vectorization" class="level1">
<h1>Text Vectorization</h1>
<p>We will be using the following code to prepare a text vectorization layer for the tensorflow models.</p>
<div id="cell-20" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> string</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>size_vocabulary <span class="op">=</span> <span class="dv">2000</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> standardization(input_data):</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>  <span class="co">"""</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="co">  cleaning the data making all text lower and removing special characters</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="co">  """</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>  lowercase <span class="op">=</span> tf.strings.lower(input_data)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>  no_punctuation <span class="op">=</span> tf.strings.regex_replace(lowercase,</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>                                  <span class="st">'[</span><span class="sc">%s</span><span class="st">]'</span> <span class="op">%</span> re.escape(string.punctuation),<span class="st">''</span>)</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> no_punctuation</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>title_vectorize_layer <span class="op">=</span> tf.keras.layers.TextVectorization(</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>  standardize<span class="op">=</span>standardization,</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>  max_tokens<span class="op">=</span>size_vocabulary, <span class="co"># only consider this many words</span></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>  output_mode<span class="op">=</span><span class="st">'int'</span>,</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>  output_sequence_length<span class="op">=</span><span class="dv">500</span>)</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>title_vectorize_layer.adapt(train.<span class="bu">map</span>(<span class="kw">lambda</span> x, y: x[<span class="dv">0</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="model-1" class="level1">
<h1>Model 1</h1>
<p>Let’s start by creating a model using only the article title as an input. We will be utilizing the text vecorization created above.</p>
<div id="cell-22" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>model_title_only <span class="op">=</span> tf.keras.Sequential([</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    title_vectorize_layer,</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    layers.Embedding(input_dim<span class="op">=</span>size_vocabulary, output_dim<span class="op">=</span><span class="dv">64</span>),</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    layers.GlobalAveragePooling1D(),</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    layers.Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">'relu'</span>),</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    layers.Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">'sigmoid'</span>, name <span class="op">=</span> <span class="st">"fake"</span>)</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>train_combined <span class="op">=</span> train.<span class="bu">map</span>(<span class="kw">lambda</span> x, y: (x[<span class="dv">0</span>], y))</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>val_combined <span class="op">=</span> val.<span class="bu">map</span>(<span class="kw">lambda</span> x, y: (x[<span class="dv">0</span>], y))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-23" class="cell" data-outputid="518a9516-e12a-4146-ae4a-34b8a1f27869" data-execution_count="12">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>model_title_only.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">'adam'</span>,</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>                          loss<span class="op">=</span><span class="st">'binary_crossentropy'</span>,</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>                          metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>history_title_only <span class="op">=</span> model_title_only.fit(train_combined, epochs<span class="op">=</span><span class="dv">20</span>, validation_data<span class="op">=</span>val_combined)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 8s 31ms/step - accuracy: 0.5080 - loss: 0.6935 - val_accuracy: 0.5260 - val_loss: 0.6865
Epoch 2/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 25ms/step - accuracy: 0.5580 - loss: 0.6814 - val_accuracy: 0.8642 - val_loss: 0.6026
Epoch 3/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 33ms/step - accuracy: 0.7476 - loss: 0.5584 - val_accuracy: 0.8278 - val_loss: 0.3927
Epoch 4/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 10s 34ms/step - accuracy: 0.8341 - loss: 0.3847 - val_accuracy: 0.9282 - val_loss: 0.2560
Epoch 5/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 27ms/step - accuracy: 0.8979 - loss: 0.2754 - val_accuracy: 0.9416 - val_loss: 0.2019
Epoch 6/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 10s 24ms/step - accuracy: 0.9268 - loss: 0.2091 - val_accuracy: 0.9462 - val_loss: 0.1740
Epoch 7/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 8s 39ms/step - accuracy: 0.9403 - loss: 0.1723 - val_accuracy: 0.9580 - val_loss: 0.1488
Epoch 8/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 8s 27ms/step - accuracy: 0.9480 - loss: 0.1465 - val_accuracy: 0.9698 - val_loss: 0.1173
Epoch 9/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 24ms/step - accuracy: 0.9559 - loss: 0.1245 - val_accuracy: 0.9704 - val_loss: 0.0981
Epoch 10/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 25ms/step - accuracy: 0.9605 - loss: 0.1112 - val_accuracy: 0.9698 - val_loss: 0.0914
Epoch 11/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 23ms/step - accuracy: 0.9607 - loss: 0.1129 - val_accuracy: 0.9558 - val_loss: 0.1126
Epoch 12/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 23ms/step - accuracy: 0.9591 - loss: 0.1119 - val_accuracy: 0.9540 - val_loss: 0.1194
Epoch 13/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 24ms/step - accuracy: 0.9567 - loss: 0.1164 - val_accuracy: 0.9469 - val_loss: 0.1359
Epoch 14/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 10s 25ms/step - accuracy: 0.9539 - loss: 0.1247 - val_accuracy: 0.9767 - val_loss: 0.0773
Epoch 15/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 22ms/step - accuracy: 0.9615 - loss: 0.1021 - val_accuracy: 0.9687 - val_loss: 0.0977
Epoch 16/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 23ms/step - accuracy: 0.9673 - loss: 0.0919 - val_accuracy: 0.9620 - val_loss: 0.1101
Epoch 17/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 26ms/step - accuracy: 0.9671 - loss: 0.0928 - val_accuracy: 0.9113 - val_loss: 0.1963
Epoch 18/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 23ms/step - accuracy: 0.9558 - loss: 0.1206 - val_accuracy: 0.9758 - val_loss: 0.0695
Epoch 19/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 28ms/step - accuracy: 0.9723 - loss: 0.0807 - val_accuracy: 0.9791 - val_loss: 0.0658
Epoch 20/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 24ms/step - accuracy: 0.9713 - loss: 0.0815 - val_accuracy: 0.9773 - val_loss: 0.0663</code></pre>
</div>
</div>
<p>Let’s generate a summary of our model.</p>
<div id="cell-25" class="cell" data-outputid="226750d8-e084-4569-a1d6-bc8805bdcffb" data-execution_count="13">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>model_title_only.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "sequential"</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)                         </span>┃<span style="font-weight: bold"> Output Shape                </span>┃<span style="font-weight: bold">         Param # </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ text_vectorization                   │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">500</span>)                 │               <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">TextVectorization</span>)                  │                             │                 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ embedding (<span style="color: #0087ff; text-decoration-color: #0087ff">Embedding</span>)                │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">500</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)             │         <span style="color: #00af00; text-decoration-color: #00af00">128,000</span> │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ global_average_pooling1d             │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)                  │               <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">GlobalAveragePooling1D</span>)             │                             │                 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                        │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)                  │           <span style="color: #00af00; text-decoration-color: #00af00">4,160</span> │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ fake (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                         │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)                   │              <span style="color: #00af00; text-decoration-color: #00af00">65</span> │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">396,677</span> (1.51 MB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">132,225</span> (516.50 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Optimizer params: </span><span style="color: #00af00; text-decoration-color: #00af00">264,452</span> (1.01 MB)
</pre>
</div>
</div>
<p>Looking at the plot below, we can see that the accuracy of our model is really high! However, in the previous blog, remember that if the training accuracy is higher than the validation accuracy, there is a possible of overfitting in our model. Let’s attempt a different model to see if overfitting will not occur.</p>
<div id="cell-27" class="cell" data-outputid="56ac4788-7b9e-4ab8-8d49-9a66e1b80df8" data-execution_count="14">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>plt.plot(history_title_only.history[<span class="st">"accuracy"</span>], label <span class="op">=</span> <span class="st">"training"</span>)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>plt.plot(history_title_only.history[<span class="st">"val_accuracy"</span>], label <span class="op">=</span> <span class="st">"validation"</span>)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>plt.gca().<span class="bu">set</span>(xlabel <span class="op">=</span> <span class="st">"epoch"</span>, ylabel <span class="op">=</span> <span class="st">"accuracy"</span>)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>plt.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="HW6_files/figure-html/cell-15-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Let’s also create a visualization of our model so we can see exactly the layers and the shapes.</p>
<div id="cell-29" class="cell" data-outputid="d3666679-62f9-4e77-e354-f0d04d99a86c" data-execution_count="15">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>utils.plot_model(model_title_only, <span class="st">"output_filename.png"</span>,</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>                       show_shapes<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>                       show_layer_names<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<div>
<figure class="figure">
<p><img src="HW6_files/figure-html/cell-16-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="model-2" class="level1">
<h1>Model 2</h1>
<p>Now, since we only used title for our previous model, let’s try to only use text for our second model. We will use the same text vectorization method as we did before.</p>
<div id="cell-31" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>text_vectorize_layer <span class="op">=</span> tf.keras.layers.TextVectorization(</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    standardize<span class="op">=</span>standardization,</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>    max_tokens<span class="op">=</span>size_vocabulary, <span class="co"># only consider this many words</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>    output_mode<span class="op">=</span><span class="st">'int'</span>,</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>    output_sequence_length<span class="op">=</span><span class="dv">500</span>)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>text_vectorize_layer.adapt(train.<span class="bu">map</span>(<span class="kw">lambda</span> x, y: x[<span class="dv">1</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we can start building our model using only text.</p>
<div id="cell-33" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>model_text_only <span class="op">=</span> tf.keras.Sequential([</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    text_vectorize_layer,</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>    layers.Embedding(input_dim<span class="op">=</span>size_vocabulary, output_dim<span class="op">=</span><span class="dv">64</span>),</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>    layers.GlobalAveragePooling1D(),</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>    layers.Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">'relu'</span>),</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>    layers.Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">'sigmoid'</span>, name <span class="op">=</span> <span class="st">"fake"</span>)</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>train_combined <span class="op">=</span> train.<span class="bu">map</span>(<span class="kw">lambda</span> x, y: (x[<span class="dv">1</span>], y))</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>val_combined <span class="op">=</span> val.<span class="bu">map</span>(<span class="kw">lambda</span> x, y: (x[<span class="dv">1</span>], y))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-34" class="cell" data-outputid="7abd1962-4486-4401-b9b4-6e987742297f" data-execution_count="18">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>model_text_only.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">'adam'</span>,</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>                          loss<span class="op">=</span><span class="st">'binary_crossentropy'</span>,</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>                          metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>history_text_only <span class="op">=</span> model_text_only.fit(train_combined, epochs<span class="op">=</span><span class="dv">20</span>, validation_data<span class="op">=</span>val_combined)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 39ms/step - accuracy: 0.6653 - loss: 0.6015 - val_accuracy: 0.9276 - val_loss: 0.2148
Epoch 2/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 32ms/step - accuracy: 0.9300 - loss: 0.2042 - val_accuracy: 0.9598 - val_loss: 0.1350
Epoch 3/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 10s 33ms/step - accuracy: 0.9446 - loss: 0.1528 - val_accuracy: 0.9660 - val_loss: 0.1112
Epoch 4/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 10s 32ms/step - accuracy: 0.9499 - loss: 0.1334 - val_accuracy: 0.9709 - val_loss: 0.0977
Epoch 5/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 11s 39ms/step - accuracy: 0.9536 - loss: 0.1204 - val_accuracy: 0.9744 - val_loss: 0.0878
Epoch 6/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 10s 37ms/step - accuracy: 0.9572 - loss: 0.1104 - val_accuracy: 0.9773 - val_loss: 0.0805
Epoch 7/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 10s 38ms/step - accuracy: 0.9607 - loss: 0.1018 - val_accuracy: 0.9787 - val_loss: 0.0761
Epoch 8/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 35ms/step - accuracy: 0.9636 - loss: 0.0943 - val_accuracy: 0.9807 - val_loss: 0.0755
Epoch 9/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 38ms/step - accuracy: 0.9662 - loss: 0.0882 - val_accuracy: 0.9787 - val_loss: 0.0767
Epoch 10/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 32ms/step - accuracy: 0.9678 - loss: 0.0843 - val_accuracy: 0.9636 - val_loss: 0.0784
Epoch 11/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 8s 41ms/step - accuracy: 0.9682 - loss: 0.0816 - val_accuracy: 0.9629 - val_loss: 0.0792
Epoch 12/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 33ms/step - accuracy: 0.9696 - loss: 0.0798 - val_accuracy: 0.9660 - val_loss: 0.0744
Epoch 13/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 35ms/step - accuracy: 0.9687 - loss: 0.0801 - val_accuracy: 0.9796 - val_loss: 0.0702
Epoch 14/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 37ms/step - accuracy: 0.9675 - loss: 0.0832 - val_accuracy: 0.9809 - val_loss: 0.0666
Epoch 15/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 30ms/step - accuracy: 0.9669 - loss: 0.0875 - val_accuracy: 0.9791 - val_loss: 0.0638
Epoch 16/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 35ms/step - accuracy: 0.9685 - loss: 0.0811 - val_accuracy: 0.9636 - val_loss: 0.0902
Epoch 17/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 30ms/step - accuracy: 0.9709 - loss: 0.0720 - val_accuracy: 0.9529 - val_loss: 0.1136
Epoch 18/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 11s 32ms/step - accuracy: 0.9752 - loss: 0.0641 - val_accuracy: 0.9682 - val_loss: 0.0829
Epoch 19/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 10s 31ms/step - accuracy: 0.9712 - loss: 0.0737 - val_accuracy: 0.9558 - val_loss: 0.0911
Epoch 20/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 11s 32ms/step - accuracy: 0.9795 - loss: 0.0593 - val_accuracy: 0.9609 - val_loss: 0.0818</code></pre>
</div>
</div>
<p>Now that we have completed our model, let’s output the summary.</p>
<div id="cell-36" class="cell" data-outputid="4107791d-82ec-4dc9-f15d-1379e3c86758" data-execution_count="19">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>model_text_only.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "sequential_1"</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)                         </span>┃<span style="font-weight: bold"> Output Shape                </span>┃<span style="font-weight: bold">         Param # </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ text_vectorization_1                 │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">500</span>)                 │               <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">TextVectorization</span>)                  │                             │                 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ embedding_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">Embedding</span>)              │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">500</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)             │         <span style="color: #00af00; text-decoration-color: #00af00">128,000</span> │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ global_average_pooling1d_1           │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)                  │               <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">GlobalAveragePooling1D</span>)             │                             │                 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)                  │           <span style="color: #00af00; text-decoration-color: #00af00">4,160</span> │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ fake (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                         │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)                   │              <span style="color: #00af00; text-decoration-color: #00af00">65</span> │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">396,677</span> (1.51 MB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">132,225</span> (516.50 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Optimizer params: </span><span style="color: #00af00; text-decoration-color: #00af00">264,452</span> (1.01 MB)
</pre>
</div>
</div>
<p>The accuracy of our model is still very high with accuracy close to 98%. However, in the plot, we can still observe overfitting. Let’s try a different model to see if we can prevent overfitting from occuring.</p>
<div id="cell-38" class="cell" data-outputid="055f24bc-a19a-4189-80c4-41334e44ab74" data-execution_count="20">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>plt.plot(history_text_only.history[<span class="st">"accuracy"</span>], label <span class="op">=</span> <span class="st">"training"</span>)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>plt.plot(history_text_only.history[<span class="st">"val_accuracy"</span>], label <span class="op">=</span> <span class="st">"validation"</span>)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>plt.gca().<span class="bu">set</span>(xlabel <span class="op">=</span> <span class="st">"epoch"</span>, ylabel <span class="op">=</span> <span class="st">"accuracy"</span>)</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>plt.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="HW6_files/figure-html/cell-21-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Let’s also create a visualization of our model so we can see all the different layers involved.</p>
<div id="cell-40" class="cell" data-outputid="76b12f51-9466-444a-a79b-a9970b8a9891" data-execution_count="21">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>utils.plot_model(model_text_only, <span class="st">"output_filename.png"</span>,</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>                       show_shapes<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>                       show_layer_names<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<div>
<figure class="figure">
<p><img src="HW6_files/figure-html/cell-22-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="model-3" class="level1">
<h1>Model 3</h1>
<p>Last but not least, let’s create a model that uses both title and text to predict fake news.</p>
<div id="cell-42" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define title and text inputs</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>title_input <span class="op">=</span> tf.keras.Input(shape<span class="op">=</span>(<span class="dv">1</span>,), name<span class="op">=</span><span class="st">"title"</span>, dtype<span class="op">=</span><span class="st">"string"</span>)</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>text_input <span class="op">=</span> tf.keras.Input(shape<span class="op">=</span>(<span class="dv">1</span>,), name<span class="op">=</span><span class="st">"text"</span>, dtype<span class="op">=</span><span class="st">"string"</span>)</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>title_features <span class="op">=</span> title_vectorize_layer(title_input)</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>text_features <span class="op">=</span> text_vectorize_layer(text_input)</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>title_features <span class="op">=</span> layers.Embedding(size_vocabulary, <span class="dv">3</span>)(title_features)</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>text_features <span class="op">=</span> layers.Embedding(size_vocabulary, <span class="dv">3</span>)(text_features)</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>title_features <span class="op">=</span> layers.Dropout(<span class="fl">0.2</span>)(title_features)</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>text_features <span class="op">=</span> layers.Dropout(<span class="fl">0.2</span>)(text_features)</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>title_features <span class="op">=</span> layers.GlobalAveragePooling1D()(title_features)</span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>text_features <span class="op">=</span> layers.GlobalAveragePooling1D()(text_features)</span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>title_features <span class="op">=</span> layers.Dropout(<span class="fl">0.2</span>)(title_features)</span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>text_features <span class="op">=</span> layers.Dropout(<span class="fl">0.2</span>)(text_features)</span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a>title_features <span class="op">=</span> layers.Dense(<span class="dv">32</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(title_features)</span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a>text_features <span class="op">=</span> layers.Dense(<span class="dv">32</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(text_features)</span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.concatenate([title_features, text_features], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb29-24"><a href="#cb29-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-25"><a href="#cb29-25" aria-hidden="true" tabindex="-1"></a>fake_pred <span class="op">=</span> layers.Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">"sigmoid"</span>, name<span class="op">=</span><span class="st">"fake"</span>)(x)</span>
<span id="cb29-26"><a href="#cb29-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-27"><a href="#cb29-27" aria-hidden="true" tabindex="-1"></a>combine_model <span class="op">=</span> tf.keras.Model(</span>
<span id="cb29-28"><a href="#cb29-28" aria-hidden="true" tabindex="-1"></a>    inputs<span class="op">=</span>[title_input, text_input],</span>
<span id="cb29-29"><a href="#cb29-29" aria-hidden="true" tabindex="-1"></a>    outputs<span class="op">=</span>fake_pred</span>
<span id="cb29-30"><a href="#cb29-30" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-43" class="cell" data-outputid="3ec63522-4dbf-4a5a-cb9d-42d4ac43b49e" data-execution_count="23">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compile the model</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>combine_model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">'adam'</span>,</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>                          loss<span class="op">=</span><span class="st">'binary_crossentropy'</span>,</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>                          metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model</span></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>history_combine <span class="op">=</span> combine_model.fit(train, epochs<span class="op">=</span><span class="dv">20</span>, validation_data<span class="op">=</span>val)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 10s 39ms/step - accuracy: 0.5234 - loss: 0.6898 - val_accuracy: 0.8536 - val_loss: 0.6300
Epoch 2/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 31ms/step - accuracy: 0.8328 - loss: 0.5673 - val_accuracy: 0.9500 - val_loss: 0.3295
Epoch 3/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 33ms/step - accuracy: 0.9243 - loss: 0.3150 - val_accuracy: 0.9462 - val_loss: 0.2080
Epoch 4/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 11s 36ms/step - accuracy: 0.9367 - loss: 0.2247 - val_accuracy: 0.9562 - val_loss: 0.1614
Epoch 5/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 10s 36ms/step - accuracy: 0.9462 - loss: 0.1847 - val_accuracy: 0.9564 - val_loss: 0.1407
Epoch 6/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 32ms/step - accuracy: 0.9493 - loss: 0.1666 - val_accuracy: 0.9467 - val_loss: 0.1406
Epoch 7/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 10s 30ms/step - accuracy: 0.9527 - loss: 0.1533 - val_accuracy: 0.9642 - val_loss: 0.1173
Epoch 8/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 10s 32ms/step - accuracy: 0.9596 - loss: 0.1408 - val_accuracy: 0.9524 - val_loss: 0.1214
Epoch 9/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 11s 35ms/step - accuracy: 0.9615 - loss: 0.1271 - val_accuracy: 0.9707 - val_loss: 0.0984
Epoch 10/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 11s 38ms/step - accuracy: 0.9663 - loss: 0.1185 - val_accuracy: 0.9753 - val_loss: 0.0865
Epoch 11/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 30ms/step - accuracy: 0.9683 - loss: 0.1085 - val_accuracy: 0.9778 - val_loss: 0.0780
Epoch 12/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 10s 29ms/step - accuracy: 0.9728 - loss: 0.0976 - val_accuracy: 0.9804 - val_loss: 0.0693
Epoch 13/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 11s 31ms/step - accuracy: 0.9738 - loss: 0.0880 - val_accuracy: 0.9827 - val_loss: 0.0634
Epoch 14/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 11s 34ms/step - accuracy: 0.9758 - loss: 0.0804 - val_accuracy: 0.9840 - val_loss: 0.0575
Epoch 15/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 11s 37ms/step - accuracy: 0.9767 - loss: 0.0731 - val_accuracy: 0.9851 - val_loss: 0.0544
Epoch 16/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 11s 41ms/step - accuracy: 0.9784 - loss: 0.0673 - val_accuracy: 0.9869 - val_loss: 0.0472
Epoch 17/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 30ms/step - accuracy: 0.9807 - loss: 0.0593 - val_accuracy: 0.9702 - val_loss: 0.0695
Epoch 18/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 11s 31ms/step - accuracy: 0.9815 - loss: 0.0577 - val_accuracy: 0.9796 - val_loss: 0.0528
Epoch 19/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 11s 36ms/step - accuracy: 0.9813 - loss: 0.0553 - val_accuracy: 0.9827 - val_loss: 0.0471
Epoch 20/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 10s 37ms/step - accuracy: 0.9817 - loss: 0.0537 - val_accuracy: 0.9893 - val_loss: 0.0351</code></pre>
</div>
</div>
<p>Wow! We can see that the accuracy of this model is close to 100%! We can see that the issue with overfitting has been greatly improve using this model.</p>
<div id="cell-45" class="cell" data-outputid="acc9af3e-8fd8-4471-c99b-348d0697d2ec" data-execution_count="24">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>plt.plot(history_combine.history[<span class="st">"accuracy"</span>], label <span class="op">=</span> <span class="st">"training"</span>)</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>plt.plot(history_combine.history[<span class="st">"val_accuracy"</span>], label <span class="op">=</span> <span class="st">"validation"</span>)</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>plt.gca().<span class="bu">set</span>(xlabel <span class="op">=</span> <span class="st">"epoch"</span>, ylabel <span class="op">=</span> <span class="st">"accuracy"</span>)</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>plt.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="HW6_files/figure-html/cell-25-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Let’s create a visualization of our model to see all the layers involved. This one is more complex due to the use of two inputs.</p>
<div id="cell-47" class="cell" data-outputid="7e90552a-df34-4e8b-d992-f341b7a160cb" data-execution_count="25">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>utils.plot_model(combine_model, <span class="st">"output_filename.png"</span>,</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>                       show_shapes<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>                       show_layer_names<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="25">
<div>
<figure class="figure">
<p><img src="HW6_files/figure-html/cell-26-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="model-evaluation" class="level1">
<h1>Model Evaluation</h1>
<p>Since the best performing model is the one using both title and text, let’s try to run our testing dataset.</p>
<p>We will import the dataset from the following github: “https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true”. Make sure to clean the dataset to get rid of any unhelpful columns.</p>
<div id="cell-50" class="cell" data-outputid="78df1313-be12-4d79-d368-8ae1430fe2c7" data-execution_count="26">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>test_url <span class="op">=</span> <span class="st">"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>test <span class="op">=</span> pd.read_csv(test_url)</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>test <span class="op">=</span> test[[<span class="st">"title"</span>, <span class="st">"text"</span>, <span class="st">"fake"</span>]]</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>test.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="26">

  <div id="df-82575b20-d524-44b3-8c71-28ad01c757e9" class="colab-df-container">
    <div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">title</th>
<th data-quarto-table-cell-role="th">text</th>
<th data-quarto-table-cell-role="th">fake</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>CNN And MSNBC Destroy Trump, Black Out His Fa...</td>
<td>Donald Trump practically does something to cri...</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>Exclusive: Kremlin tells companies to deliver ...</td>
<td>The Kremlin wants good news. The Russian lead...</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>Golden State Warriors Coach Just WRECKED Trum...</td>
<td>On Saturday, the man we re forced to call Pre...</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>Putin opens monument to Stalin's victims, diss...</td>
<td>President Vladimir Putin inaugurated a monumen...</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>BREAKING: DNC HACKER FIRED For Bank Fraud…Blam...</td>
<td>Apparently breaking the law and scamming the g...</td>
<td>1</td>
</tr>
</tbody>
</table>

</div>
    <div class="colab-df-buttons">

  <div class="colab-df-container">
    <button class="colab-df-convert" onclick="convertToInteractive('df-82575b20-d524-44b3-8c71-28ad01c757e9')" title="Convert this dataframe to an interactive table." style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 -960 960 960">
    <path d="M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z"></path>
  </svg>
    </button>

  <style>
    .colab-df-container {
      display:flex;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    .colab-df-buttons div {
      margin-bottom: 4px;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

    <script>
      const buttonEl =
        document.querySelector('#df-82575b20-d524-44b3-8c71-28ad01c757e9 button.colab-df-convert');
      buttonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';

      async function convertToInteractive(key) {
        const element = document.querySelector('#df-82575b20-d524-44b3-8c71-28ad01c757e9');
        const dataTable =
          await google.colab.kernel.invokeFunction('convertToInteractive',
                                                    [key], {});
        if (!dataTable) return;

        const docLinkHtml = 'Like what you see? Visit the ' +
          '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
          + ' to learn more about interactive tables.';
        element.innerHTML = '';
        dataTable['output_type'] = 'display_data';
        await google.colab.output.renderOutput(dataTable, element);
        const docLink = document.createElement('div');
        docLink.innerHTML = docLinkHtml;
        element.appendChild(docLink);
      }
    </script>
  </div>


<div id="df-59f259c6-76e1-45bf-a296-1f1bf745037f">
  <button class="colab-df-quickchart" onclick="quickchart('df-59f259c6-76e1-45bf-a296-1f1bf745037f')" title="Suggest charts" style="display:none;">

<svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 0 24 24" width="24px">
    <g>
        <path d="M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z"></path>
    </g>
</svg>
  </button>

<style>
  .colab-df-quickchart {
      --bg-color: #E8F0FE;
      --fill-color: #1967D2;
      --hover-bg-color: #E2EBFA;
      --hover-fill-color: #174EA6;
      --disabled-fill-color: #AAA;
      --disabled-bg-color: #DDD;
  }

  [theme=dark] .colab-df-quickchart {
      --bg-color: #3B4455;
      --fill-color: #D2E3FC;
      --hover-bg-color: #434B5C;
      --hover-fill-color: #FFFFFF;
      --disabled-bg-color: #3B4455;
      --disabled-fill-color: #666;
  }

  .colab-df-quickchart {
    background-color: var(--bg-color);
    border: none;
    border-radius: 50%;
    cursor: pointer;
    display: none;
    fill: var(--fill-color);
    height: 32px;
    padding: 0;
    width: 32px;
  }

  .colab-df-quickchart:hover {
    background-color: var(--hover-bg-color);
    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);
    fill: var(--button-hover-fill-color);
  }

  .colab-df-quickchart-complete:disabled,
  .colab-df-quickchart-complete:disabled:hover {
    background-color: var(--disabled-bg-color);
    fill: var(--disabled-fill-color);
    box-shadow: none;
  }

  .colab-df-spinner {
    border: 2px solid var(--fill-color);
    border-color: transparent;
    border-bottom-color: var(--fill-color);
    animation:
      spin 1s steps(1) infinite;
  }

  @keyframes spin {
    0% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
      border-left-color: var(--fill-color);
    }
    20% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    30% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
      border-right-color: var(--fill-color);
    }
    40% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    60% {
      border-color: transparent;
      border-right-color: var(--fill-color);
    }
    80% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-bottom-color: var(--fill-color);
    }
    90% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
    }
  }
</style>

  <script>
    async function quickchart(key) {
      const quickchartButtonEl =
        document.querySelector('#' + key + ' button');
      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.
      quickchartButtonEl.classList.add('colab-df-spinner');
      try {
        const charts = await google.colab.kernel.invokeFunction(
            'suggestCharts', [key], {});
      } catch (error) {
        console.error('Error during call to suggestCharts:', error);
      }
      quickchartButtonEl.classList.remove('colab-df-spinner');
      quickchartButtonEl.classList.add('colab-df-quickchart-complete');
    }
    (() => {
      let quickchartButtonEl =
        document.querySelector('#df-59f259c6-76e1-45bf-a296-1f1bf745037f button');
      quickchartButtonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';
    })();
  </script>
</div>
    </div>
  </div>
</div>
</div>
<p>Remember the function we created a while ago, make sure to use that function to clean our testing dataset.</p>
<div id="cell-52" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>data2 <span class="op">=</span> make_dataset(test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next, we will be using evaluate() to evaluate how our model does on the testing dataset. We can see that the accuracy is still very high with 98.72% accuracy!</p>
<div id="cell-54" class="cell" data-outputid="48762225-9c0e-4a95-cb4f-c2e9792f9bbe" data-execution_count="28">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>combine_model.evaluate(data2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>225/225 ━━━━━━━━━━━━━━━━━━━━ 3s 13ms/step - accuracy: 0.9872 - loss: 0.0430</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="28">
<pre><code>[0.04207614064216614, 0.9867254495620728]</code></pre>
</div>
</div>
</section>
<section id="embedding-visualization" class="level1">
<h1>Embedding Visualization</h1>
<p>The point of embedding is to place words on certain vectors and place words with similar meanings in close proximity to each other. The following code generates a dataframe of the position of each word. We can use the following dataframe to plot the words and see its relationship with other words.</p>
<div id="cell-56" class="cell" data-outputid="c5cf0e64-a54d-4a79-fd92-501aebda70e8" data-execution_count="30">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> combine_model.get_layer(<span class="st">'embedding_2'</span>).get_weights()[<span class="dv">0</span>] <span class="co"># get the weights from the embedding layer</span></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>vocab <span class="op">=</span> title_vectorize_layer.get_vocabulary()                <span class="co"># get the vocabulary from our data prep for later</span></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> pca.fit_transform(weights)</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>embedding_df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'word'</span> : vocab,</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'x0'</span>   : weights[:,<span class="dv">0</span>],</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">'x1'</span>   : weights[:,<span class="dv">1</span>]</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>embedding_df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">

  <div id="df-5df7099d-6b9a-45e7-9d5a-d93de8bbc435" class="colab-df-container">
    <div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">word</th>
<th data-quarto-table-cell-role="th">x0</th>
<th data-quarto-table-cell-role="th">x1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td></td>
<td>-0.455995</td>
<td>-0.036738</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>[UNK]</td>
<td>6.431587</td>
<td>0.031929</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>trump</td>
<td>-0.263259</td>
<td>0.037156</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>video</td>
<td>14.122568</td>
<td>0.150382</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>to</td>
<td>11.734525</td>
<td>0.069527</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1995</td>
<td>consumer</td>
<td>-0.887943</td>
<td>-0.052701</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1996</td>
<td>consider</td>
<td>-1.383336</td>
<td>0.009815</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1997</td>
<td>completely</td>
<td>-0.091579</td>
<td>-0.095368</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1998</td>
<td>closed</td>
<td>-0.439425</td>
<td>-0.005176</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1999</td>
<td>cbs</td>
<td>-0.270297</td>
<td>-0.099974</td>
</tr>
</tbody>
</table>

<p>2000 rows × 3 columns</p>
</div>
    <div class="colab-df-buttons">

  <div class="colab-df-container">
    <button class="colab-df-convert" onclick="convertToInteractive('df-5df7099d-6b9a-45e7-9d5a-d93de8bbc435')" title="Convert this dataframe to an interactive table." style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 -960 960 960">
    <path d="M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z"></path>
  </svg>
    </button>

  <style>
    .colab-df-container {
      display:flex;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    .colab-df-buttons div {
      margin-bottom: 4px;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

    <script>
      const buttonEl =
        document.querySelector('#df-5df7099d-6b9a-45e7-9d5a-d93de8bbc435 button.colab-df-convert');
      buttonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';

      async function convertToInteractive(key) {
        const element = document.querySelector('#df-5df7099d-6b9a-45e7-9d5a-d93de8bbc435');
        const dataTable =
          await google.colab.kernel.invokeFunction('convertToInteractive',
                                                    [key], {});
        if (!dataTable) return;

        const docLinkHtml = 'Like what you see? Visit the ' +
          '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
          + ' to learn more about interactive tables.';
        element.innerHTML = '';
        dataTable['output_type'] = 'display_data';
        await google.colab.output.renderOutput(dataTable, element);
        const docLink = document.createElement('div');
        docLink.innerHTML = docLinkHtml;
        element.appendChild(docLink);
      }
    </script>
  </div>


<div id="df-632eeb69-7f82-43ec-a051-2ea706892451">
  <button class="colab-df-quickchart" onclick="quickchart('df-632eeb69-7f82-43ec-a051-2ea706892451')" title="Suggest charts" style="display:none;">

<svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 0 24 24" width="24px">
    <g>
        <path d="M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z"></path>
    </g>
</svg>
  </button>

<style>
  .colab-df-quickchart {
      --bg-color: #E8F0FE;
      --fill-color: #1967D2;
      --hover-bg-color: #E2EBFA;
      --hover-fill-color: #174EA6;
      --disabled-fill-color: #AAA;
      --disabled-bg-color: #DDD;
  }

  [theme=dark] .colab-df-quickchart {
      --bg-color: #3B4455;
      --fill-color: #D2E3FC;
      --hover-bg-color: #434B5C;
      --hover-fill-color: #FFFFFF;
      --disabled-bg-color: #3B4455;
      --disabled-fill-color: #666;
  }

  .colab-df-quickchart {
    background-color: var(--bg-color);
    border: none;
    border-radius: 50%;
    cursor: pointer;
    display: none;
    fill: var(--fill-color);
    height: 32px;
    padding: 0;
    width: 32px;
  }

  .colab-df-quickchart:hover {
    background-color: var(--hover-bg-color);
    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);
    fill: var(--button-hover-fill-color);
  }

  .colab-df-quickchart-complete:disabled,
  .colab-df-quickchart-complete:disabled:hover {
    background-color: var(--disabled-bg-color);
    fill: var(--disabled-fill-color);
    box-shadow: none;
  }

  .colab-df-spinner {
    border: 2px solid var(--fill-color);
    border-color: transparent;
    border-bottom-color: var(--fill-color);
    animation:
      spin 1s steps(1) infinite;
  }

  @keyframes spin {
    0% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
      border-left-color: var(--fill-color);
    }
    20% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    30% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
      border-right-color: var(--fill-color);
    }
    40% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    60% {
      border-color: transparent;
      border-right-color: var(--fill-color);
    }
    80% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-bottom-color: var(--fill-color);
    }
    90% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
    }
  }
</style>

  <script>
    async function quickchart(key) {
      const quickchartButtonEl =
        document.querySelector('#' + key + ' button');
      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.
      quickchartButtonEl.classList.add('colab-df-spinner');
      try {
        const charts = await google.colab.kernel.invokeFunction(
            'suggestCharts', [key], {});
      } catch (error) {
        console.error('Error during call to suggestCharts:', error);
      }
      quickchartButtonEl.classList.remove('colab-df-spinner');
      quickchartButtonEl.classList.add('colab-df-quickchart-complete');
    }
    (() => {
      let quickchartButtonEl =
        document.querySelector('#df-632eeb69-7f82-43ec-a051-2ea706892451 button');
      quickchartButtonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';
    })();
  </script>
</div>
    </div>
  </div>
</div>
</div>
<p>Below, let’s generate an interactive plot that will allow people to hover over a dot and look at the words to see the associations with each other. For example, if you hover over the point x0 = -1.689893 and x1 = 0.2999115, you will encounter the word illegal. If you move your cursor around illegal, you will find other words like shrieff, holds, undercover, fighting, criticizes, and record. These words are greatly related to each other which shows that our embedding is working properly. We often see these words in articles related to crimes or the government. Feel free to play around the plot to find other associations!</p>
<div id="cell-58" class="cell" data-outputid="b0d32cc2-45b8-4f65-e8d8-9eacf4d4f3c7" data-execution_count="31">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.express <span class="im">as</span> px</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> px.scatter(embedding_df,</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>                 x <span class="op">=</span> <span class="st">"x0"</span>,</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>                 y <span class="op">=</span> <span class="st">"x1"</span>,</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>                 size <span class="op">=</span> <span class="bu">list</span>(np.ones(<span class="bu">len</span>(embedding_df))),</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>                 size_max <span class="op">=</span> <span class="dv">3</span>,</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>                 hover_name <span class="op">=</span> <span class="st">"word"</span>)</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>fig.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<meta charset="utf-8">

    <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-2.24.1.min.js"></script>                <div id="d172e7bc-8332-45cf-9e86-5f27788ee653" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("d172e7bc-8332-45cf-9e86-5f27788ee653")) {                    Plotly.newPlot(                        "d172e7bc-8332-45cf-9e86-5f27788ee653",                        [{"hovertemplate":"\u003cb\u003e%{hovertext}\u003c\u002fb\u003e\u003cbr\u003e\u003cbr\u003ex0=%{x}\u003cbr\u003ex1=%{y}\u003cbr\u003esize=%{marker.size}\u003cextra\u003e\u003c\u002fextra\u003e","hovertext":["","[UNK]","trump","video","to","the","us","for","of","in","says","on","a","and","is","obama","with","watch","house","hillary","new","after","about","his","president","trump\u2019s","clinton","white","just","at","by","from","bill","this","russia","state","who","republican","out","court","it","he","senate","over","her","north","as","donald","are","vote","not","media","will","election","black","why","breaking","how","news","him","be","calls","korea","that","police","republicans","was","gop","\u2013","muslim","campaign","tax","trumps","you","china","may","up","democrats","one","deal","obama\u2019s","gets","what","has","iran","un","back","government","former","security","down","attack","tweets","fbi","party","have","eu","top","talks","congress","russian","chief","pm","first","america","democrat","people","against","speech","senator","syria","judge","tells","fox","ban","cnn","they","plan","no","twitter","war","leader","minister","say","make","would","it\u2019s","their","makes","law","south","could","sanders","man","report","during","military","brexit","shows","presidential","supreme","official","million","when","take","she","liberal","get","probe","tweet","foreign","two","off","like","more","governor","goes","racist","healthcare","factbox","support","cruz","border","nuclear","into","illegal","american","political","if","americans","hillary\u2019s","gun","sanctions","rally","host","being","woman","putin","response","want","islamic","fight","all","urges","syrian","supporters","wants","go","attacks","poll","wow","time","them","an","show","rights","obamacare","because","national","day","conservative","turkey","next","meet","gives","debate","women","he\u2019s","claims","bernie","antitrump","race","world","lawmakers","killed","german","policy","mexico","warns","uk","refugees","our","stop","states","old","see","school","meeting","your","travel","right","panel","help","candidate","ryan","must","call","budget","big","asks","sources","saudi","crisis","administration","trade","opposition","got","while","takes","students","pick","officials","we","leaders","group","don\u2019t","death","climate","air","visit","plans","department","win","room","going","did","years","so","secretary","can","won\u2019t","voters","immigration","hilarious","comey","ted","fake","details","defense","caught","most","left","head","general","press","open","huge","cops","end","justice","free","email","before","aid","tillerson","should","move","way","texas","mayor","made","leftist","exclusive","emails","do","arrested","ties","shocking","reporter","john","i","here\u2019s","democratic","major","can\u2019t","washington","wall","seeks","work","supporter","rules","city","than","reason","presidency","live","federal","use","still","money","interview","college","myanmar","last","give","ahead","times","protesters","paul","nominee","iraq","business","truth","team","kill","conservatives","case","violence","list","change","lives","but","push","now","key","director","york","year","shooting","secret","fire","really","post","latest","high","forces","dead","boiler","sexual","puerto","need","merkel","lawyer","home","health","terrorists","protest","lie","tv","threatens","speaker","order","only","lol","investigation","george","control","britain","bid","run","never","jerusalem","dnc","voter","florida","family","decision","coalition","army","again","office","image","cut","another","week","social","said","ready","mccain","job","germany","catalan","british","terrorist","bomb","blasts","adviser","terror","muslims","korean","face","ever","billion","attorney","trying","threat","rico","reform","message","legal","isis","funding","ep","backs","amid","agency","peace","lawmaker","doesn\u2019t","destroys","busted","2016","jobs","boom","slams","know","intelligence","flag","cuba","told","story","set","ruling","keep","had","chair","refugee","or","lead","great","force","california","behind","\u201ci","tries","rohingya","release","pence","parliament","iraqi","best","asked","were","votes","violent","used","senators","radical","orders","fired","been","action","3","sean","power","msnbc","kids","japan","independence","didn\u2019t","aide","needs","march","himself","hate","even","charges","\u201cthe","young","sign","senior","lies","israel","hit","full","chinas","ad","voting","scandal","saying","rule","internet","discuss","bad","statement","public","missile","france","cop","chicago","using","under","three","son","seek","rep","pay","making","flynn","away","thing","special","source","reveals","own","matter","macron","found","cuts","called","tell","possible","nyc","london","kremlin","jr","clinton\u2019s","cia","turkish","think","sex","repeal","questions","protests","kills","hollywood","bush","arrest","wins","very","nfl","near","michelle","likely","king","images","hold","girl","facebook","dc","believe","10","shut","she\u2019s","rejects","real","kurdish","hack","does","despite","defends","congressman","committee","claim","children","admits","warren","vows","trip","refuses","ny","moore","men","inauguration","destroy","benghazi","awesome","announces","threats","these","start","spokesman","proves","pressure","pope","look","good","executive","evidence","brutal","yet","wife","water","transgender","strike","role","released","put","protect","lying","joe","hurricane","disgusting","debt","days","comments","calling","assault","ambassador","act","workers","victims","venezuela","too","street","shot","sees","running","rubio","reports","program","planned","paris","much","kelly","issues","french","forced","female","erdogan","epic","epa","elections","demands","dem","biden","audio","activist","xi","working","rips","review","pelosi","passes","let","jail","five","final","denies","corruption","citizens","break","bombshell","blames","angry","\u201cwe","whoa","thousands","rape","rant","oil","mcconnell","liberals","leave","history","groups","fraud","battle","approves","al","without","seen","sarah","question","nancy","mike","least","hits","hilariously","hard","gay","gave","block","barack","agree","1","viral","star","sessions","service","return","photo","part","middle","global","friday","exposes","east","doj","criminal","child","ben","yemen","uses","university","they\u2019re","puts","pentagon","offers","murder","migrants","loses","effort","defend","close","boy","anchor","aliens","agenda","afghan","words","suspected","summit","stunning","student","russias","navy","millions","members","killing","kellyanne","ivanka","hopes","getting","fed","envoy","crowd","conference","ceo","car","cabinet","alien","abortion","turn","trial","town","thinks","taking","strikes","spain","responds","massive","little","hell","funds","faces","drops","convention","5","4","victory","united","terrorism","staff","spicer","spending","second","picks","michael","life","labor","immigrants","future","flashback","explains","energy","cyber","crooked","country","conway","congressional","brilliant","around","arms","actually","activists","test","talk","stand","signs","perfect","member","line","join","illegals","food","fighting","exposed","every","dangerous","come","warning","vp","voted","union","ukraine","target","private","philippines","other","oregon","nafta","megyn","fund","friend","confirms","chris","catalonia","capital","wrong","where","troops","step","stay","since","releases","laws","issue","finally","fear","destroyed","deputy","carson","canada","breaks","arabia","already","accuses","911","2","stage","some","request","owner","offer","night","nato","mom","militants","michigan","manager","leaves","johnson","detroit","cnn\u2019s","church","challenge","chairman","bank","announcement","2018","wikileaks","tried","telling","sht","remarks","referendum","prison","politics","points","oops","number","names","mexican","melania","literally","letter","its","intel","human","hearing","four","find","finance","fans","economy","economic","dems","council","company","blame","access","15","system","sheriff","northern","name","lawsuit","irish","insane","hacking","guy","guest","foundation","far","europe","elizabeth","daughter","civil","cities","australia","alabama","20","\u2018the","west","waters","tucker","transition","today","sunday","speaks","resign","picture","perfectly","moscow","mocks","might","mattis","lebanon","highlights","hannity","giving","flint","event","embarrassing","dad","coal","bathroom","accused","absolutely","yr","worst","weapons","soros","something","movie","jeanine","fck","ends","embassy","criticism","crazy","conspiracy","christmas","bring","better","asia","appeals","update","steve","steps","schools","revealed","raise","percent","pass","ohio","maxine","james","ireland","immigrant","held","heads","guns","eyes","everyone","care","between","afghanistan","victim","uks","turkeys","tough","taiwan","syrias","sue","sent","sea","rich","prosecutor","prince","pastor","pakistan","officer","obamas","mueller","moves","irma","information","germanys","firm","financial","drug","companies","comment","class","carlson","become","allies","zimbabwe","worse","vietnam","videos","veteran","urge","train","totally","residents","record","place","parenthood","murdered","months","mass","less","kurds","israeli","iowa","hurt","happened","education","disaster","declares","choice","candidates","boycott","boost","ask","any","6","100","well","virginia","unhinged","things","taxes","streets","speak","sanctuary","roy","reutersipsos","red","qatar","play","parties","palin","palestinian","mugabe","meltdown","mark","journalist","important","harassment","girls","furious","front","fires","exit","confederate","coming","christian","chinese","charged","brutally","base","bans","bangladesh","arrests","allow","agencies","50","try","term","tensions","teen","strong","soldiers","seth","sends","russians","reelection","recount","proof","pact","nomination","my","missing","love","jailed","illinois","frances","fix","donations","doing","desperate","crime","comes","britains","borders","book","asking","antifa","answer","agents","african","actor","abuse","8","wearing","vs","tuesday","treasury","thought","testimony","spy","six","sick","ria","racism","powerful","nations","moment","meets","market","launches","launch","hysterical","hope","homeland","hariri","ground","game","elected","egypt","efforts","diplomatic","demand","community","charge","blow","baltimore","baby","zimbabwes","thursday","thugs","then","testify","taxpayer","targets","talking","spanish","soon","shuts","same","results","resigns","progress","potential","o\u2019reilly","outside","opens","nothing","news\u2019","migrant","looks","living","leadership","kenya","journalists","iranian","inside","humiliates","hosts","hammers","firing","finds","father","eus","enough","employees","electoral","early","due","documents","detained","corporate","citizen","check","central","ca","bundy","attempt","assad","appeal","anthem","allegations","13","12","visits","veterans","took","teacher","super","stupid","save","rnc","reporters","replace","raises","praises","maher","lost","lose","libya","leaks","injured","industry","hundreds","hands","feds","failed","eastern","done","dollars","dispute","december","cover","center","carolina","body","billionaire","amnesty","ally","wanted","unreal","unity","trump\u201d","throws","there","taxpayers","study","storm","sets","send","sec","risk","rightwing","rebels","protester","professor","prime","players","philippine","parents","nyt","nbc","nation","nails","mother","mosque","manafort","lied","leads","joins","india","humiliated","hotel","he\u2019ll","here","henningsen","harvey","green","gas","freedom","forward","farright","experts","european","dallas","condemns","commerce","chuck","changes","avoid","attacking","across","\u201cyou","worker","women\u2019s","winning","went","waiting","visa","vice","threatened","threaten","third","surprise","supporting","suicide","socialist","san","returns","regime","read","quit","pushes","popular","orlando","netanyahu","monday","longer","libyan","hispanic","harry","draws","dialogue","dept","data","crackdown","counsel","continue","concerns","concerned","christie","blocks","attend","attacked","anyone","america\u2019s","airport","aides","word","who\u2019s","whether","watchdog","wage","vegas","usa","supports","strategy","spying","sentence","riots","reuters","respond","remove","reid","reach","process","presidents","mays","low","kushner","koreas","isn\u2019t","investment","international","hand","hacked","gowdy","focus","families","facts","emergency","deals","complete","collusion","charlottesville","campus","blacks","bizarre","bannon","australian","asylum","armed","alleged","agrees","africas","advisor","address","18","11","you\u2019re","you\u2019ll","warned","turns","trey","treatment","threatening","suspect","suggests","station","scott","sales","reaction","raqqa","quits","promises","probes","price","posts","polls","past","paid","newspaper","month","militant","met","many","mainstream","loss","looms","leaked","leading","kim","jimmy","jersey","idea","hot","hateful","half","guilty","guess","golf","gingrich","expects","endorsement","drop","crimes","continues","clintons","cites","cash","cannot","buy","breitbart","blacklivesmatter","backing","amazing","9","2017","17","\u201cif","weighs","w","view","undercover","thanks","store","situation","seven","schumer","romney","remain","religious","proposes","priceless","policies","point","person","patrick","officers","nra","nightmare","long","legislation","kerry","kansas","judges","joy","islamist","insurance","gonna","fit","fears","fail","explain","eric","endorses","duterte","dossier","disturbing","delivers","defeat","cooperation","conflict","communist","committed","chance","brussels","brother","briefing","brazil","bombers","board","banks","april","allowed","agent","ads","accidentally","7","30","won","trumprussia","total","torture","tom","suspends","starts","stance","somali","showing","shooter","shoot","serious","screenshots","ridiculous","relief","regional","records","reality","radio","pulls","prove","problems","pledge","phony","options","movement","mnuchin","links","kkk","keeps","japans","irs","hospital","happen","gains","fuel","files","expert","ethics","donors","dirty","diplomats","defending","decide","david","credit","countries","clash","bus","brings","brilliantly","bombing","blast","blaming","arizona","apart","abc","\u201cracist\u201d","wisconsin","whining","whines","we\u2019re","vatican","trumpcare","toll","swedish","susan","speaking","snl","search","scam","safe","reportedly","relations","reforms","ratings","position","plot","pledges","planning","overhaul","others","once","numbers","nsa","newt","mi","me","marriage","marco","lynch","lee","late","jones","jeb","irans","hypocrite","hypocrisy","husband","huckabee","holds","helped","happy","gov","gorsuch","gold","environmental","drone","drive","dies","delay","deep","criticizes","coup","controversial","christians","cancels","biggest","beijing","banned","aren\u2019t","alert","aim","\u201cit\u2019s","willing","wasn\u2019t","van","va","unlikely","trudeau","through","taps","stealing","sports","small","rush","robert","road","rise","relationship","region","propaganda","politico","pennsylvania","openly","opening","nazi","ministry","memorial","mean","local","las","lady","kimmel","killer","kasich","joke","jeff","islam","instead","info","independent","having","guard","georgia","feel","fan","famous","false","expected","everything","entire","enforcement","detains","critics","convicted","considering","compares","chaos","burn","building","blood","berkeley","based","ballistic","appears","amendment","admit","25","24","14","\u201cthis","\u2018fake","\u2014","welcomes","vet","tpp","thug","themselves","tests","stephen","standing","spent","space","shreds","shares","sen","seeking","rice","rhetoric","reasons","poor","poland","playing","plane","pathetic","outrageous","outrage","oklahoma","nearly","mock","marine","lot","lavrov","japanese","it\u201d","interest","increase","hopeful","historic","halt","haley","gone","god","glorious","giuliani","gang","falls","door","domestic","disabled","die","dhs","declaration","deadly","courts","cost","charity","camp","broke","boss","bills","billions","beat","audience","allowing","agreement","ago","africa","actress","actions","account","accept","abe","2015","\u201cnot","\u201ci\u2019m","\u201che","wounded","wh","weeks","truck","tower","todd","teachers","swiss","surveillance","sure","sues","sudan","steal","stands","southern","silent","short","shoots","sharpton","sexist","server","seriously","restaurant","responsible","resolve","resignation","removed","proposed","proposal","promote","progressive","primary","phone","path","online","offered","nuts","network","neil","needed","nasty","memo","medical","medicaid","measures","losing","knows","kid","kicked","june","jewish","jackson","i\u2019m","infrastructure","influence","indonesian","hours","holding","hiding","hezbollah","happens","gulf","governors","given","fair","facing","expose","egypts","editor","display","dinner","critical","congresswoman","commander","classified","claiming","chilling","challenges","ceasefire","cause","caucus","career","cancer","beautiful","beating","baghdad","article","arab","appearance","answers","accusations","21","\u201cwhite","\u2018i","zone","worked","withdrawal","which","what\u2019s","wednesday","warming","veto","usbacked","tim","that\u2019s","thai","tech","tape","tantrum","taken","sweden","straight","stolen","silence","shutdown","shouldn\u2019t","sharia","several","sending","sell","scotus","safety","runs","rock","risks","rioters","retirement","resume","remember","refuse","rand","putting","project","problem","priebus","president\u201d","paying","parade","page","ordered","opposes","nine","nigeria","multiple","morning","mn","minutes","minimum","militia","meddling","massacre","malaysia","maine","limits","limit","leaving","lahren","kidding","kenyan","karma","joint","italy","interior","indian","incident","idlib","horrible","herself","grave","graham","graft","google","gift","gender","gaza","football","following","floor","failure","explodes","explode","exactly","endorse","easy","dr","discussed","democracy","demanding","de","dark","cuban","course","consumer","consider","completely","closed","cbs"],"legendgroup":"","marker":{"color":"#636efa","size":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0],"sizemode":"area","sizeref":0.1111111111111111,"symbol":"circle"},"mode":"markers","name":"","showlegend":false,"x":[-0.4559951424598694,6.431586742401123,-0.26325932145118713,14.122568130493164,11.734524726867676,8.74008846282959,-3.795135021209717,8.713017463684082,8.985897064208984,7.695858478546143,-7.481626033782959,7.434899806976318,5.624730110168457,6.5038909912109375,5.847403049468994,2.0346410274505615,6.0144758224487305,5.095067977905273,-3.1246626377105713,5.239904403686523,-0.8972963094711304,4.482493877410889,4.819025993347168,4.401358127593994,2.1300079822540283,4.7652268409729,-0.3585810363292694,-2.1352388858795166,4.346032619476318,4.024728775024414,4.470835208892822,4.061949253082275,-1.7214888334274292,3.971679925918579,-1.2706725597381592,-0.1204717829823494,3.9911842346191406,-1.0649864673614502,3.8155598640441895,-0.8381219506263733,3.6713674068450928,3.1805779933929443,-3.303086996078491,3.8356707096099854,3.4470224380493164,-2.3453245162963867,3.0657856464385986,2.55983304977417,3.3702218532562256,-0.7416682839393616,3.2799859046936035,0.217706099152565,3.8692703247070312,-0.7070148587226868,2.2222654819488525,3.422520399093628,5.067722797393799,2.5767104625701904,0.8942399621009827,2.407439708709717,3.0371930599212646,-0.8809947967529297,-2.023193359375,3.1885271072387695,-1.612740397453308,0.43523454666137695,3.162536382675171,2.973288059234619,3.1554813385009766,2.99544095993042,0.24700981378555298,-1.5696399211883545,-3.4461324214935303,3.110682487487793,-2.2975733280181885,-1.8221923112869263,3.092336654663086,0.8524780869483948,2.0990681648254395,-1.2254465818405151,3.5573301315307617,1.7114864587783813,2.5841097831726074,2.7117886543273926,-0.6979739665985107,-1.2933660745620728,1.1461740732192993,-0.38723623752593994,-0.1960001289844513,-0.6670522689819336,2.8455862998962402,-0.3151915371417999,0.5712146759033203,0.8562870621681213,-0.3478875458240509,2.85929536819458,-1.0362237691879272,-1.014546275138855,-2.9102206230163574,-1.7098891735076904,-1.1651036739349365,-1.751631259918213,-3.294394016265869,-0.19258998334407806,2.1688501834869385,1.4681822061538696,1.0666166543960571,2.9357008934020996,0.44192779064178467,0.40158000588417053,-1.6072560548782349,1.2634559869766235,0.6519345045089722,-0.03250093013048172,-0.35350313782691956,2.0319149494171143,2.8461177349090576,-0.6972973942756653,1.2870274782180786,-2.7314703464508057,-0.44026607275009155,-0.4382006525993347,-1.4968485832214355,-1.2382067441940308,1.5031483173370361,-0.6371250748634338,1.7107853889465332,2.3367812633514404,1.888318419456482,-0.4734712541103363,-3.0227978229522705,-0.051866527646780014,1.2916227579116821,0.9802364706993103,-0.06728563457727432,2.586277723312378,-1.2602802515029907,-1.8342798948287964,0.9735074639320374,-1.8095083236694336,0.24403773248195648,-1.7584657669067383,-0.02389421872794628,2.684680938720703,0.925570547580719,2.3666648864746094,2.691837787628174,1.609703540802002,-2.478076696395874,0.2147831916809082,-0.022153615951538086,-0.22976809740066528,1.6916106939315796,2.1208415031433105,1.2314705848693848,-0.5194272398948669,2.1793181896209717,1.1897388696670532,-2.6500649452209473,-4.193418502807617,-0.14682909846305847,0.6777569651603699,-0.6119533777236938,-1.7218215465545654,2.6524147987365723,1.5813267230987549,1.4644535779953003,-0.027934623882174492,1.7388725280761719,0.9697210788726807,2.516885757446289,0.4387190341949463,-1.2972023487091064,1.4059826135635376,1.5952163934707642,2.160360813140869,0.5412952899932861,-1.055174469947815,0.9204398989677429,0.9950605630874634,-1.6328741312026978,-1.3803824186325073,1.595658540725708,-2.7800848484039307,-1.5932694673538208,1.9261736869812012,-0.20547515153884888,0.498075008392334,-0.35400453209877014,-0.08916038274765015,2.308474063873291,0.8626013994216919,1.7179691791534424,1.0333613157272339,0.8740045428276062,-0.7983309030532837,0.3999437987804413,1.6302440166473389,0.21657460927963257,0.8371004462242126,0.39349955320358276,-1.9257742166519165,-0.2513853907585144,-1.8172729015350342,1.2292245626449585,-0.34337949752807617,-0.2897253930568695,1.7109744548797607,0.9787030816078186,2.2873470783233643,1.235784888267517,-0.891858696937561,0.8816928267478943,-1.4471516609191895,-0.8020281791687012,-0.9922699928283691,-0.53956538438797,-0.7027620673179626,0.5346154570579529,-0.9992433190345764,1.1594172716140747,2.1799638271331787,-0.12582068145275116,0.11686526983976364,1.8819905519485474,0.7163481712341309,1.5440421104431152,-0.026320748031139374,2.2090914249420166,-0.5961887240409851,1.204346776008606,-2.0812828540802,0.10059348493814468,-0.24414269626140594,-1.1407877206802368,0.3342774212360382,-0.4022371172904968,-1.4324028491973877,0.9041029214859009,0.4862407445907593,-2.069974899291992,-1.082602620124817,-1.2781065702438354,-0.9702653288841248,-1.3338276147842407,-2.6871440410614014,1.0719367265701294,1.726818323135376,0.438291996717453,0.8959574103355408,-0.5162940621376038,-1.4284337759017944,1.64399254322052,-0.7687193155288696,0.32657715678215027,2.236611843109131,-0.043779436498880386,0.017983263358473778,-1.2857767343521118,-1.522714376449585,-0.1831686645746231,0.036065589636564255,1.0586538314819336,1.6200066804885864,0.8079120516777039,1.7851173877716064,-0.07981503009796143,1.5589396953582764,-1.0577422380447388,1.704963207244873,2.3267526626586914,-0.3277900218963623,-0.9069533944129944,1.5658578872680664,-0.2493974268436432,1.6018613576889038,0.5460375547409058,1.2211772203445435,-1.4110342264175415,1.501001238822937,1.6568571329116821,1.1596864461898804,-0.32782673835754395,-0.7206979393959045,1.0610647201538086,-0.40051984786987305,2.1699233055114746,1.4122880697250366,-0.3476053774356842,0.14474840462207794,0.7087917923927307,-0.24649827182292938,1.9652440547943115,-0.5220507979393005,-2.1412792205810547,1.8129947185516357,0.469032347202301,0.4843358099460602,0.356192946434021,0.8965244889259338,-0.41841617226600647,1.5731961727142334,-1.4881826639175415,0.5954845547676086,1.088659644126892,0.09001760929822922,-0.9436078667640686,1.792156457901001,1.3357813358306885,1.278378963470459,0.8089699149131775,1.9994932413101196,-1.3322250843048096,0.7643208503723145,1.8363138437271118,-0.7197238206863403,0.38985320925712585,-1.950903058052063,-0.9460492134094238,1.5159056186676025,-0.03847264125943184,0.2545098662376404,1.3070961236953735,1.3487998247146606,-0.9904548525810242,1.0614688396453857,0.595186173915863,-0.3177316188812256,-0.4055923521518707,-0.13082517683506012,-0.8126677870750427,1.3267271518707275,-1.9119740724563599,-0.01484257448464632,0.44842734932899475,-0.37051454186439514,0.35609373450279236,0.7039145827293396,1.1311733722686768,-1.00593900680542,-1.448111653327942,-0.6083164215087891,1.4403610229492188,-0.03167356923222542,-0.07696946710348129,0.39587363600730896,-0.8683004975318909,0.4187220633029938,1.1749986410140991,0.14400853216648102,1.3751327991485596,1.5147528648376465,-0.6741440892219543,1.3426628112792969,-0.9251517057418823,0.1464335173368454,-1.0496107339859009,-0.6342697143554688,-0.4424430727958679,0.8077945709228516,-0.8629546761512756,1.1867107152938843,-0.1351654976606369,0.36058181524276733,-0.703970730304718,-1.506949543952942,-0.4999426603317261,1.7987695932388306,0.7054457068443298,-1.3127936124801636,0.45766282081604004,-0.6454996466636658,-0.801650881767273,0.6106150150299072,-1.2715011835098267,1.553840160369873,-0.545335054397583,1.1978739500045776,-0.8957939743995667,0.7005889415740967,-1.71558678150177,-1.037208080291748,1.458992838859558,1.6176996231079102,0.5557343363761902,1.257795810699463,-0.09084207564592361,-1.5534859895706177,-1.7761859893798828,-0.6685145497322083,0.8947550654411316,-0.8882573843002319,1.3957304954528809,0.31744492053985596,-0.35981646180152893,0.8371902704238892,-1.419156789779663,-1.740453839302063,-1.6035069227218628,1.3654826879501343,-0.3135904371738434,0.07950358092784882,0.12367228418588638,0.7308332920074463,-1.0829010009765625,-0.4595498740673065,0.8197461366653442,0.011712800711393356,-0.28404349088668823,0.29322803020477295,-0.5432440638542175,-2.199202537536621,-1.479577660560608,0.861755907535553,-1.4270390272140503,0.20609121024608612,-1.615630030632019,1.0083245038986206,0.03138427063822746,-0.957423985004425,0.48074543476104736,1.17974054813385,0.19483499228954315,-1.3224056959152222,0.03442865237593651,-0.41699978709220886,-1.4703810214996338,-1.831274390220642,1.0339852571487427,-0.31942737102508545,1.9319955110549927,-0.23953096568584442,1.5673547983169556,-1.1827062368392944,-2.092970848083496,-1.8996243476867676,-1.2017788887023926,-0.8917553424835205,1.2897635698318481,1.3254566192626953,1.6168335676193237,0.5859548449516296,0.5455859899520874,1.7854140996932983,0.31977415084838867,1.2083361148834229,-0.8144460320472717,1.3094695806503296,-1.2455490827560425,0.3808918595314026,0.17840276658535004,-0.6606199741363525,-1.6945605278015137,-1.2644439935684204,1.245851993560791,0.5284019112586975,0.4013921618461609,1.787224531173706,-1.2544234991073608,0.7014012336730957,0.3930838704109192,-0.5088040828704834,0.28400129079818726,1.7703258991241455,0.37401655316352844,-1.7228914499282837,-0.5106120109558105,0.2398144155740738,-2.1475772857666016,-1.3847565650939941,1.441701889038086,0.46225056052207947,0.8790626525878906,-0.3178653120994568,1.1311419010162354,0.5239976048469543,-0.9919593334197998,1.604222059249878,-0.9317119717597961,0.8876917958259583,1.1998023986816406,-1.2671900987625122,0.3976084291934967,1.0456552505493164,-1.254438877105713,0.5801387429237366,0.7803637385368347,-1.8320741653442383,-0.930488109588623,1.419722080230713,-1.5737717151641846,0.1697995811700821,-0.9219538569450378,0.9799154996871948,1.209723949432373,0.9198402762413025,-0.3831520974636078,1.603053331375122,-0.18749727308750153,0.18327292799949646,-0.01845727488398552,1.295127272605896,-0.6611545085906982,-0.3946880102157593,0.9132768511772156,-1.765974998474121,0.6888948082923889,-0.3144722282886505,0.0022677395027130842,0.9102159738540649,-1.0720347166061401,-0.3376978039741516,-0.7176951766014099,0.9840373396873474,-1.2531766891479492,0.03555313125252724,-0.9990456104278564,-1.0747231245040894,1.1034523248672485,0.48143547773361206,0.428345650434494,1.4161248207092285,-1.226210117340088,-0.14144380390644073,-1.898402452468872,0.48882490396499634,0.5661415457725525,0.5474436283111572,-0.9343397617340088,-0.08579332381486893,0.10148728638887405,-0.4119984209537506,-2.1214959621429443,1.038641333580017,1.2089561223983765,0.919876217842102,-1.6992372274398804,-0.15966257452964783,-0.6377782225608826,0.9452430009841919,0.5750062465667725,-0.6022624969482422,1.2990859746932983,-1.9531112909317017,-2.3023228645324707,-0.36266544461250305,1.3275301456451416,0.6187700629234314,-1.6593000888824463,1.366269588470459,-0.29503777623176575,-1.0765101909637451,-0.0005109563935548067,-0.5311678051948547,-1.1685172319412231,1.1813006401062012,0.7317086458206177,-1.0699347257614136,-1.1365501880645752,0.033173929899930954,-0.3473702073097229,-0.4357271194458008,-0.08346962928771973,-0.7765016555786133,-0.808918297290802,0.763559877872467,-0.9985628724098206,0.008514386601746082,0.056927092373371124,1.2954740524291992,0.9271827936172485,0.2037288248538971,0.6620393991470337,1.271942377090454,-0.9177252650260925,0.9748694896697998,-1.7158584594726562,0.5700893998146057,1.3957898616790771,-1.123543381690979,-0.8503208160400391,1.1757715940475464,-1.227791428565979,0.628631591796875,-0.40500161051750183,0.894687294960022,0.01003551110625267,-1.5382949113845825,-0.17904682457447052,0.4141924977302551,0.061370600014925,-0.5987836718559265,0.401607871055603,-0.13286496698856354,0.5111570358276367,0.7375555038452148,1.572670817375183,1.230334758758545,0.7346376776695251,1.0386921167373657,-0.0606808178126812,-2.1174583435058594,1.053964614868164,-1.1858233213424683,-0.622344434261322,0.39897236227989197,-0.17097754776477814,0.24977682530879974,0.6792311668395996,1.1252518892288208,-0.2234080582857132,0.4268212914466858,-0.17813275754451752,-0.48427924513816833,-0.9053012728691101,-0.8497081398963928,0.42752009630203247,0.27649441361427307,0.39622223377227783,1.0155469179153442,0.6500226259231567,-1.3601951599121094,1.01707124710083,-0.6624143123626709,0.17155691981315613,-0.7959215044975281,0.7330472469329834,0.21590456366539001,-0.43290525674819946,-0.4422302544116974,0.6371473670005798,0.00857063289731741,-1.263969898223877,0.24812503159046173,0.45661479234695435,0.3918820917606354,-1.3900866508483887,0.9564769864082336,0.10502620786428452,-1.183813452720642,-0.5351221561431885,0.4743819832801819,0.10613255202770233,0.8415555357933044,0.8448523283004761,-0.858726978302002,-0.8614945411682129,0.269478976726532,0.7058231830596924,-1.4083037376403809,1.608477234840393,-0.4370937645435333,0.3241945505142212,0.02797635830938816,1.44560706615448,-0.8289881944656372,0.601180374622345,-0.27532875537872314,-1.675254225730896,0.33170488476753235,1.4588342905044556,-0.8224669098854065,1.3790897130966187,-0.5026456117630005,0.5444720387458801,0.012817132286727428,-0.16313765943050385,-0.8009220361709595,-0.6208198666572571,-0.4818032383918762,0.35198310017585754,0.03344692662358284,1.148575782775879,0.37458837032318115,1.1903361082077026,1.1212455034255981,1.4280223846435547,-0.2544806897640228,0.12605686485767365,0.9702161550521851,-1.5314195156097412,-0.9834274649620056,1.2258423566818237,-0.0007316068513318896,-0.4095584750175476,-0.6483793258666992,-0.4944479763507843,-0.548651933670044,-1.940783143043518,0.587120771408081,-0.6617400646209717,-1.0795561075210571,0.8286852836608887,0.622272789478302,1.5641201734542847,0.9197168946266174,-0.9733359813690186,-0.21150080859661102,0.52751624584198,-0.3511163890361786,0.3366188108921051,0.9783397316932678,-0.605208694934845,1.1182667016983032,-1.2944458723068237,-0.6589978933334351,0.9709337949752808,0.18529416620731354,-0.9022026062011719,-0.3725128769874573,-0.8007068634033203,0.4804517924785614,-0.005111392121762037,0.5340445041656494,-0.4325527846813202,-1.6461260318756104,0.9547731876373291,-0.8484756946563721,1.2880843877792358,0.7869770526885986,-0.13668334484100342,0.7463000416755676,-1.3389112949371338,0.85216224193573,-0.30634474754333496,0.970958411693573,-0.0507897324860096,-1.485526204109192,-0.8006786108016968,-0.21028773486614227,-0.5649224519729614,0.6598161458969116,-0.3906886875629425,-0.2752808928489685,-0.9905258417129517,0.2410048544406891,1.3847469091415405,1.0538798570632935,0.1536841094493866,-1.1882948875427246,0.34369534254074097,-1.3045227527618408,-0.6673234105110168,0.9804831743240356,0.4761040210723877,-2.3491814136505127,0.379354327917099,0.9397584199905396,-0.08642425388097763,0.04014251381158829,0.8169338703155518,0.6980165243148804,-1.7776672840118408,0.8539146184921265,0.2222406566143036,-1.6437445878982544,1.0346108675003052,-0.26240962743759155,0.1111006885766983,-0.6904146671295166,-0.1408616602420807,0.9814832210540771,-0.4131013751029968,0.19543027877807617,-1.626126766204834,-0.28944724798202515,0.07888037711381912,0.3403347432613373,-0.9908788800239563,-1.675341248512268,0.6066761612892151,0.8129071593284607,-0.5889402031898499,0.646695613861084,0.3114144504070282,-0.5078871250152588,-0.3932262361049652,-1.2035071849822998,0.9658588767051697,0.22978942096233368,-0.824199914932251,0.5752811431884766,-0.7115271091461182,-0.6255932450294495,0.7678413391113281,-0.5230691432952881,-0.12842155992984772,-0.885906994342804,-0.13780872523784637,-0.18495313823223114,-0.070290707051754,0.46807584166526794,-0.29769760370254517,1.4484108686447144,1.133284568786621,-0.8523627519607544,-1.3392903804779053,1.3381315469741821,0.12460633367300034,0.8350573182106018,-1.4452061653137207,1.4054515361785889,0.48086556792259216,-0.0951334536075592,1.0493463277816772,-0.6638138294219971,-0.7883672118186951,-0.856940507888794,0.5500520467758179,-0.9154321551322937,0.5136682987213135,0.2614312767982483,-0.4010925889015198,0.28214845061302185,1.1970250606536865,0.11203186213970184,0.15062913298606873,0.8085405826568604,0.5271850824356079,-0.5838783383369446,-0.3544270992279053,0.6517021059989929,-0.32631781697273254,1.007429838180542,-0.9255976676940918,-1.6865413188934326,-0.8651835918426514,0.35669490694999695,-1.5555649995803833,0.7859975695610046,0.6563619375228882,-1.5065803527832031,0.7457099556922913,-0.43897876143455505,0.23212623596191406,-0.3911909759044647,0.5077726244926453,-1.9543007612228394,-1.4609754085540771,0.773990273475647,0.684660792350769,-0.18986429274082184,-0.09212261438369751,-0.8907063603401184,0.33261486887931824,0.40967950224876404,0.11758848279714584,-0.6287823915481567,0.9456306099891663,0.16786672174930573,1.0320124626159668,-1.2590649127960205,0.3325486183166504,-1.3857289552688599,0.6584524512290955,-1.312806248664856,0.577176034450531,-0.5287860035896301,0.6872773766517639,-0.24949058890342712,0.11911081522703171,0.29726308584213257,-0.033645011484622955,0.4339572489261627,-0.44051340222358704,0.2133036106824875,-0.7432063817977905,0.9070778489112854,-1.5094536542892456,-0.03368682041764259,0.15894177556037903,-0.8213762640953064,-0.9642273783683777,0.9151901006698608,0.9904705882072449,0.3443436026573181,-0.7110878825187683,-1.1062325239181519,-1.2661981582641602,0.798980176448822,-0.9620945453643799,0.755145251750946,0.61643385887146,0.8261383175849915,0.377930223941803,-0.606306791305542,-1.4511535167694092,-0.8929144144058228,-0.6792312860488892,-0.19580014050006866,0.8106296062469482,0.67800372838974,-1.2310118675231934,-1.0420726537704468,0.3911733031272888,0.6155120134353638,-0.3502313792705536,-0.21063631772994995,0.5417519807815552,0.27810022234916687,-0.6132118105888367,-0.5959874987602234,-0.2710598111152649,-1.64714777469635,0.2612287402153015,-0.6977967619895935,-0.4373502731323242,0.9996548891067505,0.23714983463287354,0.1876024305820465,-0.030277477577328682,-0.6486417055130005,-0.1381666660308838,-0.3661305606365204,0.7621384263038635,-1.4762295484542847,-0.10041828453540802,-0.7001976370811462,-1.6466515064239502,0.580007791519165,-0.4887843132019043,0.6218898892402649,0.9174202084541321,0.2366151511669159,-0.771541178226471,-0.9613094329833984,-0.14045241475105286,0.12450302392244339,-0.028919553384184837,-0.17515841126441956,-1.6348427534103394,-0.6164876818656921,-0.30514082312583923,0.939176619052887,-1.0557150840759277,0.4864477217197418,1.4327338933944702,-0.1885242611169815,-0.29948967695236206,-0.6339497566223145,0.3974111080169678,-0.31061914563179016,-0.21305835247039795,0.6371406316757202,-1.5949007272720337,1.0531426668167114,0.623831033706665,-1.112086534500122,-1.0014346837997437,-1.6786876916885376,0.3119765520095825,0.43450337648391724,-0.3640628457069397,-0.14900346100330353,0.1511792242527008,0.18165089190006256,-0.3223160207271576,-0.02928803861141205,0.08587814122438431,-0.033165398985147476,0.898800790309906,0.6990002393722534,-0.0983978807926178,1.0306333303451538,0.3845011293888092,0.4459325075149536,1.746828317642212,0.6733714938163757,-0.9079079031944275,0.3829568922519684,-0.7583203911781311,0.6955662369728088,0.5262562036514282,0.7349171042442322,-0.010983504354953766,0.5301754474639893,-1.0933400392532349,-0.5963383913040161,0.6340401768684387,-0.2336588054895401,-0.023405777290463448,0.18853799998760223,0.3160470128059387,-0.9599000811576843,-1.3173562288284302,-0.8614282608032227,0.24639765918254852,0.7895186543464661,0.8415231704711914,-1.3516558408737183,0.09242787957191467,-1.021010160446167,0.2678295075893402,-0.4414686858654022,-1.1677480936050415,0.5117931962013245,0.7166622281074524,1.0888320207595825,-1.511529803276062,0.7790442705154419,-1.5328630208969116,-1.5625836849212646,-0.43480733036994934,-0.8954553604125977,-1.7200084924697876,0.010533003136515617,-0.13578104972839355,-1.452129602432251,-0.005223368760198355,-0.6624855995178223,-1.5574681758880615,0.6577795147895813,-0.8020643591880798,0.19053620100021362,-1.3075835704803467,0.07727625966072083,-0.11310657113790512,-1.5546783208847046,-1.3634358644485474,-1.586295247077942,-0.9366077780723572,-0.42636561393737793,-1.355765461921692,0.17444376647472382,-0.11032377928495407,0.5258181095123291,1.4213727712631226,0.5499633550643921,-1.328927993774414,-2.1357052326202393,0.6074632406234741,-0.9460353255271912,0.6189857721328735,0.6422238945960999,-1.1988343000411987,-0.873835027217865,0.1265687644481659,-0.23264946043491364,-0.48721256852149963,0.27240195870399475,0.6292078495025635,0.2498907446861267,-0.4511708915233612,0.1272467076778412,0.036160312592983246,-1.3130624294281006,-0.09782250970602036,0.203242689371109,-0.9128826856613159,0.5998809933662415,-0.5122947692871094,-0.045919377356767654,0.002587155671790242,-0.3326979875564575,-0.2558221220970154,0.5957937836647034,-1.430009126663208,-0.8606885075569153,0.6997964382171631,0.059598881751298904,-0.18673652410507202,-0.7310682535171509,-0.5050647258758545,0.9792746305465698,0.1787426471710205,0.3209121823310852,-0.5452762842178345,-0.19862285256385803,0.27223026752471924,-0.7235317230224609,-1.490713119506836,-0.3547852635383606,-1.2317116260528564,0.008140153251588345,-0.8892673254013062,0.16484305262565613,-0.32533079385757446,-2.073922634124756,0.3175484836101532,0.7063677906990051,-0.400067538022995,-0.11277497559785843,-0.617389976978302,-0.1617531180381775,0.6602038145065308,0.4017828106880188,-0.22891345620155334,-0.7366428971290588,0.8698969483375549,-0.19757944345474243,0.6493791341781616,-0.7078606486320496,-0.6393136382102966,0.6460620760917664,-0.6539766192436218,-0.21492303907871246,-1.1872315406799316,-0.8206962943077087,-0.16728875041007996,-1.6053152084350586,0.20981882512569427,-0.06812693178653717,-0.5113001465797424,-1.1204452514648438,0.31870532035827637,-1.4659345149993896,-1.288931131362915,0.40219518542289734,0.3312758803367615,0.31128332018852234,0.2843519449234009,0.5940207242965698,0.4627167284488678,-0.8845667243003845,-0.5942725539207458,0.9879675507545471,-0.6008182168006897,0.4668719172477722,-1.4353278875350952,-0.939609944820404,-1.506730318069458,-0.8827308416366577,0.22888140380382538,0.3237660229206085,0.2980496287345886,-0.4805675745010376,0.6880052089691162,-1.6186386346817017,0.4631463587284088,-0.6441473960876465,0.5456048846244812,0.5011966228485107,0.9257173538208008,-0.23217244446277618,-0.5970144271850586,0.4645979702472687,-0.2093300223350525,0.37184032797813416,0.6562578678131104,0.42981359362602234,-1.2402139902114868,-1.6505745649337769,0.4320026636123657,-0.3671720027923584,0.20004592835903168,-1.2335466146469116,0.5624454021453857,-2.33084774017334,0.4779514968395233,0.23620568215847015,0.6218042969703674,0.7019836902618408,0.14700080454349518,-1.768289566040039,-1.2010791301727295,-0.2971137762069702,1.5330443382263184,-0.6981276869773865,-0.22877106070518494,-1.307029128074646,0.1880088597536087,0.3204393684864044,0.34168967604637146,-1.6016426086425781,-1.0890834331512451,-1.123810887336731,-0.02707076072692871,0.31270232796669006,-0.37840697169303894,0.5603865385055542,1.1322473287582397,0.5991104245185852,-1.7913265228271484,-1.6451146602630615,0.6748274564743042,0.366862028837204,-1.5264848470687866,0.6635550260543823,-1.388677716255188,0.5457785129547119,-1.5209664106369019,-0.7848806977272034,0.7384403347969055,0.7279243469238281,-0.5704075694084167,-1.0779809951782227,-1.4561711549758911,-0.6870285272598267,0.34784844517707825,-0.13964559137821198,-0.6304566860198975,-0.29298853874206543,0.3769618570804596,-0.7032185792922974,-0.7160049676895142,-0.06369703263044357,-0.10843698680400848,-1.2706234455108643,-0.5281874537467957,-0.06450170278549194,-0.17492781579494476,0.32317453622817993,0.6171807646751404,1.0450890064239502,-0.6817417740821838,-0.26144346594810486,0.021949097514152527,-1.4562608003616333,-0.11514274775981903,0.5103634595870972,-0.7261317372322083,-0.3726910948753357,-1.3630720376968384,0.09198295325040817,-1.3210362195968628,-0.9469315409660339,-0.20103664696216583,0.5669775009155273,-0.9271271824836731,0.6292309165000916,0.3960039019584656,0.24665848910808563,-0.9120251536369324,-0.8630509972572327,-0.2089923769235611,-0.8715837001800537,-0.09618773311376572,-1.106445074081421,-0.6532732248306274,-0.30875030159950256,0.24095746874809265,0.5543175339698792,-0.297343373298645,0.4825607240200043,-0.3173356056213379,0.6155654788017273,0.6499104499816895,0.19062964618206024,-1.1196538209915161,-0.46422910690307617,0.40889760851860046,0.6138535141944885,-0.1800161600112915,-0.30228158831596375,0.10144517570734024,-1.1018699407577515,-0.3453070819377899,-0.09145905077457428,0.2784799039363861,0.8867079615592957,0.2794382870197296,-1.260520100593567,0.33953410387039185,0.6183526515960693,-1.5198159217834473,-1.6805332899093628,0.11026596277952194,-0.4947911500930786,-0.11667079478502274,-1.0492947101593018,-0.12250521034002304,0.351049542427063,-1.024571418762207,0.4126560688018799,1.3694188594818115,-0.5972260236740112,0.9233475923538208,0.5845599174499512,-0.6710447669029236,0.6580283641815186,-0.5226669907569885,-0.23566359281539917,-0.6212096214294434,-0.6064145565032959,-0.6350345015525818,-0.715960681438446,0.35792651772499084,-0.5640397071838379,0.5837324261665344,0.6720238924026489,0.4533059895038605,0.26465705037117004,-1.3631623983383179,0.3513335585594177,-1.1079888343811035,-0.4309435486793518,0.790002167224884,0.9526443481445312,0.4348548948764801,-0.9272369146347046,-1.189878225326538,0.6816616654396057,-1.5730959177017212,-0.6514918804168701,-1.5155377388000488,0.46561557054519653,0.48084864020347595,0.8401122689247131,0.5850238800048828,0.7823743224143982,-0.9862878322601318,-0.5187622308731079,-0.7142137885093689,-0.005644422955811024,0.1344289779663086,-1.2300750017166138,-0.8363431692123413,-0.1206074133515358,-0.20328393578529358,-0.9855531454086304,-0.7851227521896362,-0.06628908962011337,-0.3905797302722931,-0.976718544960022,0.1986515074968338,0.28569304943084717,0.9620864391326904,-1.0038715600967407,0.6679019331932068,0.745550274848938,0.38235488533973694,0.4647941589355469,-0.5222596526145935,-0.6816557049751282,0.5034725666046143,-0.2459615170955658,0.72801274061203,0.1322975754737854,0.15982486307621002,-0.673066258430481,0.9895410537719727,-0.13066110014915466,-0.6013501286506653,0.7293394804000854,0.3981061577796936,-0.9191027283668518,0.04649290814995766,0.1462690532207489,-0.1419685035943985,-1.29581880569458,-0.6465481519699097,-0.05715969204902649,-1.250898003578186,0.6035323739051819,0.5173895955085754,-1.1159186363220215,-1.1020809412002563,-0.48840683698654175,-0.945482075214386,-0.8221602439880371,-1.1927299499511719,-1.2917050123214722,-0.6980317234992981,-1.0768722295761108,-0.15425990521907806,-0.6921756863594055,-0.6115149259567261,-0.3571403920650482,0.7049760222434998,0.760467529296875,-0.846491813659668,-0.6282021403312683,0.5239652395248413,0.8990126252174377,-0.48710712790489197,-1.2642487287521362,-0.006339450366795063,0.2591382563114166,0.1797216385602951,-0.731615424156189,-0.34280097484588623,0.15776421129703522,-1.1946048736572266,0.3911939561367035,-1.6850733757019043,-0.24547578394412994,0.2166178822517395,1.1324025392532349,-1.6183605194091797,-0.5453022718429565,-0.192213237285614,-1.5781025886535645,0.18363700807094574,-0.627723217010498,-1.470796823501587,0.780022144317627,-0.7821905016899109,-0.18206557631492615,0.16280965507030487,0.21626968681812286,1.5035665035247803,-0.5892107486724854,-0.7333300113677979,-0.23359031975269318,-0.5556479096412659,-0.1380797177553177,-0.7631144523620605,0.40264061093330383,-0.5341843962669373,0.5646820068359375,0.5094935297966003,0.17518778145313263,-0.5420228838920593,-0.9279429316520691,-0.40520715713500977,-0.23219306766986847,-0.7129926681518555,-0.24252629280090332,-1.5391755104064941,0.8907256126403809,-0.07057662308216095,-0.9957414269447327,-0.8486480712890625,1.124241828918457,0.626478374004364,0.04074039310216904,0.3105596601963043,1.3600209951400757,-0.5251132845878601,0.3186158835887909,-0.7705419063568115,-0.26840490102767944,-1.3920756578445435,-0.31663426756858826,-0.12105877697467804,-0.12108612060546875,-1.384324073791504,-1.3035496473312378,-0.06197574734687805,-1.2406103610992432,-0.15130499005317688,-0.3893366754055023,-0.08428116887807846,-0.6506848931312561,0.4145658016204834,-0.8115535974502563,0.19350753724575043,-1.1326618194580078,0.12731529772281647,-0.1766934096813202,-0.007230270188301802,0.3001772165298462,-0.9706260561943054,0.6239597201347351,-0.31018301844596863,-0.5568361282348633,0.5493170022964478,-0.6298425793647766,0.10586792975664139,0.3397248685359955,0.5010496973991394,0.2890259623527527,-0.3838857412338257,0.5503713488578796,-0.15686628222465515,0.6733759045600891,-1.6595243215560913,-0.017935700714588165,-0.3558432161808014,-1.2056924104690552,0.33992063999176025,-0.8758140206336975,-1.5956305265426636,0.12186126410961151,-1.0456275939941406,-0.3724365830421448,0.49318236112594604,0.6526474356651306,-0.3700464367866516,0.47244182229042053,-0.6812050938606262,-0.12690576910972595,0.01706799492239952,0.8913553357124329,0.08225424587726593,0.3668089807033539,-1.2275274991989136,0.4740903079509735,-0.06094743683934212,0.5664358139038086,-1.291446328163147,-1.446366548538208,-0.2169935256242752,-0.2420186847448349,-1.3000472784042358,-0.21562494337558746,-0.3897513151168823,0.9061551690101624,-0.9555941820144653,-0.266519159078598,-0.182346910238266,0.6792949438095093,0.20957307517528534,-0.05699286237359047,0.7603094577789307,-0.45389050245285034,-0.8745099902153015,-0.7769070863723755,-0.18282869458198547,-0.9965888261795044,0.29209470748901367,-0.4538383185863495,-1.1035640239715576,0.6756601929664612,0.12322451174259186,0.035861529409885406,-0.2584950029850006,0.4077483117580414,0.04617917165160179,-0.7218552827835083,-1.5175000429153442,0.146789088845253,0.2901928722858429,0.5053927302360535,-0.8099687695503235,-1.2366106510162354,0.2904492914676666,0.6834282875061035,-0.17583563923835754,-0.8589755892753601,-1.1858863830566406,0.6385362148284912,-0.194686621427536,-1.0547266006469727,-0.46586689352989197,-0.43066632747650146,-0.43235763907432556,-1.2308982610702515,0.40575850009918213,0.33187294006347656,-0.20850716531276703,0.317634254693985,-0.309722900390625,0.14419981837272644,0.2171775996685028,0.032242823392152786,0.21846643090248108,-0.773276686668396,-0.01889128051698208,-1.05098295211792,-0.5713737607002258,-0.9771531820297241,0.14519114792346954,0.4760568141937256,-0.2773333489894867,-0.4031958281993866,0.15031535923480988,0.2552221119403839,0.48430511355400085,-0.23825745284557343,-1.3604848384857178,-0.5078174471855164,0.6308583617210388,-0.1942025125026703,-0.04533451795578003,0.3925791084766388,-0.6200659871101379,0.4929366409778595,0.530855119228363,-1.1069958209991455,0.17550738155841827,-1.2360984086990356,-0.9259926676750183,0.4428483843803406,-0.3774605989456177,-1.260625958442688,0.7104288935661316,-0.4676872193813324,-0.8575075268745422,-0.19764456152915955,-0.8653682470321655,-0.6001935601234436,0.26322826743125916,-0.2576163411140442,0.026325875893235207,0.7422483563423157,-0.689923107624054,0.36414265632629395,0.36593177914619446,0.4490993022918701,-0.7713096737861633,-0.39549657702445984,-0.5073891878128052,0.43053722381591797,0.3627239763736725,0.3100183308124542,-1.0928443670272827,-0.9920135736465454,0.48403316736221313,-0.8519904613494873,-0.1608186662197113,-0.8086904287338257,0.5017678737640381,-0.3422029912471771,0.5587221384048462,0.09380941838026047,0.5597478747367859,-1.4423907995224,0.5025583505630493,-1.4317617416381836,-0.1460234522819519,0.38457974791526794,0.11478455364704132,0.26456761360168457,-0.7337920069694519,0.567147433757782,-0.0840986967086792,0.45537254214286804,-0.12873242795467377,-1.1188280582427979,-0.6547781229019165,0.10090711712837219,-0.15470263361930847,-1.0239086151123047,-0.8034722805023193,-0.9450246095657349,-0.6827489137649536,0.13665376603603363,0.45776939392089844,-0.029590211808681488,0.8909645676612854,0.5475976467132568,0.45465508103370667,-0.5289313197135925,0.4461143910884857,-0.28510844707489014,-0.7577537298202515,-0.8618002533912659,-0.08107279986143112,0.19765445590019226,-1.6477550268173218,0.4525133967399597,0.39238882064819336,-0.4700111746788025,0.1832856386899948,-0.9463343024253845,-0.22292806208133698,-0.16670575737953186,0.4537101089954376,-0.049155350774526596,0.03409002721309662,-0.7573820948600769,-0.4556121528148651,-0.5490009784698486,-0.032975077629089355,-0.5711982250213623,0.30225569009780884,-0.775307834148407,-1.2183310985565186,-0.671329915523529,0.274917870759964,-0.19331267476081848,-0.12494060397148132,-1.152071475982666,-0.27539536356925964,0.7538515329360962,0.4309702217578888,-0.8781101107597351,0.7460495829582214,-0.6068398356437683,0.3864847719669342,-0.10916728526353836,0.597548246383667,-0.7246881127357483,-0.20520144701004028,0.5052049160003662,-0.6420581936836243,0.4843095541000366,-0.411545068025589,-0.20636703073978424,-0.00174842425622046,-0.15732528269290924,-1.2407978773117065,-0.7732027769088745,-0.5759427547454834,-0.8341777324676514,0.050939567387104034,-1.151277780532837,-1.316129446029663,0.4508613348007202,-0.6266955137252808,0.06094149500131607,-1.1423485279083252,-0.5316725969314575,-0.5343673825263977,0.050183698534965515,0.40645653009414673,0.316335529088974,0.42470782995224,0.3284629285335541,-0.9166823625564575,-0.3423260450363159,0.650947093963623,0.19951464235782623,0.5402671098709106,0.6113941073417664,-0.8841266632080078,0.2483784556388855,-0.3288525640964508,-0.3620084524154663,0.43011602759361267,-0.18804912269115448,0.7185848355293274,-0.5688061118125916,-1.1153753995895386,0.1892307996749878,0.494895875453949,-0.6358049511909485,-0.5229203701019287,-0.16950367391109467,0.0680093988776207,-1.5559039115905762,0.4267183542251587,-0.2870451509952545,0.2302742302417755,-0.015542218461632729,0.432468980550766,0.3483004570007324,-0.4345317482948303,0.07853133231401443,-0.34984102845191956,0.3633774220943451,0.3521634042263031,-0.12882386147975922,-1.2536571025848389,-0.3723220229148865,1.000472068786621,0.2089022845029831,0.5098000764846802,-1.3513429164886475,0.46479836106300354,-0.4100340008735657,0.6453855037689209,0.2707308828830719,-0.8431142568588257,0.09459192305803299,0.16667482256889343,0.35721442103385925,-0.6537984609603882,0.23172762989997864,-0.11374053359031677,0.31901174783706665,-0.9769027829170227,0.43245765566825867,-0.7287997603416443,0.21333996951580048,0.4983731508255005,-0.7528806328773499,0.19369646906852722,-0.01523759588599205,0.24060548841953278,1.0597243309020996,0.37387630343437195,-0.1846659928560257,-0.6254923939704895,0.3881646394729614,0.7559216022491455,0.08225822448730469,-1.5880578756332397,-1.3455774784088135,0.7321345806121826,-0.8369407057762146,-0.444010466337204,-1.2391184568405151,0.12113407999277115,-0.8507823348045349,-0.17002081871032715,-0.20129165053367615,0.5559105277061462,0.21213266253471375,0.32305392622947693,0.08885065466165543,-0.2476307600736618,-0.233931764960289,-0.10169214010238647,-0.2910252511501312,-0.25743237137794495,0.32140445709228516,0.044109322130680084,-1.2700841426849365,-0.5452549457550049,-0.3966981768608093,-0.08157385140657425,-0.36526593565940857,0.47955483198165894,-0.777347981929779,-0.8512776494026184,0.5463449954986572,0.21320819854736328,-0.18273621797561646,0.06832148879766464,-0.8559210896492004,0.39678940176963806,-0.43950292468070984,-0.02773595042526722,-1.358938455581665,-0.5708840489387512,-0.3987290561199188,-1.225798487663269,0.1941957175731659,0.6557292342185974,0.5812721252441406,0.6473110318183899,-0.41946011781692505,0.24746598303318024,-0.4208155870437622,-0.04789110645651817,-1.3658887147903442,0.23584261536598206,0.08424228429794312,-1.3937828540802002,-0.324828565120697,0.3577922284603119,-0.5193548798561096,-1.0122970342636108,0.6755408644676208,-0.004100326914340258,-0.540529191493988,-0.12345363199710846,-0.2893159091472626,0.4453963339328766,0.7990971803665161,0.26896917819976807,-0.15864115953445435,-0.7304345965385437,0.3752272129058838,0.4027591347694397,-1.332126498222351,-1.2166283130645752,0.03883906826376915,-1.2928593158721924,-0.8886733055114746,0.18090170621871948,0.2836918532848358,-0.7192933559417725,-0.3014239966869354,-1.3266183137893677,-0.9132358431816101,0.30399927496910095,0.5557119250297546,-0.5525345206260681,0.46052873134613037,-1.0141839981079102,-0.07882371544837952,-0.8941167593002319,-0.9453467130661011,-0.7573940753936768,-0.7140663862228394,-0.11475138366222382,0.39371466636657715,0.3032873570919037,0.28834694623947144,-1.0340474843978882,-0.008566492237150669,0.32906574010849,0.30774417519569397,-1.0045254230499268,-0.17847569286823273,-1.2122470140457153,-0.5786681771278381,-0.08152591437101364,0.44830960035324097,-0.8787621855735779,0.2205020636320114,-1.1565897464752197,-0.8151939511299133,0.14068326354026794,-0.7101751565933228,-0.4475114941596985,0.20856991410255432,-1.155914068222046,0.0511629693210125,-0.21703501045703888,-0.2775498926639557,-0.8532683849334717,0.18045608699321747,-0.8259068131446838,-0.1927090883255005,0.554964542388916,0.5701532959938049,-0.1890679895877838,-0.8733764290809631,-0.598772406578064,-0.19825835525989532,-0.25888004899024963,-0.30953627824783325,0.5984092950820923,0.3720313310623169,-1.0355134010314941,0.31809040904045105,-0.5166659951210022,-0.3208061158657074,-0.2651885151863098,-1.1686826944351196,-1.0876129865646362,0.42771708965301514,0.5102002620697021,-0.7351119518280029,-0.6452993154525757,-1.268660545349121,0.4097622334957123,0.556563675403595,-1.1419250965118408,-0.2272621989250183,-0.4740018844604492,-1.0477317571640015,0.26792651414871216,0.4784758388996124,-1.109579086303711,-0.8030157089233398,-0.4540894329547882,0.23121260106563568,-0.021247627213597298,-0.3942837119102478,0.2567278742790222,-0.33623167872428894,-0.03395415097475052,-1.2394741773605347,0.4949180483818054,0.6375081539154053,0.28873294591903687,0.13429951667785645,0.15382719039916992,0.19799423217773438,-0.3257271349430084,-0.3819344937801361,0.009287393651902676,-0.9830077886581421,0.6678320169448853,-0.3002791702747345,-1.345632791519165,0.4815935790538788,0.16263620555400848,0.26007983088493347,0.012603657320141792,-0.48799484968185425,-0.003961422014981508,-0.17109836637973785,0.8403189182281494,0.09297488629817963,-0.06777606904506683,-0.09030743688344955,0.26909512281417847,-1.2486203908920288,-0.6156145334243774,-1.2286744117736816,0.3007449209690094,0.21372272074222565,0.3951601982116699,-0.3162168264389038,-0.3761667013168335,-0.43370872735977173,-0.5966045260429382,-0.022617211565375328,-1.2848867177963257,-0.3099762499332428,-1.0300962924957275,-1.078356146812439,0.1746877133846283,0.6121417284011841,0.5870774388313293,-0.957274854183197,0.44449228048324585,-0.4546506106853485,-1.2278765439987183,-1.2623348236083984,-0.8422213196754456,-1.7362111806869507,-1.1853770017623901,0.4626394212245941,0.3860858380794525,-1.044228434562683,-0.547793984413147,-0.9768531322479248,-0.2565697133541107,-0.014072656631469727,-0.1262064427137375,-1.2004913091659546,0.48869436979293823,-0.6178291440010071,0.25656288862228394,0.19992154836654663,0.26319193840026855,0.6613613367080688,0.3661133348941803,-0.3885040581226349,0.38545483350753784,0.8266750574111938,-1.2383610010147095,-0.8960404992103577,0.2598939836025238,-0.0071000466123223305,0.11958184093236923,-0.7817507982254028,0.2731409966945648,-0.8879433274269104,-1.3833359479904175,-0.09157884120941162,-0.43942543864250183,-0.27029669284820557],"xaxis":"x","y":[-0.03673819452524185,0.03192884847521782,0.037155650556087494,0.15038177371025085,0.06952713429927826,0.15414558351039886,-0.040291592478752136,0.008756132796406746,0.0038619120605289936,0.0493793711066246,-0.10116951167583466,0.04088969901204109,-1.830660767154768e-05,0.022811057046055794,0.020450269803404808,-0.0653436928987503,0.08296758681535721,0.05767393112182617,-0.07210884243249893,0.03203578665852547,0.0627116709947586,0.06451141089200974,-0.06974522024393082,0.03683529794216156,0.03817188739776611,0.04022693634033203,0.015926789492368698,0.03616486117243767,0.13256262242794037,-0.04299837723374367,0.04339344799518585,0.1294575035572052,0.006263113580644131,-0.03455860912799835,0.07314653694629669,-0.05701867863535881,0.06592211872339249,0.02608289197087288,-0.024109242483973503,-0.009060033597052097,0.02110264264047146,0.01750962808728218,0.07114267349243164,-0.03988683223724365,-0.01717364601790905,0.01521014142781496,-0.001268406049348414,0.012370927259325981,0.005634765140712261,-0.04325522482395172,0.041478317230939865,0.05260012671351433,0.08698409795761108,0.024684205651283264,-0.011511729098856449,-0.051061321049928665,0.016268935054540634,0.04516156390309334,-0.09539572894573212,0.049925968050956726,-0.030553342774510384,-0.023451434448361397,-0.07952804118394852,0.04523615166544914,-0.07497406005859375,0.10402906686067581,0.0017246841453015804,-0.012762325815856457,0.0032550247851759195,-0.009491240605711937,0.04231685400009155,-0.10477659106254578,0.01993158645927906,0.019902613013982773,-0.08886892348527908,0.019280118867754936,-0.005643413867801428,-0.028898976743221283,0.0329023152589798,0.10139159858226776,-0.01232588104903698,-0.023383934050798416,0.11990898847579956,0.008191525004804134,0.050705686211586,-0.03794203698635101,0.06480606645345688,-0.09720148891210556,-0.10479556024074554,-0.010211546905338764,-0.02802969701588154,0.0017688032239675522,-0.054002951830625534,0.10208360850811005,-0.03489259257912636,-0.03016543947160244,0.04852847382426262,0.1744493991136551,0.08871330320835114,0.07411667704582214,-0.009444138035178185,-0.13498631119728088,-0.021646734327077866,-0.02121264673769474,-0.07217250764369965,0.133299320936203,0.042555518448352814,0.021536896005272865,-0.045427560806274414,-0.04385621100664139,-0.1445494294166565,-0.04333624988794327,0.00784932542592287,-0.026168780401349068,-0.04703713580965996,0.033040475100278854,0.04951458424329758,0.0740167498588562,-0.024141695350408554,-0.05181661248207092,0.03922621160745621,0.04725812003016472,0.09179551899433136,-0.00025897848536260426,0.04642803594470024,-0.050870250910520554,0.03127145767211914,-0.025951189920306206,0.05969199538230896,-0.11426883935928345,-0.03464069962501526,-0.014270703308284283,-0.03810049220919609,0.017842452973127365,-0.11836423724889755,-0.007830983027815819,-0.00889354757964611,0.11874494701623917,0.028020333498716354,-0.04415547847747803,0.02909971959888935,-0.010293480940163136,0.08180389553308487,0.017391376197338104,0.05334118753671646,-0.016488531604409218,0.06828171014785767,-0.10458381474018097,-0.03528580814599991,-0.052316851913928986,0.05628837272524834,-0.09026200324296951,-0.051722414791584015,0.0009327374282293022,-0.11843694746494293,-0.03520070016384125,-0.010309399105608463,-0.08376257121562958,-0.0008650212548673153,0.0017833843594416976,0.0404728464782238,-0.019848311319947243,0.0018345187418162823,-0.052339643239974976,0.01869291253387928,-0.09645643830299377,-0.019986029714345932,0.0034006815403699875,-0.03320159763097763,-0.0037527168169617653,-0.024228347465395927,0.04048394039273262,0.0013014806900173426,-0.02789425663650036,-0.005938969552516937,-0.06978629529476166,-0.08113088458776474,-0.0261529628187418,-0.032142769545316696,0.07799231261014938,-0.04532500356435776,-0.008265446871519089,0.055604662746191025,0.0678856149315834,-0.09151444584131241,0.03870914503931999,-0.03657280281186104,-0.032220520079135895,0.0571645051240921,0.011562224477529526,-0.00392856216058135,-0.03800925984978676,0.008697477169334888,-0.08046970516443253,-0.07444953173398972,0.08179570734500885,-0.08928268402814865,-0.021085023880004883,-0.036528363823890686,-0.008565628901124,-0.012209544889628887,0.04469161853194237,0.1096130758523941,-0.019555959850549698,0.028950674459338188,0.0704776868224144,0.021930057555437088,0.02727895975112915,0.04442224279046059,0.010273131541907787,-0.032671764492988586,0.018152570351958275,-0.040641266852617264,-0.03466527536511421,0.06593164801597595,0.020146062597632408,0.030132895335555077,-0.0117474514991045,-0.0020287916995584965,0.016191577538847923,0.03221923112869263,0.00838007964193821,0.12047425657510757,0.07169585675001144,0.0027009681798517704,-0.004778739530593157,0.04161710664629936,-0.000751001643948257,0.14052779972553253,-0.09461930394172668,-0.06368402391672134,-0.10746162384748459,0.005428657867014408,-0.019196519628167152,-0.1119883805513382,0.06194750964641571,-0.04989360272884369,0.10317599773406982,0.04071670025587082,0.07411900162696838,0.0138453533872962,0.07272172719240189,0.03188605606555939,0.009080200456082821,0.019482851028442383,-0.009858989156782627,-0.06953166425228119,-0.015873178839683533,-0.061335254460573196,0.0006604177178815007,0.03861722722649574,0.012294326908886433,-0.04115396738052368,-0.13467468321323395,0.05699547752737999,0.0019242814742028713,-0.019833240658044815,0.08159217983484268,-0.010587704367935658,-0.11105559021234512,-0.07225099951028824,0.012495862320065498,0.01919344626367092,-0.0690082386136055,-0.019764836877584457,-0.054026469588279724,-0.15195195376873016,-0.034058406949043274,0.028762223199009895,-0.06107784062623978,0.05622350797057152,0.050398726016283035,0.09219618141651154,-0.031197935342788696,0.025362534448504448,0.0005838303477503359,-0.1070781946182251,-0.06954722106456757,0.028859341517090797,-0.046046409755945206,-0.033317532390356064,-0.08429573476314545,-0.09851905703544617,-0.018655043095350266,0.041610077023506165,0.017702240496873856,-0.034739427268505096,0.008878987282514572,0.03946774825453758,-0.10259765386581421,0.017813585698604584,-0.012813406065106392,0.018666058778762817,0.04455411061644554,-0.05822710320353508,0.014743662439286709,0.10501990467309952,-0.11395837366580963,-0.025794826447963715,0.04386972263455391,0.007064312230795622,-0.014486056752502918,0.04842418059706688,-0.04067067429423332,-0.04592900350689888,0.024566790089011192,-0.01483701728284359,0.022035563364624977,-0.12212716788053513,-0.0654761791229248,0.0456760972738266,0.07083174586296082,0.005689662415534258,-0.06516651064157486,0.01790064200758934,-0.08392725139856339,-0.0028115243185311556,-0.027685828506946564,0.11777146905660629,-0.009755057282745838,0.10930075496435165,-0.08369622379541397,-0.030189456418156624,0.053822483867406845,-0.0011225348571315408,-0.04366593062877655,-0.046673551201820374,-0.020150423049926758,0.019629068672657013,-0.08694853633642197,-0.03661741688847542,-0.023533141240477562,0.04102976247668266,0.07142220437526703,0.04021638259291649,0.006525053177028894,0.05206422880291939,-0.1272287666797638,0.028735488653182983,0.1109878346323967,-0.06400453299283981,-0.06599876284599304,-0.04367149993777275,0.07067548483610153,-0.007349204737693071,0.0071803126484155655,0.04169369116425514,-0.01859891414642334,0.06441167742013931,0.047912731766700745,0.008517892099916935,-0.05141985043883324,-0.06397506594657898,-0.017582032829523087,-0.061031829565763474,0.04235372319817543,0.05136910825967789,0.01328112930059433,0.05898485332727432,-0.05379294231534004,-0.129394069314003,-0.06049467995762825,-0.08644866198301315,0.06332298368215561,0.025844421237707138,0.0478312112390995,0.0101173659786582,0.032077863812446594,0.07890156656503677,0.019198017194867134,-0.00776714039966464,0.004982692189514637,0.007347328122705221,0.025043154135346413,0.06593560427427292,-0.021815506741404533,0.02528252825140953,-0.021821603178977966,-0.14101868867874146,0.05643704533576965,0.04760169982910156,0.03098136931657791,-0.019284045323729515,0.00671997619792819,-0.039416417479515076,-0.02604508027434349,0.10584793239831924,0.09131298959255219,-0.005715351086109877,0.04117406904697418,-0.07272934913635254,-0.035895250737667084,-0.024945160374045372,-0.020725339651107788,0.008909355849027634,0.0357370525598526,-0.0366174653172493,0.14075228571891785,-0.00762613071128726,-0.020690497010946274,0.027455199509859085,-0.06221462041139603,-0.10193174332380295,-0.0047662267461419106,0.054130226373672485,0.08568968623876572,0.009882176294922829,0.08785728365182877,-0.049173615872859955,0.020242493599653244,-0.0463375598192215,0.03359211981296539,0.018408460542559624,0.09883511811494827,0.04631999135017395,-0.03604899346828461,0.015619941987097263,0.026987090706825256,-0.1350354701280594,0.00920608639717102,-0.002681222977116704,0.008168383501470089,-0.0635015219449997,-0.055240485817193985,0.02223350666463375,0.0780128613114357,0.01794176548719406,-0.008731907233595848,0.05543773993849754,0.06024669110774994,-0.036722581833601,0.0569453127682209,-0.012388677336275578,0.0347098670899868,0.04006174951791763,0.03798006474971771,0.049713946878910065,0.02605067938566208,0.024360135197639465,-0.0667615458369255,-0.015944309532642365,-0.0005449209711514413,0.014591106213629246,-0.016731858253479004,-0.11191290616989136,-0.024484725669026375,-0.012772473506629467,-0.041952941566705704,-0.002125837840139866,-0.09637477993965149,-0.05663735046982765,0.08942615985870361,-0.03379897400736809,0.021466944366693497,-0.05693785101175308,0.055066779255867004,-0.04816875606775284,0.03380489721894264,-0.05630682781338692,-0.06776329129934311,-0.02166084200143814,0.06519961357116699,-0.03707638010382652,-0.02847766876220703,-0.015544138848781586,0.026145728304982185,0.025775684043765068,0.011276631616055965,-0.007122961338609457,0.006191239692270756,-0.00982818752527237,-0.1002771332859993,-0.05016949400305748,-0.007101372350007296,-0.025788167491555214,-0.04090388864278793,0.005869705229997635,0.03989988937973976,0.0862542986869812,0.034359272569417953,-0.031309522688388824,-0.007692348677664995,-0.020189378410577774,-0.027900442481040955,0.03583092242479324,-0.01010485365986824,-0.029954982921481133,0.06122007220983505,0.08403219282627106,-0.039209552109241486,0.07171298563480377,-0.07671286165714264,-0.06722982227802277,-0.04887629672884941,0.0090425293892622,0.030058400705456734,0.027720164507627487,0.02397323027253151,-0.05113968998193741,0.06882039457559586,0.0847388356924057,-0.0716000646352768,0.01700442284345627,0.004496136214584112,0.004681215155869722,-0.0060800667852163315,0.052183013409376144,-0.06888383626937866,-0.01928449049592018,0.018748996779322624,0.14495934545993805,-0.012519381940364838,-0.01784607209265232,-0.026801306754350662,0.003732310375198722,-0.1228659600019455,0.006124166771769524,0.08447838574647903,0.05603477731347084,0.022508373484015465,-0.11405174434185028,-0.04407178983092308,-0.029148856177926064,0.197823166847229,0.01789143681526184,-0.012150350958108902,-0.08520627021789551,0.009318595752120018,0.18042393028736115,0.06131642311811447,-0.09421902149915695,0.060376718640327454,-0.005744639318436384,-0.07745687663555145,-0.06434547901153564,-0.04603712260723114,0.05189855024218559,0.09464564919471741,0.011421949602663517,-0.015916956588625908,0.093977190554142,-0.009042599238455296,0.030863989144563675,-0.10217519104480743,-0.01385401003062725,0.010357758961617947,-0.010750118643045425,-0.07675287872552872,0.040109314024448395,-0.01926974020898342,0.049804821610450745,0.042187802493572235,-0.0945127010345459,0.014738015830516815,0.02490590699017048,-0.06363193690776825,0.022369850426912308,0.07042761892080307,0.005046600475907326,-0.03520205244421959,0.04807116836309433,-0.12219925224781036,0.03263462707400322,-0.036577124148607254,-0.05181458964943886,-0.024206170812249184,0.01779269240796566,-0.013825795613229275,-0.059220559895038605,-0.13942532241344452,0.10334603488445282,0.043909985572099686,0.04899350553750992,0.013197880238294601,-0.012808474712073803,0.029087621718645096,0.019244803115725517,-0.04052666947245598,0.028775306418538094,0.0043122125789523125,0.07037945836782455,0.07469223439693451,0.10097774118185043,0.1174193024635315,0.023911090567708015,-0.0866861417889595,-0.0698641836643219,-0.019299451261758804,-0.05332256853580475,0.05518598482012749,0.08693749457597733,-0.10971852391958237,-0.03696325793862343,-0.019734015688300133,0.041226524859666824,-0.05935802683234215,0.0634252205491066,0.08400918543338776,0.06520258635282516,-0.09464211761951447,0.009719458408653736,0.03478384390473366,-0.005486710462719202,0.06849493086338043,0.061321109533309937,0.004463064018636942,0.021060379222035408,0.0009510702802799642,0.044266823679208755,-0.0964282900094986,-0.006829752586781979,0.10334564000368118,7.21755568520166e-05,0.0014352522557601333,-0.027137596160173416,0.011423297226428986,-0.01981942169368267,0.03587152436375618,0.020477814599871635,-0.033648714423179626,0.02951725386083126,0.03984103724360466,0.031460657715797424,-0.010429364629089832,0.10681966692209244,-0.04529987648129463,0.0065569388680160046,0.021243920549750328,-0.02976723201572895,-0.07969362288713455,-0.0319143608212471,-0.13699300587177277,0.046984028071165085,0.0027773689944297075,0.07062694430351257,-0.0969858318567276,0.028755467385053635,-0.013184593059122562,0.0019373983377590775,0.013411697000265121,0.05781346932053566,-0.00519039249047637,0.02265389822423458,0.03791816905140877,0.011961173266172409,0.021079756319522858,0.018665563315153122,-0.00829088594764471,-0.0566331222653389,0.03672655671834946,-0.011933819390833378,-0.0010875510051846504,0.003070519771426916,-0.03751209005713463,-0.040624458342790604,-0.07897654920816422,0.04570756480097771,0.0024564783088862896,-0.05324606969952583,0.032087381929159164,0.009411003440618515,0.024375054985284805,0.0005246076616458595,0.04776264354586601,0.12297430634498596,0.039610251784324646,-0.06949508935213089,-0.02516154572367668,-0.0833563581109047,0.12889376282691956,-0.014830224215984344,0.01258887629956007,-0.09287254512310028,-0.08244682848453522,0.025240348652005196,0.07151134312152863,0.04971609264612198,-0.12184996157884598,-0.030186684802174568,0.031849972903728485,-0.027849338948726654,-0.026689503341913223,-0.022746097296476364,0.020967459306120872,-0.09765475988388062,-0.0011649474035948515,-0.005618701688945293,-0.01526434812694788,-0.0012551241088658571,-0.16935859620571136,0.07489001005887985,0.010001735761761665,0.038719937205314636,0.039396025240421295,-0.0860610157251358,-0.06631385535001755,-0.08112262189388275,0.03588198125362396,-0.0882522389292717,0.10553823411464691,-0.13586652278900146,-0.03021836280822754,0.013745572417974472,0.0005998428096063435,0.043512746691703796,0.011444292962551117,-0.037527672946453094,0.017009461298584938,-0.006115625146776438,0.12040155380964279,0.017590217292308807,-0.04357706755399704,-0.10796651244163513,0.052853915840387344,-0.07380343228578568,0.04254723712801933,-0.054021287709474564,0.09177232533693314,0.05837913602590561,-0.006366270128637552,0.03629511222243309,0.03812563419342041,0.03143863007426262,0.02518066018819809,0.01786193810403347,-0.031178783625364304,-0.07078991085290909,0.01951969787478447,0.002629286842420697,0.10448707640171051,0.04520260915160179,-0.018375767394900322,0.041328687220811844,-0.10560114681720734,0.06399229913949966,0.038254667073488235,-0.01232678722590208,-0.08557403832674026,-0.05462320148944855,0.08960738778114319,-0.10189400613307953,-0.013108056038618088,0.024449005722999573,-0.04043915495276451,0.10062692314386368,-0.010561442002654076,0.12512598931789398,0.0006975732394494116,0.0722535103559494,0.09063925594091415,-0.03833596780896187,0.12115899473428726,0.020621227100491524,-0.03718132525682449,0.006764677818864584,0.03497420996427536,-0.06695535033941269,-0.0019947884138673544,-0.15584933757781982,-0.021949147805571556,0.03877820447087288,-0.07985866814851761,0.05796068534255028,-0.03992890566587448,-0.025966869667172432,-0.018153861165046692,-0.011541193351149559,-0.0180127564817667,-0.0170031376183033,0.05187125504016876,0.02979646623134613,-0.09935633838176727,0.04494418576359749,-0.05424429103732109,0.057540301233530045,0.12370593100786209,-0.1335931271314621,-0.029449153691530228,-0.032574497163295746,-0.002000474603846669,-0.005593481473624706,-0.0255373977124691,-0.11217605322599411,-0.02810697630047798,-0.03669511899352074,-0.10863740742206573,-0.07063723355531693,0.025500256568193436,0.12369294464588165,0.1266852617263794,0.12295208871364594,0.06872480362653732,-0.010862738825380802,-0.016115978360176086,0.016455572098493576,-0.0320836566388607,0.05513356998562813,-0.016678448766469955,0.03719615563750267,-0.02303551696240902,0.061167966574430466,-0.007973573170602322,-0.06673308461904526,-0.07899884879589081,-0.05804968252778053,-0.06260852515697479,0.014362156391143799,0.030796151608228683,-0.028539558872580528,0.014447405934333801,-0.0316741056740284,-0.09610972553491592,-0.038214486092329025,-0.01217060536146164,0.009092193096876144,-0.03197067975997925,-0.06598290055990219,0.02036280371248722,0.015860965475440025,-0.0007639551186002791,-0.08404061943292618,-0.027254989370703697,-0.14132477343082428,-0.0008781271171756089,-0.03301936015486717,0.03012983873486519,0.03623320907354355,-0.03875339403748512,-0.02413320541381836,-0.037869539111852646,0.020253868773579597,-0.013606151565909386,-0.05339343845844269,-0.086888886988163,-0.002443973906338215,-0.05590979382395744,0.042663753032684326,-0.03486798703670502,-0.02896973490715027,0.04194771870970726,-0.04054027050733566,-0.04971102625131607,0.07027291506528854,0.09766831248998642,-0.09357021003961563,0.011402782052755356,0.021177664399147034,0.06573677808046341,-0.07605048269033432,0.027806539088487625,-0.07907170802354813,-0.01515146717429161,0.118724524974823,0.022602548822760582,-0.06774349510669708,0.03314672410488129,-0.07818392664194107,-0.0056166150607168674,-0.025676673278212547,-0.012284898199141026,-0.07044456899166107,-0.10977047681808472,-0.0008934902725741267,-0.09513399004936218,0.00525939604267478,0.0067669302225112915,-0.005901224911212921,-0.021054038777947426,0.02339744195342064,0.06469248980283737,0.04502182453870773,0.001791475573554635,0.05554453656077385,-0.013253500685095787,-0.04746043682098389,-0.08992110937833786,-0.05065428093075752,-0.0483819879591465,-0.017088457942008972,0.09109567850828171,0.028270995244383812,0.03626800328493118,-0.06932409107685089,-0.013245709240436554,-0.08695971965789795,0.002350631169974804,0.028923500329256058,0.039356254041194916,-0.04390672594308853,-0.03731893375515938,-0.09354671835899353,-0.0005203987238928676,0.01278096716850996,0.05397377535700798,0.04479934647679329,0.0054703583009541035,-0.06975586712360382,-0.04317190498113632,0.00435210345312953,0.050838652998209,0.0026283024344593287,-0.038227379322052,-0.050039928406476974,0.016497517004609108,0.01125218253582716,0.05847661942243576,0.07830701768398285,-0.09247787296772003,0.04558349773287773,0.0669107511639595,-0.10091997683048248,0.08040689677000046,0.03762713074684143,-0.06659215688705444,-0.018301859498023987,-0.03751901164650917,-0.08929035067558289,0.06494118273258209,0.017439398914575577,0.0015420654090121388,-0.03567301109433174,-0.06305208802223206,-0.0009831315837800503,-0.008886754512786865,0.009257924742996693,-0.023844415321946144,-0.027172788977622986,-0.006022045388817787,0.022669386118650436,0.04675350710749626,0.029961343854665756,0.07824500650167465,-0.04146319627761841,-0.04773278906941414,-0.047840509563684464,-0.09395501017570496,-0.04446276277303696,0.006881401874125004,0.08187191933393478,-0.07628991454839706,-0.023679174482822418,-0.0034015607088804245,0.0030528022907674313,0.02336495742201805,0.056691974401474,-0.014508284628391266,0.032771337777376175,0.05100788176059723,0.055238354951143265,-0.010055610910058022,-0.014220578595995903,0.09657181799411774,0.02827785164117813,-0.03438693284988403,-0.10049782693386078,-0.09473773092031479,-0.0668625459074974,0.004454539157450199,0.04717869311571121,0.029830150306224823,0.015112529508769512,-0.011579909361898899,-0.11135143041610718,0.05941735580563545,0.03159977123141289,0.03456764295697212,-0.013043115846812725,-0.07198816537857056,-0.06283872574567795,0.08031868934631348,0.014483395032584667,-0.015247629955410957,0.10383538156747818,-0.008283833041787148,0.04564882814884186,-0.06484531611204147,-0.01114385575056076,0.07892148941755295,0.017176518216729164,0.06892604380846024,-0.05040591210126877,-0.05665283650159836,0.031246880069375038,-0.04144110530614853,0.10136805474758148,0.0027959130238741636,-0.020613444969058037,-0.07587362080812454,-0.07072912156581879,-0.0025255365762859583,-0.03924190253019333,-0.0013600523816421628,-0.026371357962489128,-0.03320784494280815,0.02367040142416954,0.04548310860991478,-0.037304870784282684,-0.00911275390535593,-0.0069494228810071945,0.07924547791481018,-0.011973978020250797,-0.011762534268200397,0.029905160889029503,-0.019594483077526093,0.06040587276220322,0.022824032232165337,0.013239102438092232,0.02936358004808426,-0.024977922439575195,-0.07407509535551071,-0.016413357108831406,0.04543526470661163,-0.03745326027274132,-0.07846362888813019,0.001135923550464213,0.09984556585550308,-0.0016823424957692623,-0.05750259384512901,-0.07135225087404251,0.041730016469955444,0.01152657438069582,0.02595670521259308,0.03420890495181084,-0.022853698581457138,0.06764081120491028,-0.042788002640008926,-0.02189256250858307,-0.1025436744093895,0.005218434613198042,-0.041450388729572296,0.06835245341062546,0.04955748841166496,0.026935363188385963,0.021766256541013718,0.028829753398895264,-0.04127838835120201,-0.11741328984498978,0.08609422296285629,-0.07330632954835892,0.03710031509399414,-0.07893204689025879,0.024572230875492096,-0.04536745697259903,0.0030139603186398745,-0.03733449801802635,0.01944643072783947,0.0395951122045517,0.013319132849574089,0.03171310946345329,0.05665528029203415,0.008093210868537426,0.059898946434259415,0.009142845869064331,0.033806782215833664,0.12802889943122864,-0.02809472382068634,0.031642671674489975,-0.02106078527867794,-0.01033808197826147,0.028562694787979126,0.04263177886605263,-0.0013005186337977648,0.07563596218824387,-0.05804416537284851,0.08186817169189453,-0.038319095969200134,0.028640471398830414,-0.11163236200809479,0.01434896420687437,-0.10302287340164185,0.04575046896934509,0.032501544803380966,0.06746510416269302,-0.0457795150578022,0.007229315582662821,0.11073820292949677,0.07742853462696075,-0.01818734034895897,0.08682311326265335,0.09565190970897675,0.06145063787698746,-0.018308166414499283,0.09003244340419769,0.029569260776042938,0.013486801646649837,-0.08254559338092804,0.017825044691562653,-0.15760819613933563,-0.08978892117738724,-0.04806804284453392,0.04727710038423538,0.0019705966114997864,-0.01419715117663145,-0.023862041532993317,-0.05245346203446388,0.0225615706294775,0.0534406416118145,0.01188618689775467,-0.0027135612908750772,-0.031186524778604507,0.08376502990722656,0.009413477033376694,-0.11293070018291473,0.07986698299646378,0.037950076162815094,-0.02240913361310959,0.028879668563604355,-0.0007393172127194703,-0.017268195748329163,-0.028597934171557426,0.052013617008924484,-0.018713057041168213,-0.008743525482714176,0.0957101359963417,0.07081644982099533,0.04433256387710571,0.07718499004840851,-0.04651210457086563,0.010601629503071308,-0.08631274849176407,0.001718182465992868,0.058907296508550644,0.0015820760745555162,-0.03654825687408447,0.07253246754407883,0.02899942733347416,0.10695663839578629,-0.02926359698176384,0.14747147262096405,-0.05355064570903778,-0.05918482691049576,0.0002550914650782943,0.0016621341928839684,-0.06082626432180405,0.018147235736250877,0.003785336622968316,-0.0262912567704916,-0.002328698756173253,-0.09484273195266724,0.146884024143219,-0.037500422447919846,-0.04669128730893135,-0.013163009658455849,0.01137731596827507,-0.07132274657487869,-0.040611062198877335,0.03125422075390816,-0.025881415233016014,0.04476002976298332,0.037125226110219955,0.027038607746362686,0.002781098708510399,-0.09967096894979477,-0.040112998336553574,-0.010919789783656597,-0.03433792665600777,0.06455183029174805,0.10267531126737595,0.015535391867160797,0.04896176978945732,0.0051084584556519985,0.09083148092031479,0.05780177190899849,-0.025975840166211128,0.03345537185668945,-0.01754901558160782,-0.02457256428897381,-0.0786576047539711,0.030351094901561737,0.01675298437476158,-0.00860867090523243,-0.004044619854539633,-0.03897698223590851,0.046730849891901016,0.11963805556297302,0.024576295167207718,-0.004900844767689705,0.014070935547351837,0.04384183511137962,0.02669047936797142,0.08507721871137619,-0.019131222739815712,-0.040547002106904984,-0.030972875654697418,0.015554284676909447,0.09492349624633789,0.01317451149225235,0.05476460978388786,-0.015743764117360115,0.04861173778772354,-0.008990086615085602,-0.026784876361489296,0.062021404504776,-0.022461963817477226,-0.07720743119716644,-0.02568603679537773,-0.025208454579114914,-0.08412130177021027,-0.0277852900326252,-0.002556832041591406,0.0737675353884697,-0.002102953614667058,-0.04181280359625816,0.05404911935329437,-0.007387543562799692,-0.04104774072766304,-0.07083268463611603,-0.003405472729355097,-0.07785255461931229,-0.02097182348370552,0.028647789731621742,-0.0029884877149015665,0.005996872205287218,0.052364591509103775,0.03700337931513786,-0.05261591076850891,0.04621463268995285,-0.02628030627965927,-0.004359361715614796,-0.026777509599924088,-0.11751613765954971,-0.037996985018253326,-0.08863937854766846,-0.02801976166665554,0.005783773027360439,0.05626792088150978,0.04766126722097397,-0.020245639607310295,0.09144587814807892,0.013559376820921898,0.023451123386621475,-0.03345153480768204,0.07921204715967178,-0.013890083879232407,-0.0770488977432251,-0.035536665469408035,-0.0107731269672513,0.039435598999261856,-0.02774977684020996,-0.055982641875743866,-0.030710941180586815,0.048228319734334946,0.04710736870765686,-0.062465012073516846,-0.02254234068095684,0.04739265888929367,-0.01964697800576687,0.003995737060904503,0.1273331642150879,-9.465603216085583e-05,0.053011853247880936,-0.021720336750149727,-0.08133789896965027,-0.030435943976044655,0.0514175221323967,-0.050036873668432236,0.03957256302237511,0.012960749678313732,0.018196076154708862,0.006227881647646427,0.1255049854516983,-0.06025395169854164,0.015573391690850258,-0.04399232938885689,-0.0070144846104085445,-0.02009664848446846,0.06486143171787262,0.07503513991832733,-0.0040271892212331295,-0.047434497624635696,-0.08587208390235901,-0.09166401624679565,-0.07490935921669006,0.010480096563696861,0.0394413024187088,0.05296999216079712,-0.046990104019641876,-0.033427197486162186,0.002995220711454749,0.029682163149118423,0.0035929870791733265,-0.04342855140566826,-0.008600454777479172,0.01304623018950224,-0.024522706866264343,0.08594932407140732,0.015172713436186314,-0.024311823770403862,-0.06024806201457977,-0.05146852135658264,-0.024667974561452866,0.008482939563691616,-0.04058859497308731,-0.0012483377940952778,0.05123503506183624,-0.020317448303103447,-0.04416150599718094,-0.037492476403713226,-0.10808657109737396,-0.043197281658649445,-0.03647429496049881,-0.06466159969568253,0.02589506283402443,0.030439326539635658,0.008025569841265678,0.0692741796374321,-0.034289076924324036,-0.041742146015167236,0.0381428524851799,-0.01217537373304367,-0.1215338259935379,0.014514616690576077,0.06752529740333557,0.0016800245502963662,0.0008884207927621901,-0.05995167791843414,-0.025678634643554688,0.031098654493689537,-0.020966283977031708,-0.014078802429139614,0.03129265829920769,0.014364541508257389,-0.001272657886147499,0.0957518145442009,-0.048740778118371964,0.03449588641524315,-0.07438974827528,0.09674022346735,-0.01093276496976614,0.03984766826033592,-0.030156521126627922,-0.007121969945728779,-0.005204884335398674,-0.022339247167110443,-0.07064453512430191,-0.03992193564772606,-0.06912928074598312,0.0754069834947586,-0.06169872358441353,0.05445444956421852,0.04906550422310829,-0.041209783405065536,-0.03773428872227669,-0.04656033590435982,-0.07495816051959991,-0.04070454090833664,-0.015524953603744507,0.061363834887742996,0.018813498318195343,0.11526326090097427,0.0684642493724823,-0.0028144707903265953,0.0007072568405419588,0.012195918709039688,0.014400873333215714,0.031560491770505905,-0.05444076657295227,-0.047707509249448776,0.03274748474359512,-0.0442233607172966,-0.029180118814110756,-0.05053829029202461,-0.01885848119854927,-0.0319891981780529,-0.10474429279565811,0.04229804873466492,-0.013940890319645405,-0.08430454879999161,-0.08597701787948608,0.02135222963988781,0.07381555438041687,-0.019259275868535042,0.06255701184272766,-0.06919509172439575,-0.008233671076595783,-0.02436429262161255,-0.023485558107495308,0.05722275376319885,0.04369218274950981,0.017257651314139366,0.018527474254369736,-0.008992516435682774,-0.008240164257586002,-0.11741903424263,0.010051066987216473,0.02439754083752632,0.005957864690572023,0.06369154155254364,-0.09647876769304276,0.07355933636426926,0.01711910218000412,0.04336617887020111,0.10289902240037918,-0.04997347667813301,0.039319030940532684,-0.024019377306103706,0.06138652190566063,0.01312814000993967,-0.0005480694235302508,-0.06550581753253937,-0.04526253417134285,0.03014085814356804,0.07377488911151886,0.04828136786818504,0.0060141924768686295,-0.022580532357096672,0.0012761136749759316,0.06948652118444443,0.035344742238521576,-0.03161732479929924,-0.01418851874768734,-0.05898566171526909,-0.054890211671590805,-0.03161347657442093,0.05456548184156418,-0.05012642219662666,-0.014343837276101112,-0.024423662573099136,-0.07337735593318939,-0.040410831570625305,-0.002583959838375449,0.01381303183734417,-0.07891499251127243,0.018777713179588318,0.034314677119255066,0.037926189601421356,0.05782168731093407,-0.04762038215994835,0.026279186829924583,0.014733586460351944,0.06190105900168419,0.011127247475087643,0.02610987052321434,0.04171500355005264,-0.02480391226708889,0.01600181870162487,0.10624710470438004,0.04765636846423149,0.0006946508074179292,-0.08648595958948135,-0.016342297196388245,0.02135768160223961,0.0029372612480074167,-0.005709350109100342,-0.00966863613575697,-0.11233140528202057,-0.02825811132788658,-0.029760466888546944,0.01635102555155754,-0.04144032672047615,-0.08573339134454727,0.05637173354625702,0.005106337368488312,0.016518358141183853,-0.0498146116733551,0.010307836346328259,0.010242310352623463,-0.07633669674396515,0.02143183909356594,0.07575910538434982,-0.05084327235817909,0.04747218266129494,-0.06231186166405678,-0.0008026068098843098,0.06700533628463745,0.02548123337328434,-0.009669355116784573,-0.010969080962240696,-0.024461880326271057,-0.07588212192058563,0.013539595529437065,0.08587432652711868,0.05402570590376854,-0.014876792207360268,-0.05544808506965637,0.04620087891817093,0.018215566873550415,-0.0100028021261096,0.03611897677183151,-0.0016849058447405696,0.05756527930498123,0.08326885849237442,-0.07140614837408066,0.028798826038837433,0.031118590384721756,-0.011799922212958336,-0.1476433277130127,-0.015398696064949036,-0.007327748462557793,0.009841368533670902,-0.03928051143884659,0.007975626736879349,-0.07715655863285065,-0.04182137921452522,-0.003365481738001108,0.035853635519742966,-0.07711682468652725,0.08297882229089737,-0.07293273508548737,-0.037012599408626556,0.03980810195207596,-0.02700481377542019,-0.0532069094479084,0.006175054237246513,-0.004578190855681896,-0.04764636978507042,-0.0305201206356287,-0.020775247365236282,0.02262888103723526,-0.010271078906953335,0.019766803830862045,-0.028738362714648247,-0.05422725901007652,0.01597273349761963,0.04263555631041527,0.11140689998865128,0.0554710328578949,0.0726882666349411,-0.08521439135074615,-0.0011348790721967816,0.0007292648660950363,0.03680161014199257,0.009822775609791279,0.10103584080934525,0.04346579685807228,0.018398359417915344,-0.03413766622543335,0.043918028473854065,-0.026547126471996307,0.019166728481650352,-0.007694033440202475,-0.05228506773710251,0.024114392697811127,0.0723554939031601,-0.02767009660601616,-0.02111891843378544,0.036521803587675095,0.04412675276398659,-0.0376957468688488,4.9353413487551734e-05,0.01752425916492939,0.0010321714216843247,-0.015210891142487526,0.011682639829814434,0.017800036817789078,0.036478810012340546,0.061108093708753586,-0.008114113472402096,-0.01850867085158825,-0.019621029496192932,0.00701837707310915,0.022376351058483124,0.02816854603588581,-0.009043300524353981,0.074813611805439,0.0068421210162341595,0.0007943316595628858,-0.015838567167520523,0.039617929607629776,0.011884179897606373,7.343445031438023e-05,0.04499400407075882,0.049456480890512466,0.007054953370243311,-0.007377223577350378,-0.0032826950773596764,-0.009706341661512852,0.05625935271382332,-0.036612484604120255,0.13478846848011017,-0.0654103085398674,-0.04216054826974869,-0.05253211781382561,-0.030486298725008965,-0.13003340363502502,0.021424325183033943,0.10210936516523361,0.009945779107511044,0.011152578517794609,-0.049651287496089935,-0.05198294296860695,-0.025429999455809593,0.16155248880386353,0.02492901124060154,0.07834839075803757,-0.047428976744413376,-0.025814121589064598,-0.01913633942604065,0.05763503164052963,-0.05680083483457565,0.04956432804465294,-0.029897518455982208,-0.03364826738834381,-0.028285188600420952,0.0008236532448790967,-0.016697969287633896,-0.02299521304666996,0.007744450122117996,0.012837111949920654,0.059125348925590515,0.034284524619579315,0.010202682577073574,0.022206762805581093,-0.010719440877437592,-0.0035186144523322582,-0.02106780931353569,-0.02336990088224411,-0.03421953320503235,0.04528332129120827,0.09743677079677582,-0.07437437027692795,0.03384047746658325,0.04648993909358978,0.005136231891810894,-0.06885594874620438,0.0021519374568015337,0.030911382287740707,-0.049258459359407425,-0.02390037104487419,0.016864879056811333,-0.036208610981702805,0.03142925351858139,-0.003095278050750494,-0.028920283541083336,0.00012587245146278292,0.05538810044527054,0.049448348581790924,0.04664849862456322,-0.019930938258767128,-0.06339343637228012,0.05917911231517792,-0.05280223861336708,-0.048375170677900314,-0.012775156646966934,0.016430433839559555,0.0008833353058435023,0.042129162698984146,0.031005656346678734,0.006401460617780685,-0.009635836817324162,-0.011469160206615925,-0.02416851744055748,0.01729656383395195,0.013685908168554306,0.0033270660787820816,0.03578528016805649,-0.01189349964261055,-0.015114095993340015,-0.013530896045267582,-0.05583414435386658,-0.0059229531325399876,0.014333856292068958,0.024505577981472015,-0.041763849556446075,-0.0010529117425903678,-0.016859687864780426,-0.07449344545602798,0.010846780613064766,0.04080692678689957,-0.004824325907975435,0.01830366998910904,0.022174110636115074,0.002104584826156497,0.0007363940239883959,0.03244564309716225,-0.050402477383613586,0.061226580291986465,-0.05803059786558151,0.026123104616999626,-0.052175119519233704,-0.01794423721730709,0.051155224442481995,0.04070587828755379,-0.015016123652458191,0.08210087567567825,0.04149797558784485,0.06737818568944931,0.01678798347711563,0.05754079669713974,-0.008902301080524921,0.03967975080013275,0.0008411705493927002,0.02923434041440487,0.05223146453499794,-0.08042735606431961,0.011000564321875572,0.013351730071008205,-0.04117967560887337,-0.05825002118945122,-0.013791670091450214,-0.09020312130451202,0.11400429904460907,0.08962453156709671,-0.060807984322309494,-8.61311491462402e-05,0.04992107301950455,0.01716298609972,-0.06959071010351181,-0.050138216465711594,-0.011226520873606205,0.014152943156659603,0.12259846925735474,-0.07014764845371246,-0.028540048748254776,0.05283287167549133,0.06944232434034348,0.05295694246888161,-0.10291006416082382,0.16501572728157043,-0.024780182167887688,0.14504428207874298,-0.022929929196834564,-0.056477755308151245,-0.0018525037448853254,0.10156680643558502,0.050530653446912766,0.03512437641620636,0.0006504825432784855,-0.06040685623884201,0.015713902190327644,-0.05617271736264229,-0.014244921505451202,0.042838022112846375,0.055136486887931824,0.014766788110136986,0.08081137388944626,-0.026328064501285553,0.029680471867322922,-0.02974356897175312,0.0246818196028471,-0.03424791246652603,-0.013234165497124195,0.017793981358408928,0.06596904247999191,0.08887273818254471,-0.03841551020741463,-0.01974790170788765,7.028287654975429e-05,0.05003621056675911,-0.07606034725904465,-0.012257140129804611,0.06360464543104172,0.04802703857421875,0.0412168949842453,0.06362935155630112,-0.03702109679579735,0.026290180161595345,0.021086832508444786,0.02744111604988575,-0.004187530837953091,0.01404853630810976,0.02223893813788891,0.0013097213814035058,0.004110714420676231,-0.015907421708106995,0.008174731396138668,-0.04402841255068779,-0.000405058148317039,0.008021052926778793,0.06543081253767014,-0.0035062984097748995,0.0304639283567667,-0.04745928570628166,-0.0688975602388382,0.04829699546098709,-0.00257432390935719,0.01797337457537651,-0.0916774719953537,0.02413991279900074,-0.047934640198946,-0.04193845018744469,0.0014648985816165805,-0.0734793171286583,-0.06450182944536209,0.0553484745323658,0.024513619020581245,-0.015238405205309391,-0.024353647604584694,0.03879496455192566,0.05770694836974144,0.06339004635810852,-0.03657200559973717,-0.005768932867795229,0.14687608182430267,-0.015662161633372307,0.025461791083216667,0.05618758499622345,0.01273971889168024,0.06286390125751495,0.04084505885839462,0.12870952486991882,-0.021608352661132812,-0.014169307425618172,0.031757455319166183,-0.05167967826128006,-0.01516908872872591,0.04470284655690193,-0.08841914683580399,-0.020417802035808563,-0.06916347146034241,-0.07326175272464752,-0.05092407390475273,0.013480287976562977,0.04207061231136322,0.031245658174157143,-0.018821602687239647,0.009731735102832317,-0.014780838042497635,0.04566749185323715,0.06733644753694534,-0.0032810340635478497,-0.021767696365714073,0.019762376323342323,0.07611524313688278,0.0656217411160469,0.0477781742811203,-0.054736293852329254,0.02563159354031086,-0.02930518612265587,-0.008223149925470352,0.07437267899513245,0.01849959045648575,0.048841897398233414,-0.07211726158857346,-0.044601064175367355,0.023269856348633766,0.006119709927588701,0.007243430707603693,-0.03445654734969139,-0.04408958926796913,0.012270438484847546,0.11020681262016296,-0.033189188688993454,-0.01117449440062046,0.05830817297101021,0.0853571891784668,0.07355186343193054,0.10312934219837189,-0.006252238526940346,-0.05236886069178581,-0.03367752954363823,-0.11847587674856186,-0.014639653265476227,-0.01674627512693405,-0.059322405606508255,0.017240626737475395,0.07766954600811005,-0.07838597893714905,-0.010319960303604603,-0.029167572036385536,-0.06059250235557556,-0.019336141645908356,-0.019628215581178665,0.020262664183974266,-0.06610986590385437,-0.00852958858013153,0.035646166652441025,-0.025576723739504814,-0.00815290305763483,0.07508014142513275,0.04239941015839577,-0.09353941679000854,-0.08873406052589417,-0.03679880499839783,0.03937796503305435,-0.05768350139260292,-0.036632638424634933,0.04121450334787369,-0.06999066472053528,-0.0633501335978508,0.05243859812617302,0.038169339299201965,0.01656506024301052,0.15518207848072052,-0.06486731022596359,0.04892211779952049,0.07510226219892502,0.013629891909658909,-0.0011014558840543032,0.045157793909311295,0.09092230349779129,0.022970063611865044,-0.05882759019732475,-0.10943642258644104,0.01671845093369484,0.03178143501281738,-0.02146284654736519,0.009686002507805824,0.01914656162261963,0.004709722474217415,-0.005878670606762171,-0.0550239160656929,-0.005826056003570557,0.08954863995313644,-0.03097478672862053,-0.0018655307358130813,-0.03411377593874931,0.015071053057909012,0.06596291810274124,-0.033374086022377014,0.03347356989979744,0.012265007011592388,0.130051851272583,-0.13243967294692993,0.052760686725378036,0.0713488981127739,-0.006173484027385712,-0.013834185898303986,-0.06966525316238403,0.0815071240067482,-0.05622463673353195,-0.003028579754754901,-0.09114193171262741,-0.01536458171904087,-0.03183354437351227,-0.07601185142993927,0.04038948938250542,0.04520334675908089,-0.01706882193684578,-0.010213528759777546,-0.03279782831668854,-0.0037128536496311426,-0.00238421862013638,-0.026516754180192947,0.0373772457242012,0.07168149948120117,-0.04338044300675392,-0.0002134770038537681,0.09694512188434601,0.008727425709366798,0.02498845010995865,-0.01739257574081421,0.01445801742374897,-0.07545376569032669,-0.05254443362355232,-0.11972728371620178,-0.009649356827139854,-0.045134786516427994,-0.03475400432944298,-0.05129389092326164,-0.0273442380130291,0.062148839235305786,-0.09410622715950012,0.07989495247602463,-0.05577085539698601,0.02931612730026245,-0.0562257319688797,-0.09808894246816635,-0.03648238629102707,-0.023368878290057182,-0.027896881103515625,0.005902094300836325,0.024651721119880676,0.009273509494960308,-0.011936312541365623,0.0055614616721868515,0.02683953382074833,-0.046109020709991455,-0.017907731235027313,0.001978275366127491,-0.031171245500445366,-0.0026445488911122084,-0.02441141940653324,-0.017318125814199448,0.031018875539302826,-0.01551393698900938,0.07053525745868683,-0.0152317825704813,0.022049572318792343,0.026859568431973457,0.03245755657553673,0.0315316766500473,-0.05564378947019577,-0.03291434422135353,0.033094171434640884,0.046770691871643066,-0.05499603599309921,0.02162434160709381,-0.01768762245774269,0.05289306864142418,-0.053507231175899506,-0.04264526441693306,-0.038422100245952606,0.02850966900587082,0.007213409524410963,-0.039860375225543976,-0.018161341547966003,-0.03349253535270691,-0.03142614662647247,-0.0012002747971564531,-0.013559756800532341,-0.04615170508623123,-0.0065607852302491665,0.021059123799204826,0.052771419286727905,-0.05270145833492279,0.00981523934751749,-0.09536837041378021,-0.0051758852787315845,-0.09997431933879852],"yaxis":"y","type":"scattergl"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"white","polar":{"bgcolor":"white","angularaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""},"radialaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""}},"ternary":{"bgcolor":"white","aaxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"baxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"caxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8","gridwidth":2},"yaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8","gridwidth":2},"zaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"white","subunitcolor":"#C8D4E3","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"x0"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"x1"}},"legend":{"tracegroupgap":0,"itemsizing":"constant"},"margin":{"t":60}},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('d172e7bc-8332-45cf-9e86-5f27788ee653');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                            </script>        </div>


</div>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="HW6_files/figure-html/cell-59-1-fig.png" class="img-fluid figure-img"></p>
<figcaption>fig.png</figcaption>
</figure>
</div>
</section>
<section id="final-takeaway" class="level1">
<h1>Final Takeaway</h1>
<p>That is all of creating a fake news classifier using Keras. Hope you found this tutorial interesting!</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>